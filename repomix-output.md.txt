# Directory Structure
```
pyproject.toml
pytest.ini
README.md
requirements.txt
src/google_takeout_metadata/__init__.py
src/google_takeout_metadata/__main__.py
src/google_takeout_metadata/cli.py
src/google_takeout_metadata/exif_writer.py
src/google_takeout_metadata/processor_batch.py
src/google_takeout_metadata/processor.py
src/google_takeout_metadata/sidecar.py
src/google_takeout_metadata/statistics.py
test_assets/README.md
tests/test_hybrid_approach.py
tests/test_cli.py
tests/test_end_to_end.py
tests/test_exif_writer.py
tests/test_hybrid_approach.py
tests/test_improvements.py
tests/test_integration.py
tests/test_processor_batch.py
tests/test_processor.py
tests/test_sidecar.py
```

# Files

## File: pyproject.toml
````toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "google_takeout_metadata"
version = "0.1.0"
description = "Merge Google Takeout metadata into images"
readme = "README.md"
license = {file = "LICENSE"}
authors = [
    {name = "Anthony", email = "anthony@example.com"}
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: End Users/Desktop",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
]
dependencies = []

[project.optional-dependencies]
test = ["pytest", "pillow"]

[project.scripts]
google-takeout-metadata = "google_takeout_metadata.cli:main"

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
testpaths = ["tests"]
markers = [
    "integration: marks tests as integration tests (requiring exiftool)",
]
````

## File: pytest.ini
````
[pytest]
addopts = -ra
pythonpath = src
markers =
    integration: marks tests as integration tests requiring exiftool
````

## File: README.md
````markdown
# exif_metadata_google_photo_takeout

Ce projet permet d'incorporer les m√©tadonn√©es des fichiers JSON produits par Google Takeout dans les photos correspondantes.

## Fonctionnalit√©s

‚úÖ **M√©tadonn√©es support√©es:**
- Descriptions/l√©gendes
- Personnes identifi√©es (avec d√©duplication automatique)
- Dates de prise de vue et de cr√©ation
- Coordonn√©es GPS (filtrage automatique des coordonn√©es 0/0 peu fiables)
- Favoris (mapp√©s sur le tag Favorite bool√©en)
- **Albums** (d√©tect√©s depuis les fichiers metadata.json de dossier et ajout√©s comme mots-cl√©s "Album: <nom>")

‚úÖ **Mode de fonctionnement s√ªr par d√©faut:**
- **Append-only par d√©faut**: Les m√©tadonn√©es existantes sont pr√©serv√©es
- Les descriptions ne sont √©crites que si elles n'existent pas d√©j√†
- Les personnes et albums sont ajout√©s aux listes existantes sans suppression
- Utiliser `--overwrite` pour forcer l'√©crasement des m√©tadonn√©es existantes

‚úÖ **Options avanc√©es:**
- `--localtime`: Conversion des timestamps en heure locale au lieu d'UTC
- `--overwrite`: Force l'√©crasement des m√©tadonn√©es existantes (mode destructif)
- `--batch`: Mode batch pour traitement optimis√© de gros volumes de fichiers

‚úÖ **Qualit√©:**
- Tests unitaires complets
- Tests d'int√©gration E2E avec exiftool
- Support des formats photo et vid√©o
- **Arguments s√©curis√©s** : Protection contre l'injection shell avec noms contenant des espaces
- **Op√©rateur `+=` optimis√©** : Utilise l'op√©rateur exiftool `+=` pour accumulation s√ªre des tags de type liste

## Installation

Pr√©requis: `exiftool` doit √™tre install√© et accessible dans le PATH.

```bash
pip install -e .
```

## Utilisation

### Utilisation basique (mode s√ªr par d√©faut)
```bash
# Mode append-only par d√©faut - pr√©serve les m√©tadonn√©es existantes
google-takeout-metadata /chemin/vers/le/dossier
```

### Avec options
```bash
# Utiliser l'heure locale pour les timestamps
google-takeout-metadata --localtime /chemin/vers/le/dossier

# Mode destructif: √©craser les m√©tadonn√©es existantes (√† utiliser avec pr√©caution)
google-takeout-metadata --overwrite /chemin/vers/le/dossier

# Nettoyer les fichiers sidecars apr√®s traitement
google-takeout-metadata --clean-sidecars /chemin/vers/le/dossier

# Combiner les options (mode s√ªr avec heure locale)
google-takeout-metadata --localtime /chemin/vers/le/dossier
```

### Mode batch (optimis√© pour gros volumes)
```bash
# Mode batch: traitement optimis√© pour de nombreux fichiers
google-takeout-metadata --batch /chemin/vers/le/dossier

# Mode batch avec autres options
google-takeout-metadata --batch --localtime /chemin/vers/le/dossier
google-takeout-metadata --batch --overwrite /chemin/vers/le/dossier

# Exemple concret avec toutes les options (pointer vers le dossier Takeout)
google-takeout-metadata --batch --localtime --clean-sidecars "C:\Users\anthony\Downloads\google photos\Takeout"
```

**Si la commande `google-takeout-metadata` n'est pas trouv√©e:**
```bash
# Option 1: Utiliser le module Python directement (attention aux underscores)
python -m google_takeout_metadata --batch --localtime --clean-sidecars "/chemin/vers/dossier"

# Option 2: Utiliser l'environnement virtuel complet avec le module
C:/Users/anthony/Documents/PROJETS/exif_metadata_google_photo_takeout/.venv/Scripts/python.exe -m google_takeout_metadata --batch --localtime --clean-sidecars "C:\Users\anthony\Downloads\google photos\Takeout"

# Option 3: Utiliser l'ex√©cutable directement depuis l'environnement virtuel
C:/Users/anthony/Documents/PROJETS/exif_metadata_google_photo_takeout/.venv/Scripts/google-takeout-metadata.exe --batch --localtime --clean-sidecars "C:\Users\anthony\Downloads\google photos\Takeout"

# Option 4: Activer l'environnement virtuel d'abord
.venv/Scripts/activate  # Sur Windows
google-takeout-metadata --batch --localtime --clean-sidecars "/chemin/vers/dossier"
```

**Avantages du mode batch:**
- **Performance am√©lior√©e** : Traitement par lots avec exiftool pour r√©duire le nombre d'appels syst√®me
- **Id√©al pour gros volumes** : Optimis√© pour traiter des milliers de fichiers
- **Moins de fragmentation** : R√©duit la charge syst√®me en groupant les op√©rations
- **M√™me s√©curit√©** : Conserve le comportement append-only par d√©faut

**Quand utiliser le mode batch:**
- Traitement de biblioth√®ques photo importantes (>100 fichiers)
- Archives Google Takeout volumineuses
- Situations o√π la performance est critique

**Note de performance:**
Le mode batch r√©duit significativement le temps de traitement en groupant les appels √† exiftool. 
Pour 1000 fichiers, le gain peut √™tre de 50-80% selon la configuration syst√®me.

Le programme parcourt r√©cursivement le dossier, cherche les fichiers `*.json` et √©crit les informations pertinentes dans les fichiers image correspondants √† l'aide d'`exiftool`.

## Comportement par d√©faut (S√©curis√©)

**Le mode append-only est d√©sormais activ√© par d√©faut** pour √©viter la perte accidentelle de m√©tadonn√©es:

### ‚úÖ M√©tadonn√©es pr√©serv√©es:
- **Descriptions existantes** ne sont jamais √©cras√©es
- **Dates existantes** ne sont jamais modifi√©es
- **Coordonn√©es GPS existantes** ne sont jamais remplac√©es
- **Ratings existants** ne sont jamais chang√©s

### ‚úÖ M√©tadonn√©es ajout√©es:
- **Personnes** sont ajout√©es aux listes existantes (pas de suppression)
- **Albums** sont ajout√©s aux mots-cl√©s existants (pas de suppression)

### ‚ö†Ô∏è Mode destructif:
Utilisez `--overwrite` seulement si vous voulez explicitement √©craser les m√©tadonn√©es existantes.

## D√©tails techniques

### Op√©rateur exiftool `+=` pour les listes
Notre impl√©mentation utilise l'op√©rateur `+=` d'exiftool pour une gestion s√ªre des tags de type liste :

```bash
# ‚úÖ Correct : L'op√©rateur += ajoute ET cr√©e le tag si n√©cessaire
exiftool "-XMP-iptcExt:PersonInImage+=John Doe" photo.jpg

# ‚ùå Incorrect : L'op√©rateur += seul ne cr√©e pas un tag inexistant
# (ancien comportement qui √©chouait)
```

**Avantages de notre approche :**
- **Cr√©ation automatique** : `+=` cr√©e le tag s'il n'existe pas
- **Accumulation s√ªre** : Ajoute aux listes existantes sans duplication
- **S√©curit√©** : Arguments s√©par√©s pr√©viennent l'injection shell avec espaces
- **Mode overwrite** : Vide explicitement puis reremplit avec `+=`

### Format Google Takeout support√©
```json
{
  "title": "IMG_20240716_200232.jpg",
  "description": "Description de la photo",
  "photoTakenTime": {"timestamp": "1721152952"},
  "creationTime": {"timestamp": "1721152952"},
  "geoData": {
    "latitude": 48.8566,
    "longitude": 2.3522,
    "altitude": 35.0
  },
  "people": [
    {"name": "John Doe"},
    {"name": "Jane Smith"}
  ],
  "favorited": true,
  "archived": false
}
```

## Tests

```bash
# Tests unitaires
pytest tests/ -m "not integration"

# Tests complets (n√©cessite exiftool)
pytest tests/

# Tests d'int√©gration uniquement
pytest tests/ -m "integration"
```

Les tests comprennent:
- **Tests unitaires**: Parsing des sidecars, g√©n√©ration des arguments exiftool
- **Tests d'int√©gration**: √âcriture et relecture effective des m√©tadonn√©es avec exiftool
- **Tests du mode batch**: V√©rification des performances et de la compatibilit√© du traitement par lots
- **Tests CLI**: Validation de l'interface en ligne de commande et de toutes les options
````

## File: requirements.txt
````
pillow
pytest
````

## File: src/google_takeout_metadata/__init__.py
````python
"""Utilitaires pour fusionner les m√©tadonn√©es annexes Google Takeout dans des fichiers image."""
__all__ = [
    "sidecar",
    "exif_writer",
    "processor",
]
````

## File: src/google_takeout_metadata/__main__.py
````python
"""Point d'entr√©e principal du paquet google_takeout_metadata."""
from google_takeout_metadata.cli import main
if __name__ == "__main__":
    main()
````

## File: src/google_takeout_metadata/cli.py
````python
"""Interface en ligne de commande."""
from __future__ import annotations
import argparse
import logging
import shutil
import sys
from pathlib import Path
from .processor import process_directory
from .processor_batch import process_directory_batch
from .statistics import ProcessingStats
import google_takeout_metadata.statistics as stats_module
def main(argv: list[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description="Fusionner les m√©tadonn√©es Google Takeout dans les images")
    parser.add_argument("path", type=Path, help="R√©pertoire √† analyser r√©cursivement")
    parser.add_argument(
        "--localtime", action="store_true",
        help="Convertir les horodatages en heure locale au lieu de l'UTC (par d√©faut : UTC)"
    )
    parser.add_argument(
        "--overwrite", action="store_true",
        help="Autoriser l'√©crasement des champs de m√©tadonn√©es existants (par d√©faut, les m√©tadonn√©es existantes sont pr√©serv√©es)"
    )
    parser.add_argument(
        "--append-only", action="store_true",
        help=argparse.SUPPRESS  # Cache l'option d√©pr√©ci√©e de l'aide
    )
    parser.add_argument(
        "--clean-sidecars", action="store_true",
        help="Supprimer les fichiers JSON annexes apr√®s un transfert de m√©tadonn√©es r√©ussi"
    )
    parser.add_argument(
        "-v", "--verbose", action="store_true",
        help="Activer les logs d√©taill√©s (niveau DEBUG)"
    )
    parser.add_argument(
        "--batch", action="store_true",
        help="Traiter les fichiers par lots"
    )
    args = parser.parse_args(argv)
    # Configuration du logging avec le niveau appropri√©
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level, 
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    # Gestion de la r√©trocompatibilit√© et validation des options
    if args.append_only and args.overwrite:
        logging.error("Impossible d'utiliser simultan√©ment --append-only (obsol√®te) et --overwrite")
        sys.exit(1)
    if args.append_only:
        logging.warning("--append-only est obsol√®te et correspond d√©sormais au comportement par d√©faut. Utilisez --overwrite pour autoriser l'√©crasement des m√©tadonn√©es existantes.")
    if not args.path.is_dir():
        logging.error("Le chemin indiqu√© n'est pas un r√©pertoire : %s", args.path)
        sys.exit(1)
    # R√©initialiser les statistiques pour cette ex√©cution (nouvelle instance)
    stats_module.stats = ProcessingStats()
    # Le mode par d√©faut est maintenant append_only=True (s√©curit√© par d√©faut)
    # L'option --overwrite permet d'√©craser les m√©tadonn√©es existantes
    append_only = not args.overwrite
    # V√©rifier que exiftool est disponible uniquement si on va traiter
    if shutil.which("exiftool") is None:
        logging.error("exiftool introuvable. Veuillez l'installer pour utiliser ce script.")
        sys.exit(1)
    if args.batch:
        process_directory_batch(args.path, use_localtime=args.localtime, append_only=append_only, clean_sidecars=args.clean_sidecars)
    else:
        process_directory(args.path, use_localtime=args.localtime, append_only=append_only, clean_sidecars=args.clean_sidecars)
if __name__ == "__main__":  # pragma: no cover - CLI entry
    main()
````

## File: src/google_takeout_metadata/exif_writer.py
````python
# Fichier : src/google_takeout_metadata/exif_writer.py
import subprocess
import logging
from datetime import datetime, timezone
from pathlib import Path
from .sidecar import SidecarData
logger = logging.getLogger(__name__)
VIDEO_EXTS = {".mp4", ".mov", ".m4v", ".3gp"}
def _is_video_file(path: Path) -> bool:
    return path.suffix.lower() in VIDEO_EXTS
def _fmt_dt(ts: int | None, use_localtime: bool) -> str | None:
    if ts is None:
        return None
    dt = datetime.fromtimestamp(ts) if use_localtime else datetime.fromtimestamp(ts, tz=timezone.utc)
    return dt.strftime("%Y:%m:%d %H:%M:%S")
def _build_keywords(meta: SidecarData) -> list[str]:
    """Centralise la logique de cr√©ation des mots-cl√©s √† partir des personnes et albums."""
    return (meta.people or []) + [f"Album: {a}" for a in (meta.albums or [])]
def _sanitize_description(desc: str) -> str:
    """Centralise le nettoyage des descriptions pour ExifTool."""
    return desc.replace("\r", " ").replace("\n", " ").strip()
def write_metadata(media_path: Path, meta: SidecarData, use_localtime: bool = False, append_only: bool = True) -> None:
    """√âcrit les m√©tadonn√©es sur un m√©dia en utilisant ExifTool."""
    if append_only:
        # Mode append-only : utiliser build_exiftool_args qui g√®re d√©j√† tout avec -wm cg
        args = build_exiftool_args(meta, media_path, use_localtime, append_only=True)
        if args:
            _run_exiftool_command(media_path, args, _append_only=True)
    else:
        # Mode √©crasement : utiliser build_exiftool_args directement
        all_args = build_exiftool_args(meta, media_path, use_localtime, append_only=False)
        # Ex√©cuter en mode √©crasement
        if all_args:
            _run_exiftool_command(media_path, all_args, _append_only=False)
def build_exiftool_args(meta: SidecarData, media_path: Path = None, use_localtime: bool = False, append_only: bool = True) -> list[str]:
    """Construit les arguments exiftool pour traiter un fichier m√©dia avec les m√©tadonn√©es fournies.
    Args:
        meta: M√©tadonn√©es √† √©crire
        media_path: Chemin du fichier m√©dia (optionnel, pour la d√©tection vid√©o)
        use_localtime: Utiliser l'heure locale au lieu d'UTC
        append_only: Mode append-only (-wm cg) ou mode √©crasement
    Returns:
        Liste des arguments exiftool
    """
    args = []
    if append_only:
        # Mode append-only : inclusion de tous les tags avec -wm cg pour compatibilit√© batch
        # Note: Cela peut cr√©er des doublons dans les tags de liste
        if media_path and _is_video_file(media_path):
            args.extend(["-api", "QuickTimeUTC=1"])
        args.extend(["-wm", "cg"])
        # Description
        if meta.description:
            safe_desc = _sanitize_description(meta.description)
            args.extend([f"-EXIF:ImageDescription={safe_desc}", f"-XMP-dc:Description={safe_desc}", f"-IPTC:Caption-Abstract={safe_desc}"])
            if media_path and _is_video_file(media_path):
                args.append(f"-Keys:Description={safe_desc}")
        # Tags de liste avec += (peut cr√©er des doublons en mode batch)
        if meta.people:
            for person in meta.people:
                args.append(f"-XMP-iptcExt:PersonInImage+={person}")
        # Mots-cl√©s (personnes + albums)
        all_keywords = _build_keywords(meta)
        if all_keywords:
            for keyword in all_keywords:
                args.append(f"-XMP-dc:Subject+={keyword}")
                args.append(f"-IPTC:Keywords+={keyword}")
        # Dates
        if (s := _fmt_dt(meta.taken_at, use_localtime)):
            args.append(f"-DateTimeOriginal={s}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:CreateDate={s}")
        base_ts = meta.created_at or meta.taken_at
        if (s := _fmt_dt(base_ts, use_localtime)):
            args.append(f"-CreateDate={s}")
            args.append(f"-ModifyDate={s}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:ModifyDate={s}")
        # GPS
        if meta.latitude is not None and meta.longitude is not None:
            lat = str(abs(meta.latitude))
            lon = str(abs(meta.longitude))
            lat_ref = "N" if meta.latitude >= 0 else "S"
            lon_ref = "E" if meta.longitude >= 0 else "W"
            gps_coords = f"{meta.latitude},{meta.longitude}"
            args.append(f"-GPSLatitude={lat}")
            args.append(f"-GPSLatitudeRef={lat_ref}")
            args.append(f"-GPSLongitude={lon}")
            args.append(f"-GPSLongitudeRef={lon_ref}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:GPSCoordinates={gps_coords}")
                args.append(f"-Keys:Location={gps_coords}")
            if meta.altitude is not None:
                alt = str(abs(meta.altitude))
                alt_ref = "1" if meta.altitude < 0 else "0"
                args.append(f"-GPSAltitude={alt}")
                args.append(f"-GPSAltitudeRef={alt_ref}")
        # Rating
        if meta.favorite:
            args.append("-XMP:Rating=5")
    else:
        # Mode √©crasement : logique compl√®te
        if media_path and _is_video_file(media_path):
            args.extend(["-api", "QuickTimeUTC=1"])
        # Description
        if meta.description:
            safe_desc = _sanitize_description(meta.description)
            args.extend([f"-EXIF:ImageDescription={safe_desc}", f"-XMP-dc:Description={safe_desc}", f"-IPTC:Caption-Abstract={safe_desc}"])
            if media_path and _is_video_file(media_path):
                args.append(f"-Keys:Description={safe_desc}")
        # Vider d'abord les listes puis les remplir
        args.extend(["-XMP-iptcExt:PersonInImage=", "-XMP-dc:Subject=", "-IPTC:Keywords="])
        # Ajouter les personnes
        if meta.people:
            for person in meta.people:
                args.append(f"-XMP-iptcExt:PersonInImage+={person}")
        # Ajouter mots-cl√©s (personnes + albums)
        all_keywords = _build_keywords(meta)
        if all_keywords:
            for keyword in all_keywords:
                args.append(f"-XMP-dc:Subject+={keyword}")
                args.append(f"-IPTC:Keywords+={keyword}")
        # Rating
        if meta.favorite:
            args.append("-XMP:Rating=5")
        # Dates
        if (s := _fmt_dt(meta.taken_at, use_localtime)):
            args.append(f"-DateTimeOriginal={s}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:CreateDate={s}")
        base_ts = meta.created_at or meta.taken_at
        if (s := _fmt_dt(base_ts, use_localtime)):
            args.append(f"-CreateDate={s}")
            args.append(f"-ModifyDate={s}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:ModifyDate={s}")
        # GPS
        if meta.latitude is not None and meta.longitude is not None:
            lat = str(abs(meta.latitude))
            lon = str(abs(meta.longitude))
            lat_ref = "N" if meta.latitude >= 0 else "S"
            lon_ref = "E" if meta.longitude >= 0 else "W"
            gps_coords = f"{meta.latitude},{meta.longitude}"
            args.append(f"-GPSLatitude={lat}")
            args.append(f"-GPSLatitudeRef={lat_ref}")
            args.append(f"-GPSLongitude={lon}")
            args.append(f"-GPSLongitudeRef={lon_ref}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:GPSCoordinates={gps_coords}")
                args.append(f"-Keys:Location={gps_coords}")
            if meta.altitude is not None:
                alt = str(abs(meta.altitude))
                alt_ref = "1" if meta.altitude < 0 else "0"
                args.append(f"-GPSAltitude={alt}")
                args.append(f"-GPSAltitudeRef={alt_ref}")
    return args
def _run_exiftool_command(media_path: Path, args: list[str], _append_only: bool) -> None:
    """Ex√©cute une commande exiftool avec les arguments fournis."""
    if not args:
        return
    cmd = [
        "exiftool",
        "-overwrite_original",
        "-charset", "filename=UTF8",
        "-charset", "iptc=UTF8",
        "-charset", "exif=UTF8",
    ]
    # Ajouter les arguments m√©tadonn√©es
    cmd.extend(args)
    # Ajouter le fichier √† traiter
    cmd.append(str(media_path))
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=60, encoding='utf-8')
        logger.debug("Exiftool output for %s: %s", media_path.name, result.stdout.strip())
    except FileNotFoundError as exc:
        raise RuntimeError("exiftool introuvable") from exc
    except subprocess.CalledProcessError as exc:
        if _append_only and exc.returncode == 2:
            logger.info(f"Aucune m√©tadonn√©e manquante √† √©crire pour {media_path.name} (comportement normal en mode append-only).")
            return
        error_msg = f"exiftool a √©chou√© pour {media_path.name} (code {exc.returncode}): {exc.stderr.strip() or exc.stdout.strip()}"
        raise RuntimeError(error_msg) from exc
````

## File: src/google_takeout_metadata/processor_batch.py
````python
import logging
import subprocess
import tempfile
from pathlib import Path
from typing import List, Tuple
from datetime import datetime
from .exif_writer import build_exiftool_args
from .sidecar import find_albums_for_directory, parse_sidecar
from .processor import IMAGE_EXTS, fix_file_extension_mismatch, _is_sidecar_file 
from . import statistics
logger = logging.getLogger(__name__)
def process_batch(batch: List[Tuple[Path, Path, List[str]]], clean_sidecars: bool) -> int:
    """Traiter un lot de fichiers avec exiftool via un fichier d'arguments."""
    if not batch:
        return 0
    argfile_path = None
    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8', suffix=".txt") as argfile:
            argfile_path = argfile.name
        with open(argfile_path, 'w', encoding='utf-8') as argfile:
            for media_path, _, args in batch:
                for arg in args:
                    argfile.write(f"{arg}\n")
                argfile.write(f"{media_path}\n")
                argfile.write("-execute\n")
        logger.info(f"üì¶ Traitement d'un lot de {len(batch)} fichier(s)...")
        cmd = [
            "exiftool",
            "-overwrite_original",
            "-charset", "filename=UTF8",
            "-charset", "iptc=UTF8",
            "-charset", "exif=UTF8",
            "-@", argfile_path,
        ]
        timeout_seconds = 60 + (len(batch) * 5)
        result = subprocess.run(
            cmd, capture_output=True, text=True, check=True, timeout=timeout_seconds, encoding='utf-8'
        )
        # Analyser la sortie pour compter les fichiers trait√©s
        processed_count = 0
        if result.stdout and result.stdout.strip():
            stdout_lines = result.stdout.strip().split('\n')
            for line in stdout_lines:
                if 'image files updated' in line.lower() or 'files updated' in line.lower():
                    # Extraire le nombre de fichiers mis √† jour
                    try:
                        numbers = [int(word) for word in line.split() if word.isdigit()]
                        if numbers:
                            processed_count = numbers[0]
                    except (ValueError, IndexError):
                        pass
                    logger.info(f"‚úÖ {line.strip()}")
        # Si on n'a pas pu extraire le nombre, utiliser la taille du lot
        if processed_count == 0:
            processed_count = len(batch)
            logger.info(f"‚úÖ Lot de {len(batch)} fichier(s) trait√© avec succ√®s")
        # Mettre √† jour les statistiques pour chaque fichier du lot
        for media_path, _, _ in batch:
            is_image = media_path.suffix.lower() in IMAGE_EXTS
            statistics.stats.add_processed_file(media_path, is_image)
        if clean_sidecars:
            cleaned_count = 0
            for _, json_path, _ in batch:
                try:
                    json_path.unlink()
                    cleaned_count += 1
                except OSError as e:
                    logger.warning(f"√âchec de la suppression du fichier de m√©tadonn√©es {json_path.name}: {e}")
            statistics.stats.sidecars_cleaned += cleaned_count
        return len(batch)
    except FileNotFoundError as exc:
        raise RuntimeError("exiftool introuvable") from exc
    except subprocess.CalledProcessError as exc:
        stderr_msg = exc.stderr or ""
        stdout_msg = exc.stdout or ""
        # Analyser le type d'erreur pour donner un message plus clair
        if "files failed condition" in stderr_msg or "files failed condition" in stdout_msg:
            logger.info(f"‚ÑπÔ∏è Lot trait√© avec conditions non remplies (normal en mode append-only). "
                       f"Certaines m√©tadonn√©es existaient d√©j√† pour {len(batch)} fichier(s).")
            # En mode append-only, consid√©rer ceci comme un succ√®s partiel
            for media_path, _, _ in batch:
                is_image = media_path.suffix.lower() in IMAGE_EXTS
                statistics.stats.add_processed_file(media_path, is_image)
            # Nettoyer les sidecars si demand√© (comme dans le cas de succ√®s normal)
            if clean_sidecars:
                cleaned_count = 0
                for _, json_path, _ in batch:
                    try:
                        json_path.unlink()
                        cleaned_count += 1
                    except OSError as e:
                        logger.warning(f"√âchec de la suppression du fichier de m√©tadonn√©es {json_path.name}: {e}")
                statistics.stats.sidecars_cleaned += cleaned_count
            return len(batch)
        elif "doesn't exist or isn't writable" in stderr_msg:
            logger.warning(f"‚ö†Ô∏è Certains champs de m√©tadonn√©es non support√©s par les fichiers du lot. "
                          f"Normal pour vid√©os ou certains formats. D√©tails: {stderr_msg.strip()}")
            # Consid√©rer comme un succ√®s partiel
            for media_path, _, _ in batch:
                is_image = media_path.suffix.lower() in IMAGE_EXTS  
                statistics.stats.add_processed_file(media_path, is_image)
            # Nettoyer les sidecars si demand√© (comme dans le cas de succ√®s normal)
            if clean_sidecars:
                cleaned_count = 0
                for _, json_path, _ in batch:
                    try:
                        json_path.unlink()
                        cleaned_count += 1
                    except OSError as e:
                        logger.warning(f"√âchec de la suppression du fichier de m√©tadonn√©es {json_path.name}: {e}")
                statistics.stats.sidecars_cleaned += cleaned_count
            return len(batch)
        elif "character(s) could not be encoded" in stderr_msg:
            error_type = "encoding_error"
            error_msg = "Probl√®me d'encodage de caract√®res (√©mojis, accents)"
            logger.warning(f"‚ö†Ô∏è {error_msg}. D√©tails: {stderr_msg.strip()}")
        else:
            error_type = "exiftool_error"
            error_msg = f"Erreur exiftool (code {exc.returncode}): {stderr_msg.strip() or 'Erreur inconnue'}"
            logger.exception(f"‚ùå √âchec du traitement par lot de {len(batch)} fichier(s). {error_msg}")
        # Marquer tous les fichiers du lot comme √©chou√©s
        for media_path, _, _ in batch:
            statistics.stats.add_failed_file(media_path, error_type, error_msg)
        return 0
    finally:
        if argfile_path and Path(argfile_path).exists():
            Path(argfile_path).unlink()
def process_directory_batch(root: Path, use_localtime: bool = False, append_only: bool = True, clean_sidecars: bool = False) -> None:
    """Traiter r√©cursivement tous les fichiers sidecar sous ``root`` par lots."""
    batch: List[Tuple[Path, Path, List[str]]] = []
    BATCH_SIZE = 100
    # Initialiser les statistiques
    statistics.stats.start_processing()
    sidecar_files = [path for path in root.rglob("*.json") if _is_sidecar_file(path)]
    statistics.stats.total_sidecars_found = len(sidecar_files)
    if statistics.stats.total_sidecars_found == 0:
        logger.warning("Aucun fichier de m√©tadonn√©es (.json) trouv√© dans %s", root)
        statistics.stats.end_processing()
        return
    logger.info("üîç Traitement par lots de %d fichier(s) de m√©tadonn√©es dans %s", statistics.stats.total_sidecars_found, root)
    for json_path in sidecar_files:
        try:
            meta = parse_sidecar(json_path)
            directory_albums = find_albums_for_directory(json_path.parent)
            meta.albums.extend(directory_albums)
            media_path = json_path.with_name(meta.filename)
            if not media_path.exists():
                error_msg = f"Fichier image introuvable : {meta.filename}"
                statistics.stats.add_failed_file(json_path, "file_not_found", error_msg)
                logger.warning(f"‚ùå {error_msg}")
                continue
            fixed_media_path, fixed_json_path = fix_file_extension_mismatch(media_path, json_path)
            if fixed_json_path != json_path:
                meta = parse_sidecar(fixed_json_path)
                meta.albums.extend(find_albums_for_directory(fixed_json_path.parent))
            args = build_exiftool_args(
                meta, media_path=fixed_media_path, use_localtime=use_localtime, append_only=append_only
            )
            if args:
                batch.append((fixed_media_path, fixed_json_path, args))
            else:
                # Aucun tag √† √©crire pour ce sidecar
                statistics.stats.total_skipped += 1
                statistics.stats.skipped_files.append(json_path.name)
            if len(batch) >= BATCH_SIZE:
                process_batch(batch, clean_sidecars)
                batch = []
        except (ValueError, RuntimeError) as exc:
            error_msg = f"Erreur de pr√©paration : {exc}"
            statistics.stats.add_failed_file(json_path, "preparation_error", error_msg)
            logger.warning("‚ùå √âchec de la pr√©paration de %s : %s", json_path.name, exc)
    if batch:
        process_batch(batch, clean_sidecars)
    statistics.stats.end_processing()
    # Affichage du r√©sum√©
    statistics.stats.print_console_summary()
    # Cr√©er un dossier logs s'il n'existe pas
    logs_dir = root / "logs"
    logs_dir.mkdir(exist_ok=True)
    # Sauvegarde du rapport d√©taill√© avec un nom incluant la date
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = logs_dir / f"traitement_log_{timestamp}.json"
    statistics.stats.save_detailed_report(log_file)
````

## File: src/google_takeout_metadata/processor.py
````python
"""Traitement de haut niveau des r√©pertoires contenant des m√©tadonn√©es Google Takeout."""
from __future__ import annotations
from pathlib import Path
import logging
import json
import subprocess
from datetime import datetime
from .sidecar import parse_sidecar, find_albums_for_directory
from .exif_writer import write_metadata
from . import statistics
logger = logging.getLogger(__name__)
# S√©parer les extensions images et vid√©os pour une meilleure coh√©rence
IMAGE_EXTS = {".jpg", ".jpeg", ".png", ".gif", ".webp", ".heic", ".heif", ".avif"}
VIDEO_EXTS = {".mp4", ".mov", ".m4v", ".3gp"}
ALL_MEDIA_EXTS = IMAGE_EXTS | VIDEO_EXTS
def detect_file_type(file_path: Path) -> str | None:
    """D√©tecter le type r√©el du fichier via la commande ``file`` ou les octets magiques.
    Retourne:
        L'extension correcte (avec point) ou ``None`` si la d√©tection √©choue
    """
    try:
        # Essayer d'abord la commande ``file`` (disponible sur la plupart des syst√®mes)
        result = subprocess.run(
            ["file", str(file_path)], 
            capture_output=True, 
            text=True, 
            timeout=10
        )
        if result.returncode == 0:
            output = result.stdout.lower()
            if "jpeg" in output or "jfif" in output:
                return ".jpg"
            elif "png" in output:
                return ".png"
            elif "gif" in output:
                return ".gif"
            elif "webp" in output:
                return ".webp"
            elif "heic" in output:
                return ".heic"
            elif "heif" in output:
                return ".heif"
            elif "mp4" in output:
                return ".mp4"
            elif "quicktime" in output or "mov" in output:
                return ".mov"
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
        pass
    # Repli : lecture des octets magiques
    try:
        with open(file_path, "rb") as f:
            header = f.read(16)
            if header.startswith(b'\xff\xd8\xff'):
                return ".jpg"
            elif header.startswith(b'\x89PNG\r\n\x1a\n'):
                return ".png"
            elif header.startswith(b'GIF8'):
                return ".gif"
            elif header.startswith(b'RIFF') and b'WEBP' in header:
                return ".webp"
            elif header[4:8] == b'ftyp':
                if b'heic' in header[:16] or b'mif1' in header[:16]:
                    return ".heic"
                elif b'mp4' in header[:16] or b'isom' in header[:16]:
                    return ".mp4"
    except (OSError, IOError):
        pass
    return None
def fix_file_extension_mismatch(media_path: Path, json_path: Path) -> tuple[Path, Path]:
    """Corriger une incoh√©rence d'extension en renommant les fichiers et en mettant √† jour le JSON.
    Args:
        media_path: Chemin du fichier image/vid√©o
        json_path: Chemin du fichier JSON associ√© (sidecar)
    Retourne:
        Un tuple ``(new_media_path, new_json_path)``
    """
    # D√©tecter le type r√©el du fichier
    actual_ext = detect_file_type(media_path)
    if not actual_ext or actual_ext == media_path.suffix.lower():
        # Aucune incoh√©rence d√©tect√©e ou la d√©tection a √©chou√©
        return media_path, json_path
    # Cr√©er de nouveaux chemins avec la bonne extension
    new_media_path = media_path.with_suffix(actual_ext)
    new_json_path = json_path.with_name(new_media_path.name + ".supplemental-metadata.json")
    logger.info("üîß Extension incorrecte d√©tect√©e pour %s (devrait √™tre %s). Correction automatique...", 
                media_path.name, actual_ext)
    image_renamed = False
    try:
        # Renommer le fichier image
        media_path.rename(new_media_path)
        image_renamed = True
        logger.info("‚úÖ Fichier renomm√© : %s ‚Üí %s", media_path.name, new_media_path.name)
        # Mettre √† jour le contenu JSON et renommer le fichier JSON
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        # Mettre √† jour le champ title
        json_data['title'] = new_media_path.name
        # √âcrire le JSON mis √† jour au nouvel emplacement
        with open(new_json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        # Supprimer l'ancien fichier JSON
        json_path.unlink()
        logger.info("‚úÖ M√©tadonn√©es mises √† jour : %s ‚Üí %s", json_path.name, new_json_path.name)
        # Enregistrer la correction dans les statistiques
        statistics.stats.add_fixed_extension(media_path.name, new_media_path.name)
        return new_media_path, new_json_path
    except (OSError, IOError, json.JSONDecodeError) as exc:
        logger.warning("‚ùå √âchec de la correction d'extension pour %s : %s. "
                       "Le fichier sera trait√© avec son extension actuelle.", media_path.name, exc)
        # Si l'image a √©t√© renomm√©e mais que des √©tapes ult√©rieures √©chouent, tenter un rollback
        if image_renamed:
            try:
                # Supprimer tout nouveau JSON √©ventuellement cr√©√©
                if new_json_path.exists():
                    new_json_path.unlink()
                    logger.info("üîÑ Fichier JSON partiellement cr√©√© supprim√© : %s", new_json_path.name)
                # Renommer l'image avec son nom d'origine
                new_media_path.rename(media_path)
                logger.info("üîÑ Annulation du renommage : %s ‚Üí %s", new_media_path.name, media_path.name)
                return media_path, json_path
            except (OSError, IOError) as rollback_exc:
                logger.exception("‚ùå √âchec de l'annulation du renommage %s ‚Üí %s : %s. "
                           "ATTENTION : √âtat incoh√©rent - fichier image renomm√© mais JSON non mis √† jour.", 
                           new_media_path.name, media_path.name, rollback_exc)
                # Retourner les nouveaux chemins afin de refl√©ter l'√©tat courant
                return new_media_path, json_path
        return media_path, json_path
def _is_sidecar_file(path: Path) -> bool:
    """V√©rifier si un fichier peut √™tre un JSON annexe (Google Photos).
    Cette fonction est permissive car ``parse_sidecar()`` fait une
    validation stricte en comparant le champ ``title`` avec le nom attendu.
    Formats support√©s :
    - Nouveau format : photo.jpg.supplemental-metadata.json
    - Ancien format : photo.jpg.json
    """
    if not path.suffix.lower() == ".json":
        return False
    suffixes = [s.lower() for s in path.suffixes]
    # Nouveau format Google Takeout : photo.jpg.supplemental-metadata.json
    if len(suffixes) >= 3 and suffixes[-2] == ".supplemental-metadata" and suffixes[-3] in ALL_MEDIA_EXTS:
        return True
    # Format h√©rit√© : photo.jpg.json
    if len(suffixes) >= 2 and suffixes[-2] in ALL_MEDIA_EXTS:
        return True
    # Format plus ancien : photo.json (moins sp√©cifique mais valid√© ult√©rieurement)
    # Ne consid√©rer ceci que si le nom de base sans .json pourrait √™tre une image
    stem_parts = path.stem.split('.')
    if len(stem_parts) >= 2:
        potential_ext = '.' + stem_parts[-1].lower()
        if potential_ext in ALL_MEDIA_EXTS:
            return True
    return False
def process_sidecar_file(json_path: Path, use_localtime: bool = False, append_only: bool = True, clean_sidecars: bool = False) -> None:
    """Traiter un fichier annexe ``.json``.
    Args:
        json_path: Chemin du fichier JSON annexe
        use_localtime: Convertir les dates en heure locale au lieu d'UTC
        append_only: Ajouter uniquement les champs manquants
        clean_sidecars: Supprimer le JSON apr√®s un traitement r√©ussi
    """
    try:
        meta = parse_sidecar(json_path)
    except ValueError as exc:
        statistics.stats.add_failed_file(json_path, "parse_error", f"Erreur de lecture JSON : {exc}")
        raise
    # Trouver les albums du r√©pertoire
    directory_albums = find_albums_for_directory(json_path.parent)
    meta.albums.extend(directory_albums)
    media_path = json_path.with_name(meta.filename)
    if not media_path.exists():
        error_msg = f"Fichier image introuvable : {meta.filename}"
        statistics.stats.add_failed_file(json_path, "file_not_found", error_msg)
        raise FileNotFoundError(error_msg)
    # D√©tecter le type de fichier (image ou vid√©o)
    is_image = media_path.suffix.lower() in IMAGE_EXTS
    # Tenter d'√©crire les m√©tadonn√©es dans l'image
    try:
        write_metadata(media_path, meta, use_localtime=use_localtime, append_only=append_only)
        current_json_path = json_path
        # Enregistrer le succ√®s
        statistics.stats.add_processed_file(media_path, is_image)
    except RuntimeError as exc:
        # V√©rifier s'il s'agit d'une erreur d'incoh√©rence d'extension
        error_msg = str(exc).lower()
        if ("not a valid png" in error_msg and "looks more like a jpeg" in error_msg) or \
           ("not a valid jpeg" in error_msg and "looks more like a png" in error_msg) or \
           ("charset option" in error_msg):
            logger.info("üîç Extension possiblement incorrecte pour %s. Tentative de correction...", media_path.name)
            # Tenter de corriger l'incoh√©rence d'extension
            fixed_media_path, fixed_json_path = fix_file_extension_mismatch(media_path, json_path)
            if fixed_media_path != media_path or fixed_json_path != json_path:
                # Les fichiers ont √©t√© renomm√©s (au moins partiellement), re-analyser le JSON et r√©essayer
                # G√©rer le cas o√π l'image a √©t√© renomm√©e mais pas le JSON (√©chec de rollback partiel)
                is_image_after_fix = fixed_media_path.suffix.lower() in IMAGE_EXTS
                actual_json_path = fixed_json_path if fixed_json_path.exists() else json_path
                meta = parse_sidecar(actual_json_path)
                directory_albums = find_albums_for_directory(actual_json_path.parent)
                meta.albums.extend(directory_albums)
                write_metadata(fixed_media_path, meta, use_localtime=use_localtime, append_only=append_only)
                current_json_path = actual_json_path
                # Enregistrer le succ√®s apr√®s correction
                statistics.stats.add_processed_file(fixed_media_path, is_image_after_fix)
                logger.info("‚úÖ Traitement r√©ussi de %s apr√®s correction d'extension", fixed_media_path.name)
            else:
                # √âchec de la correction d'extension, relancer l'erreur originale
                statistics.stats.add_failed_file(media_path, "extension_mismatch", str(exc))
                raise
        else:
            # Ce n'est pas une erreur d'incoh√©rence d'extension, relancer
            statistics.stats.add_failed_file(media_path, "metadata_write_error", str(exc))
            raise
    # Nettoyer le sidecar si demand√© et si l'√©criture a r√©ussi
    if clean_sidecars:
        try:
            current_json_path.unlink()
            statistics.stats.sidecars_cleaned += 1
            logger.info("üóëÔ∏è Fichier de m√©tadonn√©es supprim√© : %s", current_json_path.name)
        except OSError as exc:
            logger.warning("√âchec de la suppression du fichier de m√©tadonn√©es %s : %s", current_json_path, exc)
def process_directory(root: Path, use_localtime: bool = False, append_only: bool = True, clean_sidecars: bool = False) -> None:
    """Traiter r√©cursivement tous les fichiers annexes sous ``root``.
    Args:
        root: R√©pertoire racine √† parcourir r√©cursivement
        use_localtime: Convertir les dates en heure locale au lieu d'UTC
        append_only: Ajouter uniquement les champs manquants
        clean_sidecars: Supprimer les JSON apr√®s un traitement r√©ussi
    """
    # Initialiser les statistiques
    statistics.stats.start_processing()
    sidecar_files = [path for path in root.rglob("*.json") if _is_sidecar_file(path)]
    statistics.stats.total_sidecars_found = len(sidecar_files)
    if statistics.stats.total_sidecars_found == 0:
        logger.warning("Aucun fichier de m√©tadonn√©es (.json) trouv√© dans %s", root)
        statistics.stats.end_processing()
        return
    logger.info("üîç Traitement de %d fichier(s) de m√©tadonn√©es dans %s", statistics.stats.total_sidecars_found, root)
    for json_file in sidecar_files:
        try:
            process_sidecar_file(json_file, use_localtime=use_localtime, append_only=append_only, clean_sidecars=clean_sidecars)
        except (FileNotFoundError, ValueError, RuntimeError) as exc:
            logger.warning("‚ùå √âchec du traitement de %s : %s", json_file.name, exc)
            # Les statistiques sont d√©j√† mises √† jour dans process_sidecar_file
    statistics.stats.end_processing()
    # Affichage du r√©sum√©
    statistics.stats.print_console_summary()
    # Cr√©er un dossier logs s'il n'existe pas
    logs_dir = root / "logs"
    logs_dir.mkdir(exist_ok=True)
    # Sauvegarde du rapport d√©taill√© avec un nom incluant la date
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = logs_dir / f"traitement_log_{timestamp}.json"
    statistics.stats.save_detailed_report(log_file)
````

## File: src/google_takeout_metadata/sidecar.py
````python
from __future__ import annotations
from dataclasses import dataclass, field
from pathlib import Path
import json
import logging
from typing import List, Optional
"""Analyse des fichiers annexes JSON de Google Takeout."""
logger = logging.getLogger(__name__)
@dataclass
class SidecarData:
    """M√©tadonn√©es s√©lectionn√©es extraites d'un JSON annexe Google Photos."""
    filename: str
    description: Optional[str]
    people: List[str]
    taken_at: Optional[int]
    created_at: Optional[int]
    latitude: Optional[float]
    longitude: Optional[float]
    altitude: Optional[float]
    favorite: bool = False
    lat_span: Optional[float] = None
    lon_span: Optional[float] = None
    albums: List[str] = field(default_factory=list)
    archived: bool = False
def parse_sidecar(path: Path) -> SidecarData:
    """Analyser ``path`` et retourner :class:`SidecarData`.
    La fonction v√©rifie que le champ ``title`` int√©gr√© correspond au nom de fichier
    du sidecar pour √©viter d'appliquer des m√©tadonn√©es au mauvais m√©dia.
    Formats support√©s :
    - Nouveau format : photo.jpg.supplemental-metadata.json -> title attendu "photo.jpg"
    - Ancien format : photo.jpg.json -> title attendu "photo.jpg"
    """
    try:
        with path.open("r", encoding="utf-8") as fh:
            data = json.load(fh)
    except FileNotFoundError as exc:  # pragma: no cover - simple wrapper
        raise FileNotFoundError(f"Sidecar introuvable : {path}") from exc
    except json.JSONDecodeError as exc:
        raise ValueError(f"JSON invalide dans {path}") from exc
    title = data.get("title")
    if not title:
        raise ValueError(f"Champ 'title' manquant dans {path}")
    # Extraire le nom de fichier attendu depuis le chemin du sidecar
    # Pour le nouveau format : IMG_001.jpg.supplemental-metadata.json -> titre attendu : IMG_001.jpg
    # Pour le format h√©rit√© : IMG_001.jpg.json -> titre attendu : IMG_001.jpg
    if path.name.lower().endswith(".supplemental-metadata.json"):
        expected_title = path.name[:-len(".supplemental-metadata.json")]
    elif path.name.lower().endswith(".supplemental-metadat.json"):
        expected_title = path.name[:-len(".supplemental-metadat.json")]
    elif path.name.lower().endswith(".supplemental-me.json"):
        expected_title = path.name[:-len(".supplemental-me.json")]
    elif path.name.lower().endswith(".supplemental-meta.json"):
        expected_title = path.name[:-len(".supplemental-meta.json")]
    elif path.name.lower().endswith(".json"):
        expected_title = path.stem
    else:
        expected_title = path.stem
    if expected_title != title:
        raise ValueError(
            f"Le titre du sidecar {title!r} ne correspond pas au nom de fichier attendu {expected_title!r} provenant de {path.name!r}"
        )
    description = data.get("description")
    # Extraire les noms de personnes, supprimer les espaces et d√©dupliquer
    # people est [{ "name": "X" }]
    raw_people = data.get("people", []) or []
    people = []
    for p in raw_people:
        if isinstance(p, dict):
            if isinstance(p.get("name"), str):
                people.append(p["name"].strip())
    # d√©duplication
    people = sorted(set(filter(None, people)))
    def get_ts(key: str) -> Optional[int]:
        ts = data.get(key, {}).get("timestamp")
        if ts is None:
            return None
        try:
            return int(ts)
        except (TypeError, ValueError):
            return None
    taken_at = get_ts("photoTakenTime")
    created_at = get_ts("creationTime")
    # Extraire les donn√©es g√©ographiques - pr√©f√©rer geoData, repli sur geoDataExif
    geo = data.get("geoData", {})
    if not geo or not geo.get("latitude"):
        geo = data.get("geoDataExif", {})
    latitude = geo.get("latitude")
    longitude = geo.get("longitude")
    altitude = geo.get("altitude")
    lat_span = geo.get("latitudeSpan")
    lon_span = geo.get("longitudeSpan")
    # Nettoyer les coordonn√©es seulement si les DEUX sont √† 0/None
    # Conserver les vraies coordonn√©es 0.0 car elles peuvent √™tre valides (√©quateur/m√©ridien de Greenwich)
    # Google met parfois 0/0 quand pas de g√©o fiable ‚Üí on nettoie uniquement dans ce cas
    if ((latitude in (0, 0.0, None)) and (longitude in (0, 0.0, None))) or \
       (latitude is None or longitude is None):
        latitude = longitude = altitude = None
    # Extraire le statut favori - format bool√©en Google Takeout
    # Note : "favorited": true si favori, champ absent si pas favori (pas false)
    favorite = bool(data.get("favorited", False))
    # Extraire le statut archiv√©
    archived = bool(data.get("archived", False))
    return SidecarData(
        filename=title,
        description=description,
        people=people,
        taken_at=taken_at,
        created_at=created_at,
        latitude=latitude,
        longitude=longitude,
        altitude=altitude,
        favorite=favorite,
        lat_span=lat_span,
        lon_span=lon_span,
        archived=archived,
    )
def parse_album_metadata(path: Path) -> List[str]:
    """Analyser un fichier metadata.json d'album et retourner la liste des noms d'albums.
    Les fichiers metadata.json d'albums (Google Takeout) contiennent g√©n√©ralement :
    {
        "title": "Nom de l'album",
        "description": "...",
        ...
    }
    Retourne une liste des noms d'albums trouv√©s dans le fichier.
    """
    try:
        with path.open("r", encoding="utf-8") as fh:
            data = json.load(fh)
    except (FileNotFoundError, json.JSONDecodeError):
        return []
    albums = []
    # Nom d'album principal depuis le champ title
    title = data.get("title")
    if title and isinstance(title, str):
        albums.append(title.strip())
    # Certains fichiers metadata.json peuvent avoir plusieurs r√©f√©rences d'albums
    # V√©rifier s'il y a des r√©f√©rences d'albums dans d'autres champs
    album_refs = data.get("albums", [])
    if isinstance(album_refs, list):
        for album_ref in album_refs:
            if isinstance(album_ref, dict) and "title" in album_ref:
                album_name = album_ref["title"]
                if isinstance(album_name, str):
                    albums.append(album_name.strip())
            elif isinstance(album_ref, str):
                albums.append(album_ref.strip())
    # Supprimer les doublons et les cha√Ænes vides
    albums = sorted(set(filter(None, albums)))
    return albums
def find_albums_for_directory(directory: Path, max_depth: int = 5) -> List[str]:
    """Trouver tous les noms d'albums applicables aux photos du r√©pertoire donn√©.
    Recherche des fichiers metadata.json dans le r√©pertoire et ses parents
    pour collecter les informations d'album.
    Args:
        directory: R√©pertoire de d√©part pour la recherche
        max_depth: Nombre maximum de niveaux parents √† v√©rifier (d√©faut: 5)
    Prend en charge plusieurs motifs de fichiers metadata :
    - metadata.json (anglais)
    - m√©tadonn√©es.json (fran√ßais)  
    - m√©tadonn√©es(1).json, m√©tadonn√©es(2).json, etc. (fran√ßais avec doublons)
    - album_metadata.json, folder_metadata.json (h√©rit√©s)
    """
    albums = []
    metadata_patterns = [
        "metadata.json",
        "m√©tadonn√©es.json", 
        "album_metadata.json", 
        "folder_metadata.json"
    ]
    # Rechercher dans le r√©pertoire courant et ses parents avec limite de profondeur
    current_dir = directory
    depth = 0
    # Motifs de r√©pertoires marqueurs (insensibles √† la casse)
    takeout_markers = ["google photos", "takeout", "google takeout"]
    while current_dir != current_dir.parent and depth < max_depth:
        # V√©rifier les motifs standards (insensible √† la casse)
        for pattern in metadata_patterns:
            # Rechercher le fichier avec la casse exacte d'abord
            metadata_file = current_dir / pattern
            if metadata_file.exists():
                try:
                    albums.extend(parse_album_metadata(metadata_file))
                except (OSError, PermissionError) as e:
                    # Ignorer les erreurs de parsing et continuer
                    logger.debug(f"Erreur lors du parsing de {metadata_file}: {e}")
            else:
                # Rechercher de mani√®re insensible √† la casse si pas trouv√©
                try:
                    for existing_file in current_dir.iterdir():
                        if existing_file.is_file() and existing_file.name.lower() == pattern.lower():
                            try:
                                albums.extend(parse_album_metadata(existing_file))
                            except (OSError, PermissionError) as e:
                                logger.debug(f"Erreur lors du parsing de {existing_file}: {e}")
                            break  # Un seul fichier correspondant par motif
                except (OSError, PermissionError):
                    # Ignorer les erreurs d'acc√®s au r√©pertoire
                    logger.debug(f"Impossible d'acc√©der au r√©pertoire {current_dir}")
        # V√©rifier les variations num√©rot√©es comme m√©tadonn√©es(1).json, m√©tadonn√©es(2).json, etc.
        # (recherche insensible √† la casse)
        try:
            for metadata_file in current_dir.iterdir():
                if (metadata_file.is_file() and 
                    metadata_file.name.lower().startswith("m√©tadonn√©es") and 
                    metadata_file.name.lower().endswith(".json") and
                    metadata_file.name.lower() not in ["m√©tadonn√©es.json"]):  # d√©j√† v√©rifi√© ci-dessus
                    try:
                        albums.extend(parse_album_metadata(metadata_file))
                    except (OSError, PermissionError) as e:
                        # Ignorer les erreurs de parsing et continuer
                        logger.debug(f"Erreur lors du parsing de {metadata_file}: {e}")
        except (OSError, PermissionError):
            # Ignorer les erreurs d'acc√®s au r√©pertoire et continuer
            logger.debug(f"Impossible d'acc√©der au r√©pertoire {current_dir}")
        # Arr√™ter si on atteint un r√©pertoire "marqueur" de Google Takeout
        # pour √©viter de remonter trop haut dans l'arborescence
        if any(marker in current_dir.name.lower() for marker in takeout_markers):
            logger.debug(f"Arr√™t de la recherche d'albums au r√©pertoire marqueur: {current_dir}")
            break
        # Remonter au r√©pertoire parent
        current_dir = current_dir.parent
        depth += 1
    # D√©duplication et tri tout en pr√©servant l'ordre de priorit√©
    # (r√©pertoires plus proches en premier)
    unique_albums = []
    seen = set()
    for album in albums:
        if album not in seen:
            unique_albums.append(album)
            seen.add(album)
    return unique_albums
````

## File: src/google_takeout_metadata/statistics.py
````python
"""Module de gestion des statistiques et rapport de synth√®se."""
from __future__ import annotations
import logging
from datetime import datetime
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Dict, Optional
import json
logger = logging.getLogger(__name__)
@dataclass
class ProcessingStats:
    """Statistiques de traitement des fichiers."""
    # Totaux
    total_sidecars_found: int = 0
    total_processed: int = 0
    total_failed: int = 0
    total_skipped: int = 0
    # Par type de fichier
    images_processed: int = 0
    videos_processed: int = 0
    # D√©tails des op√©rations
    files_fixed_extension: int = 0
    sidecars_cleaned: int = 0
    # Listes de d√©tails pour le rapport d√©taill√©
    failed_files: List[str] = field(default_factory=list)
    skipped_files: List[str] = field(default_factory=list)
    fixed_extensions: List[str] = field(default_factory=list)
    # Erreurs par cat√©gorie
    errors_by_type: Dict[str, int] = field(default_factory=dict)
    # Timing
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    def start_processing(self) -> None:
        """Marquer le d√©but du traitement."""
        self.start_time = datetime.now()
    def end_processing(self) -> None:
        """Marquer la fin du traitement."""
        self.end_time = datetime.now()
    @property
    def duration(self) -> Optional[float]:
        """Dur√©e du traitement en secondes."""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None
    @property
    def success_rate(self) -> float:
        """Taux de r√©ussite en pourcentage."""
        if self.total_sidecars_found == 0:
            return 0.0
        return (self.total_processed / self.total_sidecars_found) * 100
    def add_processed_file(self, file_path: Path, is_image: bool = True) -> None:
        """Ajouter un fichier trait√© avec succ√®s."""
        self.total_processed += 1
        if is_image:
            self.images_processed += 1
        else:
            self.videos_processed += 1
    def add_failed_file(self, file_path: Path, error_type: str, error_msg: str) -> None:
        """Ajouter un fichier en √©chec."""
        self.total_failed += 1
        self.failed_files.append(f"{file_path.name}: {error_msg}")
        # Compter les erreurs par type
        if error_type in self.errors_by_type:
            self.errors_by_type[error_type] += 1
        else:
            self.errors_by_type[error_type] = 1
    def add_skipped_file(self, file_path: Path, reason: str) -> None:
        """Ajouter un fichier ignor√©."""
        self.total_skipped += 1
        self.skipped_files.append(f"{file_path.name}: {reason}")
    def add_fixed_extension(self, old_name: str, new_name: str) -> None:
        """Ajouter une correction d'extension."""
        self.files_fixed_extension += 1
        self.fixed_extensions.append(f"{old_name} ‚Üí {new_name}")
    def print_console_summary(self) -> None:
        """Afficher un r√©sum√© concis dans la console."""
        print("\n" + "="*60)
        print("üìä R√âSUM√â DU TRAITEMENT")
        print("="*60)
        print(f"üìÅ Fichiers de m√©tadonn√©es trouv√©s : {self.total_sidecars_found}")
        print(f"‚úÖ Fichiers trait√©s avec succ√®s : {self.total_processed}")
        if self.images_processed > 0 or self.videos_processed > 0:
            print(f"   üì∏ Images : {self.images_processed}")
            print(f"   üé• Vid√©os : {self.videos_processed}")
        if self.total_failed > 0:
            print(f"‚ùå Fichiers en √©chec : {self.total_failed}")
        if self.total_skipped > 0:
            print(f"‚è≠Ô∏è  Fichiers ignor√©s : {self.total_skipped}")
        if self.files_fixed_extension > 0:
            print(f"üîß Extensions corrig√©es : {self.files_fixed_extension}")
        if self.sidecars_cleaned > 0:
            print(f"üóëÔ∏è  Fichiers de m√©tadonn√©es supprim√©s : {self.sidecars_cleaned}")
        # Taux de r√©ussite
        if self.total_sidecars_found > 0:
            print(f"üìà Taux de r√©ussite : {self.success_rate:.1f}%")
        # Dur√©e
        if self.duration:
            print(f"‚è±Ô∏è  Dur√©e : {self.duration:.1f}s")
        # Erreurs principales
        if self.errors_by_type:
            print("\nüîç Types d'erreurs principales :")
            for error_type, count in sorted(self.errors_by_type.items(), key=lambda x: x[1], reverse=True)[:3]:
                print(f"   ‚Ä¢ {error_type}: {count} fichier(s)")
        print("="*60)
        if self.total_failed > 0 or self.total_skipped > 0:
            print("üí° Consultez le fichier de log d√©taill√© pour plus d'informations.")
    def save_detailed_report(self, log_file: Path) -> None:
        """Sauvegarder un rapport d√©taill√© dans un fichier sp√©cifique √† cette ex√©cution."""
        report = {
            "execution_timestamp": datetime.now().isoformat(),
            "summary": {
                "total_sidecars_found": self.total_sidecars_found,
                "total_processed": self.total_processed,
                "total_failed": self.total_failed,
                "total_skipped": self.total_skipped,
                "images_processed": self.images_processed,
                "videos_processed": self.videos_processed,
                "files_fixed_extension": self.files_fixed_extension,
                "sidecars_cleaned": self.sidecars_cleaned,
                "success_rate": self.success_rate,
                "duration_seconds": self.duration
            },
            "details": {
                "failed_files": self.failed_files,
                "skipped_files": self.skipped_files,
                "fixed_extensions": self.fixed_extensions,
                "errors_by_type": self.errors_by_type
            }
        }
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"üìÑ Rapport d√©taill√© sauvegard√© : {log_file}")
        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde du rapport : {e}")
# Instance globale pour les statistiques
stats = ProcessingStats()
````

## File: test_assets/README.md
````markdown
# Assets de Test

Ce dossier contient des fichiers de r√©f√©rence pour les tests d'int√©gration.

## Fichiers disponibles

### Images
- **test_clean.jpg** : Image JPEG 100x100 sans m√©tadonn√©es (nettoye avec `exiftool -all=`)
- **test_with_metadata.jpg** : Image JPEG 100x100 avec m√©tadonn√©es de base
  - Description : "Existing description"
  - Rating : 3
  - Keywords : "Existing keyword"

### Vid√©os
- **test_video_clean.mp4** : Vid√©o MP4 sans m√©tadonn√©es (nettoy√©e avec `exiftool -all=`)
- **test_video_with_metadata.mp4** : Vid√©o MP4 avec m√©tadonn√©es de base
  - Description : "Existing video description"
  - DateTimeOriginal : "2020:01:01 12:00:00"
  - Keywords : "Existing video keyword"

## Utilisation dans les tests

Les tests d'int√©gration utilisent la fonction `_copy_test_asset(asset_name, dest_path)` pour copier ces fichiers vers des r√©pertoires temporaires avant chaque test. Cela garantit :

1. **Reproductibilit√©** : Chaque test commence avec les m√™mes conditions
2. **Isolation** : Les tests ne s'interf√®rent pas entre eux
3. **Contr√¥le** : Les m√©tadonn√©es existantes sont connues et pr√©visibles

## R√©g√©n√©ration des assets

Si n√©cessaire, les assets peuvent √™tre r√©g√©n√©r√©s avec :

```bash
# Se placer dans le dossier test_assets
cd test_assets

# Cr√©er les images
python -c "
from PIL import Image
img = Image.new('RGB', (100, 100), color='red')
img.save('test_clean.jpg')
img2 = Image.new('RGB', (100, 100), color='blue') 
img2.save('test_with_metadata.jpg')
"

# Copier la vid√©o de r√©f√©rence
cp "../Google Photos/essais/1686356837983.mp4" "test_video_clean.mp4"
cp "test_video_clean.mp4" "test_video_with_metadata.mp4"

# Nettoyer les fichiers clean
exiftool -overwrite_original -all= test_clean.jpg test_video_clean.mp4

# Ajouter des m√©tadonn√©es aux fichiers with_metadata
exiftool -overwrite_original -EXIF:ImageDescription="Existing description" -XMP:Rating=3 -IPTC:Keywords="Existing keyword" test_with_metadata.jpg
exiftool -overwrite_original -api QuickTimeUTC=1 -XMP-dc:Description="Existing video description" -DateTimeOriginal="2020:01:01 12:00:00" -IPTC:Keywords="Existing video keyword" test_video_with_metadata.mp4
```
````

## File: tests/test_hybrid_approach.py
````python
import shutil
import tempfile
import subprocess
from pathlib import Path
from google_takeout_metadata.sidecar import SidecarData
from google_takeout_metadata.exif_writer import write_metadata
def read_exif_people(image_path: Path) -> list[str]:
    """Lit les personnes depuis un fichier image en utilisant exiftool."""
    try:
        cmd = ["exiftool", "-s", "-s", "-s", "-XMP-iptcExt:PersonInImage", str(image_path)]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30, encoding='utf-8')
        if result.returncode == 0 and result.stdout.strip():
            # ExifTool retourne les valeurs s√©par√©es par des virgules
            people = [p.strip() for p in result.stdout.strip().split(',')]
            return [p for p in people if p]  # Filtrer les valeurs vides
    except (subprocess.SubprocessError, OSError):
        pass
    return []
def test_hybrid_approach_no_duplicates():
    """Test de l'approche hybride : pas de doublons quand on ajoute des personnes existantes."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Copier l'image test dans le r√©pertoire temporaire
        test_image_src = Path("test_assets/test_clean.jpg")
        test_image = Path(temp_dir) / "test_image.jpg"
        shutil.copy2(test_image_src, test_image)
        # √âtape 1 : Ajouter les premi√®res personnes (ancien takeout)
        meta1 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Anthony", "Bernard"],
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False,
            albums=["Vacances"]
        )
        write_metadata(test_image, meta1, append_only=True)
        # V√©rifier l'√©tat initial
        people_initial = read_exif_people(test_image)
        assert "Anthony" in people_initial
        assert "Bernard" in people_initial
        assert len([p for p in people_initial if p == "Anthony"]) == 1
        assert len([p for p in people_initial if p == "Bernard"]) == 1
        # √âtape 2 : Ajouter nouveaux + existants (nouveau takeout avec tous les gens)
        meta2 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Anthony", "Bernard", "Cindy"],  # Contient TOUS les gens, pas juste les nouveaux
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False,
            albums=["Vacances", "Famille"]
        )
        write_metadata(test_image, meta2, append_only=True)
        # V√©rifier le r√©sultat final : pas de doublons malgr√© la redondance
        people_final = read_exif_people(test_image)
        print(f"Personnes finales: {people_final}")
        # Assertions critiques : aucun doublon
        assert "Anthony" in people_final
        assert "Bernard" in people_final 
        assert "Cindy" in people_final
        assert len([p for p in people_final if p == "Anthony"]) == 1, f"Anthony appara√Æt plusieurs fois: {people_final}"
        assert len([p for p in people_final if p == "Bernard"]) == 1, f"Bernard appara√Æt plusieurs fois: {people_final}"
        assert len([p for p in people_final if p == "Cindy"]) == 1, f"Cindy appara√Æt plusieurs fois: {people_final}"
        # V√©rifier que toutes les personnes attendues sont pr√©sentes
        expected_people = {"Anthony", "Bernard", "Cindy"}
        actual_people = set(people_final)
        assert expected_people.issubset(actual_people), f"Personnes manquantes. Attendu: {expected_people}, R√©el: {actual_people}"
def test_hybrid_approach_only_new_people():
    """Test de l'approche hybride : ajouter seulement les nouvelles personnes."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Copier l'image test
        test_image_src = Path("test_assets/test_clean.jpg")
        test_image = Path(temp_dir) / "test_image.jpg"
        shutil.copy2(test_image_src, test_image)
        # √âtape 1 : Ajouter les premi√®res personnes
        meta1 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Alice", "Bob"],
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False
        )
        write_metadata(test_image, meta1, append_only=True)
        # √âtape 2 : Ajouter seulement les nouvelles personnes 
        meta2 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Charlie"],  # Seulement la nouvelle personne
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False
        )
        write_metadata(test_image, meta2, append_only=True)
        # V√©rifier le r√©sultat : toutes les personnes pr√©sentes, pas de doublons
        people_final = read_exif_people(test_image)
        expected_people = {"Alice", "Bob", "Charlie"}
        actual_people = set(people_final)
        assert expected_people == actual_people, f"Attendu: {expected_people}, R√©el: {actual_people}"
        assert len(people_final) == 3, f"Doublons d√©tect√©s: {people_final}"
if __name__ == "__main__":
    test_hybrid_approach_no_duplicates()
    test_hybrid_approach_only_new_people()
    print("‚úÖ Tests de l'approche hybride : SUCC√àS")
````

## File: tests/test_cli.py
````python
"""Tests pour l'interface en ligne de commande."""
import json
import subprocess
import sys
from unittest.mock import patch
import pytest
from PIL import Image
from google_takeout_metadata.cli import main
def test_main_no_args(capsys):
    """Tester que la CLI sans arguments affiche l'aide."""
    with pytest.raises(SystemExit):
        main([])
    captured = capsys.readouterr()
    assert "usage:" in captured.err
def test_main_help(capsys):
    """Tester l'option d'aide de la CLI."""
    with pytest.raises(SystemExit):
        main(["--help"])
    captured = capsys.readouterr()
    assert "Fusionner les m√©tadonn√©es Google Takeout dans les images" in captured.out
def test_main_invalid_directory(capsys, tmp_path):
    """Tester la CLI avec un r√©pertoire inexistant."""
    non_existent = tmp_path / "does_not_exist"
    with pytest.raises(SystemExit):
        main([str(non_existent)])
    # L'erreur est enregistr√©e mais pas affich√©e sur stderr avec la configuration actuelle
    # Donc nous ne v√©rifions pas la sortie captur√©e, juste qu'elle se termine
def test_main_file_instead_of_directory(capsys, tmp_path):
    """Tester la CLI avec un chemin de fichier au lieu d'un r√©pertoire."""
    test_file = tmp_path / "test.txt"
    test_file.write_text("test")
    with pytest.raises(SystemExit):
        main([str(test_file)])
    # L'erreur est enregistr√©e mais pas affich√©e sur stderr avec la configuration actuelle
    # Donc nous ne v√©rifions pas la sortie captur√©e, juste qu'elle se termine
@patch('google_takeout_metadata.cli.process_directory')
def test_main_normal_mode(mock_process_directory, tmp_path):
    """Tester le mode de traitement normal de la CLI."""
    main([str(tmp_path)])
    mock_process_directory.assert_called_once_with(
        tmp_path, use_localtime=False, append_only=True, clean_sidecars=False
    )
@patch('google_takeout_metadata.cli.process_directory_batch')
def test_main_batch_mode(mock_process_directory_batch, tmp_path):
    """Tester le mode de traitement par lot de la CLI."""
    main(["--batch", str(tmp_path)])
    mock_process_directory_batch.assert_called_once_with(
        tmp_path, use_localtime=False, append_only=True, clean_sidecars=False
    )
@patch('google_takeout_metadata.cli.process_directory')
def test_main_localtime_option(mock_process_directory, tmp_path):
    """Tester la CLI avec l'option localtime."""
    main(["--localtime", str(tmp_path)])
    mock_process_directory.assert_called_once_with(
        tmp_path, use_localtime=True, append_only=True, clean_sidecars=False
    )
@patch('google_takeout_metadata.cli.process_directory')
def test_main_overwrite_option(mock_process_directory, tmp_path):
    """Tester la CLI avec l'option overwrite."""
    main(["--overwrite", str(tmp_path)])
    mock_process_directory.assert_called_once_with(
        tmp_path, use_localtime=False, append_only=False, clean_sidecars=False
    )
@patch('google_takeout_metadata.cli.process_directory')
def test_main_clean_sidecars_option(mock_process_directory, tmp_path):
    """Tester la CLI avec l'option clean-sidecars."""
    main(["--clean-sidecars", str(tmp_path)])
    mock_process_directory.assert_called_once_with(
        tmp_path, use_localtime=False, append_only=True, clean_sidecars=True
    )
@patch('google_takeout_metadata.cli.process_directory_batch')
def test_main_batch_with_all_options(mock_process_directory_batch, tmp_path):
    """Tester le mode batch de la CLI avec toutes les options."""
    main(["--batch", "--localtime", "--overwrite", "--clean-sidecars", str(tmp_path)])
    mock_process_directory_batch.assert_called_once_with(
        tmp_path, use_localtime=True, append_only=False, clean_sidecars=True
    )
def test_main_conflicting_options(capsys):
    """Tester la CLI avec des options conflictuelles append-only et overwrite obsol√®tes."""
    with pytest.raises(SystemExit):
        main(["--append-only", "--overwrite", "/some/path"])
    # L'erreur est enregistr√©e mais pas affich√©e sur stderr avec la configuration actuelle
    # Donc nous ne v√©rifions pas la sortie captur√©e, juste qu'elle se termine
@patch('google_takeout_metadata.cli.process_directory')
def test_main_deprecated_append_only_warning(mock_process_directory, tmp_path, caplog):
    """Tester que la CLI avec l'option append-only obsol√®te affiche un avertissement."""
    main(["--append-only", str(tmp_path)])
    assert "--append-only est obsol√®te" in caplog.text
    mock_process_directory.assert_called_once_with(
        tmp_path, use_localtime=False, append_only=True, clean_sidecars=False
    )
@patch('google_takeout_metadata.cli.process_directory')
def test_main_verbose_logging(mock_process_directory, tmp_path, caplog):
    """Tester que la CLI avec l'option verbose active le logging de debug."""
    # Nous devons tester que basicConfig a √©t√© appel√© avec le niveau DEBUG
    # mais le niveau du logger root pourrait ne pas changer pendant le test
    main(["--verbose", str(tmp_path)])
    # S'assurer simplement que la fonction a √©t√© appel√©e - le test de logging est plus complexe
    # en raison de la fa√ßon dont pytest g√®re le logging
    mock_process_directory.assert_called_once()
@pytest.mark.integration
def test_main_integration_normal_mode(tmp_path):
    """Test d'int√©gration pour le mode normal de la CLI avec des fichiers r√©els."""
    try:
        # Cr√©er une image de test
        media_path = tmp_path / "cli_test.jpg"
        img = Image.new('RGB', (100, 100), color='purple')
        img.save(media_path)
        # Cr√©er le sidecar
        sidecar_data = {
            "title": "cli_test.jpg",
            "description": "CLI integration test"
        }
        json_path = tmp_path / "cli_test.jpg.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Ex√©cuter la CLI
        main([str(tmp_path)])
        # V√©rifier que les m√©tadonn√©es ont √©t√© √©crites
        cmd = [
            "exiftool",
            "-j",
            "-EXIF:ImageDescription",
            str(media_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        metadata = json.loads(result.stdout)[0]
        assert metadata.get("ImageDescription") == "CLI integration test"
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping CLI integration test")
@pytest.mark.integration
def test_main_integration_batch_mode(tmp_path):
    """Test d'int√©gration pour le mode batch de la CLI avec des fichiers r√©els."""
    try:
        # Cr√©er plusieurs images de test
        files_data = [
            ("batch1.jpg", "CLI batch test 1"),
            ("batch2.jpg", "CLI batch test 2")
        ]
        for filename, description in files_data:
            # Cr√©er l'image
            media_path = tmp_path / filename
            img = Image.new('RGB', (100, 100), color='orange')
            img.save(media_path)
            # Cr√©er le sidecar
            sidecar_data = {
                "title": filename,
                "description": description
            }
            json_path = tmp_path / f"{filename}.json"
            json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Ex√©cuter la CLI en mode batch
        main(["--batch", str(tmp_path)])
        # V√©rifier que tous les fichiers ont √©t√© trait√©s
        for filename, expected_description in files_data:
            media_path = tmp_path / filename
            cmd = [
                "exiftool",
                "-j",
                "-EXIF:ImageDescription",
                str(media_path)
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
            metadata = json.loads(result.stdout)[0]
            assert metadata.get("ImageDescription") == expected_description
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping CLI batch integration test")
@pytest.mark.integration
def test_main_integration_clean_sidecars(tmp_path):
    """Test d'int√©gration pour la CLI avec nettoyage des sidecars."""
    try:
        # Cr√©er une image de test
        media_path = tmp_path / "cleanup.jpg"
        img = Image.new('RGB', (100, 100), color='cyan')
        img.save(media_path)
        # Cr√©er le sidecar
        sidecar_data = {
            "title": "cleanup.jpg",
            "description": "CLI cleanup test"
        }
        json_path = tmp_path / "cleanup.jpg.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # V√©rifier que le sidecar existe
        assert json_path.exists()
        # Ex√©cuter la CLI avec nettoyage
        main(["--clean-sidecars", str(tmp_path)])
        # V√©rifier que le sidecar a √©t√© supprim√©
        assert not json_path.exists()
        # V√©rifier que les m√©tadonn√©es ont quand m√™me √©t√© √©crites
        cmd = [
            "exiftool",
            "-j",
            "-EXIF:ImageDescription",
            str(media_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        metadata = json.loads(result.stdout)[0]
        assert metadata.get("ImageDescription") == "CLI cleanup test"
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping CLI cleanup integration test")
def test_main_entry_point():
    """Tester que la fonction main peut √™tre appel√©e sans arguments depuis le point d'entr√©e."""
    # Cela teste principalement que la signature de la fonction main est correcte pour les points d'entr√©e
    # Nous ne pouvons pas tester l'analyse CLI r√©elle sans mocker sys.argv
    with patch.object(sys, 'argv', ['google-takeout-metadata', '--help']):
        with pytest.raises(SystemExit):
            main()
````

## File: tests/test_end_to_end.py
````python
from pathlib import Path
import json
import subprocess
import shutil
import pytest
from PIL import Image
from google_takeout_metadata.processor import process_directory
@pytest.mark.skipif(shutil.which("exiftool") is None, reason="exiftool not installed")
def test_end_to_end(tmp_path: Path) -> None:
    # cr√©er une image factice
    img_path = tmp_path / "sample.jpg"
    Image.new("RGB", (10, 10), color="red").save(img_path)
    # cr√©er le sidecar correspondant
    data = {
        "title": "sample.jpg",
        "description": 'Magicien "en" or',
        "photoTakenTime": {"timestamp": "1736719606"},
        "people": [{"name": "anthony vincent"}],
    }
    (tmp_path / "sample.jpg.json").write_text(json.dumps(data), encoding="utf-8")
    process_directory(tmp_path)
    exe = shutil.which("exiftool") or "exiftool"
    result = subprocess.run(
        [
            exe,
            "-j",
            "-XMP-iptcExt:PersonInImage",
            "-XMP-dc:Subject",
            "-IPTC:Keywords",
            "-EXIF:ImageDescription",
            str(img_path),
        ],
        capture_output=True,
        text=True,
        check=True,
    )
    tags = json.loads(result.stdout)[0]
    # exiftool retourne les valeurs uniques en cha√Ænes, les valeurs multiples en listes
    # Normaliser en listes pour la comparaison
    def normalize_to_list(value):
        if value is None:
            return []
        elif isinstance(value, list):
            return value
        else:
            return [value]
    assert normalize_to_list(tags.get("PersonInImage")) == ["anthony vincent"]
    assert normalize_to_list(tags.get("Subject")) == ["anthony vincent"]
    assert normalize_to_list(tags.get("Keywords")) == ["anthony vincent"]
    assert tags.get("ImageDescription") == 'Magicien "en" or'
````

## File: tests/test_exif_writer.py
````python
from google_takeout_metadata.sidecar import SidecarData
from google_takeout_metadata.exif_writer import write_metadata, build_exiftool_args
import subprocess
import pytest
from pathlib import Path
def test_write_metadata_error(tmp_path, monkeypatch):
    meta = SidecarData(
        filename="a.jpg",
        description="test",  # Add description to ensure args are generated
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
    )
    img = tmp_path / "a.jpg"
    img.write_bytes(b"data")
    def fake_run(*args, **kwargs):
        raise subprocess.CalledProcessError(1, "exiftool", stderr="bad")
    monkeypatch.setattr(subprocess, "run", fake_run)
    with pytest.raises(RuntimeError):
        write_metadata(img, meta)
def test_build_args_video():
    """Tester que les balises sp√©cifiques aux vid√©os sont ajout√©es pour les fichiers MP4/MOV."""
    meta = SidecarData(
        filename="video.mp4",
        description="Video description",
        people=["alice"],
        taken_at=1736719606,
        created_at=None,
        latitude=48.8566,
        longitude=2.3522,
        altitude=None,
        favorite=False,
    )
    video_path = Path("video.mp4")
    args = build_exiftool_args(meta, media_path=video_path)
    # V√©rifier les balises sp√©cifiques aux vid√©os
    assert "-Keys:Description=Video description" in args
    assert any("-QuickTime:CreateDate=" in arg for arg in args)
    assert any("-QuickTime:ModifyDate=" in arg for arg in args)
    assert "-Keys:Location=48.8566,2.3522" in args
    assert "-QuickTime:GPSCoordinates=48.8566,2.3522" in args
    assert "-api" in args
    assert "QuickTimeUTC=1" in args
def test_build_args_localtime():
    """Tester que le formatage de l'heure locale fonctionne."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=1736719606,  # 2025-01-12 22:06:46 UTC
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
    )
    # Test UTC (default)
    args_utc = build_exiftool_args(meta, media_path=Path("a.jpg"), use_localtime=False)
    # Test local time
    args_local = build_exiftool_args(meta, media_path=Path("a.jpg"), use_localtime=True)
    # Les cha√Ænes de date-heure seront diff√©rentes (sauf si ex√©cut√© dans le fuseau horaire UTC)
    # mais les deux devraient contenir une forme de DateTimeOriginal
    assert any("-DateTimeOriginal=" in arg for arg in args_utc)
    assert any("-DateTimeOriginal=" in arg for arg in args_local)
def test_build_args_append_only() -> None:
    """Tester que le mode append-only utilise la syntaxe exiftool correcte.
    Note: En production, la logique append-only est maintenant dans write_metadata()
    avec l'approche hybride (ajout + nettoyage NoDups). Ce test v√©rifie seulement 
    la fonction d'adaptation pour maintenir la compatibilit√©.
    """
    meta = SidecarData(
        filename="a.jpg",
        description="desc",
        people=["alice", "bob"],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
    )
    # Normal mode
    args_normal = build_exiftool_args(meta, append_only=False)
    assert "-EXIF:ImageDescription=desc" in args_normal
    # En mode overwrite, on vide d'abord puis on ajoute
    assert "-XMP-iptcExt:PersonInImage=" in args_normal
    assert "-XMP-iptcExt:PersonInImage+=alice" in args_normal
    # Append-only mode (fonction d'adaptation simplifi√©e)
    args_append = build_exiftool_args(meta, append_only=True)
    # Notre nouvelle approche hybride utilise += puis NoDups cleanup
    # La fonction d'adaptation refl√®te cette logique simplifi√©e
    assert "-EXIF:ImageDescription=desc" in args_append
    assert "-XMP-iptcExt:PersonInImage+=alice" in args_append
    assert "-XMP-iptcExt:PersonInImage+=bob" in args_append
def test_build_args_favorite() -> None:
    """Tester que les photos favorites obtiennent rating=5."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=True,
    )
    args = build_exiftool_args(meta, append_only=False)
    assert "-XMP:Rating=5" in args
    # Tester le mode append-only (maintenant le comportement par d√©faut)
    args_append = build_exiftool_args(meta, append_only=True)
    # Devrait utiliser -wm cg pour l'√©criture conditionnelle
    assert "-wm" in args_append
    assert "cg" in args_append
    assert "-XMP:Rating=5" in args_append
def test_build_args_no_favorite() -> None:
    """Tester que les photos non favorites n'obtiennent pas de rating."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
    )
    args = build_exiftool_args(meta)
    assert not any("Rating" in arg for arg in args)
def test_build_args_albums() -> None:
    """Tester que les albums sont √©crits comme mots-cl√©s avec le pr√©fixe Album:."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=["Vacances 2024", "Famille"]
    )
    args = build_exiftool_args(meta, append_only=False)
    # En mode overwrite, on vide d'abord puis on ajoute
    assert "-XMP-dc:Subject=" in args
    assert "-IPTC:Keywords=" in args
    assert "-XMP-dc:Subject+=Album: Vacances 2024" in args
    assert "-IPTC:Keywords+=Album: Vacances 2024" in args
    assert "-XMP-dc:Subject+=Album: Famille" in args
    assert "-IPTC:Keywords+=Album: Famille" in args
def test_build_args_video_append_only() -> None:
    """Tester que les balises sp√©cifiques aux vid√©os sont incluses en mode append-only."""
    meta = SidecarData(
        filename="video.mp4",
        description="Video description",
        people=["alice"],
        taken_at=1736719606,
        created_at=None,
        latitude=48.8566,
        longitude=2.3522,
        altitude=35.0,
        favorite=False,
    )
    video_path = Path("video.mp4")
    args = build_exiftool_args(meta, media_path=video_path, append_only=True)
    # V√©rifier que l'approche append-only utilise -wm cg
    assert "-wm" in args
    assert "cg" in args
    assert "-Keys:Description=Video description" in args
    # V√©rifier que les dates QuickTime sont pr√©sentes
    assert any("QuickTime:CreateDate=" in arg for arg in args)
    assert any("QuickTime:ModifyDate=" in arg for arg in args)
    # V√©rifier que les champs GPS sp√©cifiques √† la vid√©o sont pr√©sents
    assert "-QuickTime:GPSCoordinates=48.8566,2.3522" in args
    assert "-Keys:Location=48.8566,2.3522" in args
    # V√©rifier que l'altitude est pr√©sente
    assert "-GPSAltitude=35.0" in args
    # V√©rifier la configuration vid√©o
    assert "-api" in args
    assert "QuickTimeUTC=1" in args
def test_build_args_albums_append_only() -> None:
    """Tester les albums en mode append-only."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=["Test Album"]
    )
    args = build_exiftool_args(meta, append_only=True)
    # Les albums utilisent += qui ajoute et accumule pour les balises de type liste en mode append-only
    assert "-XMP-dc:Subject+=Album: Test Album" in args
    assert "-IPTC:Keywords+=Album: Test Album" in args
def test_build_args_no_albums() -> None:
    """Tester que la liste d'albums vide n'ajoute aucune balise d'album."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=[]
    )
    args = build_exiftool_args(meta)
    assert not any("Album:" in arg for arg in args)
def test_build_args_default_behavior() -> None:
    """Tester que le comportement par d√©faut est append-only (mode s√©curis√©)."""
    meta = SidecarData(
        filename="a.jpg",
        description="Safe description",
        people=["Safe Person"],
        taken_at=1736719606,
        created_at=None,
        latitude=48.8566,
        longitude=2.3522,
        altitude=None,
        favorite=True,
    )
    # Le comportement par d√©faut devrait √™tre append-only (s√©curis√©)
    args = build_exiftool_args(meta)
    # Devrait utiliser -wm cg pour l'√©criture conditionnelle
    assert "-wm" in args
    assert "cg" in args
    assert "-EXIF:ImageDescription=Safe description" in args
    # Devrait utiliser += pour les personnes (accumulation en mode append-only)
    assert "-XMP-iptcExt:PersonInImage+=Safe Person" in args
    # Le rating devrait √™tre pr√©sent
    assert "-XMP:Rating=5" in args
    # GPS devrait √™tre pr√©sent
    assert "-GPSLatitude=48.8566" in args
def test_build_args_overwrite_mode() -> None:
    """Mode de r√©√©criture explicite (destructif)."""
    meta = SidecarData(
        filename="a.jpg",
        description="Overwrite description",
        people=["Overwrite Person"],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=True,
    )
    # Mode de r√©√©criture explicite
    args = build_exiftool_args(meta, append_only=False)
    # Devrait utiliser l'assignation directe pour les descriptions et les ratings
    assert "-EXIF:ImageDescription=Overwrite description" in args
    # En mode overwrite, on vide d'abord puis on ajoute
    assert "-XMP-iptcExt:PersonInImage=" in args
    assert "-XMP-iptcExt:PersonInImage+=Overwrite Person" in args
    assert "-XMP:Rating=5" in args
    # Ne devrait PAS avoir de conditions -if
    assert "-if" not in args
    assert "not $EXIF:ImageDescription" not in args
    assert "not $XMP-iptcExt:PersonInImage" not in args
    assert "not $XMP:Rating" not in args
def test_build_args_people_default() -> None:
    """Tester que les personnes sont g√©r√©es de mani√®re s√©curis√©e par d√©faut."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=["Alice Dupont", "Bob Martin", "Charlie Bernard"],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
    )
    # Comportement par d√©faut (append-only)
    args = build_exiftool_args(meta)
    # Chaque personne devrait utiliser += (accumulation en mode append-only)
    for person in ["Alice Dupont", "Bob Martin", "Charlie Bernard"]:
        assert f"-XMP-iptcExt:PersonInImage+={person}" in args
        assert f"-XMP-dc:Subject+={person}" in args
        assert f"-IPTC:Keywords+={person}" in args
    # Ne devrait PAS avoir de conditions -if pour les personnes (elles sont des listes, utiliser +=)
    assert "not $XMP-iptcExt:PersonInImage" not in args
    assert "not $XMP-dc:Subject" not in args
    assert "not $IPTC:Keywords" not in args
def test_build_args_albums_default() -> None:
    """Tester que les albums sont g√©r√©s de mani√®re s√©curis√©e par d√©faut."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=["Vacances √ât√© 2024", "Photos de Famille", "√âv√©nements Sp√©ciaux"]
    )
    # Comportement par d√©faut (append-only)
    args = build_exiftool_args(meta)
    # Chaque album devrait utiliser += (accumulation en mode append-only)
    for album in ["Vacances √ât√© 2024", "Photos de Famille", "√âv√©nements Sp√©ciaux"]:
        album_keyword = f"Album: {album}"
        assert f"-XMP-dc:Subject+={album_keyword}" in args
        assert f"-IPTC:Keywords+={album_keyword}" in args
    # Ne devrait PAS avoir de conditions -if pour les albums (ils sont des listes, utiliser +=)
    assert "not $XMP-dc:Subject" not in args
    assert "not $IPTC:Keywords" not in args
````

## File: tests/test_hybrid_approach.py
````python
import shutil
import tempfile
import subprocess
from pathlib import Path
from google_takeout_metadata.sidecar import SidecarData
from google_takeout_metadata.exif_writer import write_metadata
def read_exif_people(image_path: Path) -> list[str]:
    """Lit les personnes depuis un fichier image en utilisant exiftool."""
    try:
        cmd = ["exiftool", "-s", "-s", "-s", "-XMP-iptcExt:PersonInImage", str(image_path)]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30, encoding='utf-8')
        if result.returncode == 0 and result.stdout.strip():
            # ExifTool retourne les valeurs s√©par√©es par des virgules
            people = [p.strip() for p in result.stdout.strip().split(',')]
            return [p for p in people if p]  # Filtrer les valeurs vides
    except (subprocess.SubprocessError, OSError):
        pass
    return []
def test_hybrid_approach_no_duplicates():
    """Test de l'approche hybride : pas de doublons quand on ajoute des personnes existantes."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Copier l'image test dans le r√©pertoire temporaire
        test_image_src = Path("test_assets/test_clean.jpg")
        test_image = Path(temp_dir) / "test_image.jpg"
        shutil.copy2(test_image_src, test_image)
        # √âtape 1 : Ajouter les premi√®res personnes (ancien takeout)
        meta1 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Anthony", "Bernard"],
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False,
            albums=["Vacances"]
        )
        write_metadata(test_image, meta1, append_only=True)
        # V√©rifier l'√©tat initial
        people_initial = read_exif_people(test_image)
        assert "Anthony" in people_initial
        assert "Bernard" in people_initial
        assert len([p for p in people_initial if p == "Anthony"]) == 1
        assert len([p for p in people_initial if p == "Bernard"]) == 1
        # √âtape 2 : Ajouter nouveaux + existants (nouveau takeout avec tous les gens)
        meta2 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Anthony", "Bernard", "Cindy"],  # Contient TOUS les gens, pas juste les nouveaux
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False,
            albums=["Vacances", "Famille"]
        )
        write_metadata(test_image, meta2, append_only=True)
        # V√©rifier le r√©sultat final : pas de doublons malgr√© la redondance
        people_final = read_exif_people(test_image)
        print(f"Personnes finales: {people_final}")
        # Assertions critiques : aucun doublon
        assert "Anthony" in people_final
        assert "Bernard" in people_final 
        assert "Cindy" in people_final
        assert len([p for p in people_final if p == "Anthony"]) == 1, f"Anthony appara√Æt plusieurs fois: {people_final}"
        assert len([p for p in people_final if p == "Bernard"]) == 1, f"Bernard appara√Æt plusieurs fois: {people_final}"
        assert len([p for p in people_final if p == "Cindy"]) == 1, f"Cindy appara√Æt plusieurs fois: {people_final}"
        # V√©rifier que toutes les personnes attendues sont pr√©sentes
        expected_people = {"Anthony", "Bernard", "Cindy"}
        actual_people = set(people_final)
        assert expected_people.issubset(actual_people), f"Personnes manquantes. Attendu: {expected_people}, R√©el: {actual_people}"
def test_hybrid_approach_only_new_people():
    """Test de l'approche hybride : ajouter seulement les nouvelles personnes."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Copier l'image test
        test_image_src = Path("test_assets/test_clean.jpg")
        test_image = Path(temp_dir) / "test_image.jpg"
        shutil.copy2(test_image_src, test_image)
        # √âtape 1 : Ajouter les premi√®res personnes
        meta1 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Alice", "Bob"],
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False
        )
        write_metadata(test_image, meta1, append_only=True)
        # √âtape 2 : Ajouter seulement les nouvelles personnes 
        meta2 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Charlie"],  # Seulement la nouvelle personne
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False
        )
        write_metadata(test_image, meta2, append_only=True)
        # V√©rifier le r√©sultat : toutes les personnes pr√©sentes, pas de doublons
        people_final = read_exif_people(test_image)
        expected_people = {"Alice", "Bob", "Charlie"}
        actual_people = set(people_final)
        assert expected_people == actual_people, f"Attendu: {expected_people}, R√©el: {actual_people}"
        assert len(people_final) == 3, f"Doublons d√©tect√©s: {people_final}"
if __name__ == "__main__":
    test_hybrid_approach_no_duplicates()
    test_hybrid_approach_only_new_people()
    print("‚úÖ Tests de l'approche hybride : SUCC√àS")
````

## File: tests/test_improvements.py
````python
"""Tests unitaires pour les am√©liorations des statistiques et de la recherche d'albums."""
import json
import pytest
import subprocess
import tempfile
from pathlib import Path
from unittest.mock import patch
from PIL import Image
from google_takeout_metadata.processor_batch import process_batch
from google_takeout_metadata.sidecar import find_albums_for_directory
from google_takeout_metadata.statistics import ProcessingStats
class TestProcessingStats:
    """Tests pour la classe ProcessingStats."""
    def test_init(self):
        """Test de l'initialisation des statistiques."""
        stats = ProcessingStats()
        assert stats.total_sidecars_found == 0
        assert stats.total_processed == 0
        assert stats.total_failed == 0
        assert stats.total_skipped == 0
        assert stats.images_processed == 0
        assert stats.videos_processed == 0
        assert stats.files_fixed_extension == 0
        assert stats.sidecars_cleaned == 0
        assert stats.failed_files == []
        assert stats.skipped_files == []
        assert stats.fixed_extensions == []
        assert stats.errors_by_type == {}
        assert stats.start_time is None
        assert stats.end_time is None
    def test_add_processed_file(self):
        """Test de l'ajout de fichiers trait√©s."""
        stats = ProcessingStats()
        test_path = Path("test_image.jpg")
        # Test image
        stats.add_processed_file(test_path, is_image=True)
        assert stats.total_processed == 1
        assert stats.images_processed == 1
        assert stats.videos_processed == 0
        # Test vid√©o
        stats.add_processed_file(Path("test_video.mp4"), is_image=False)
        assert stats.total_processed == 2
        assert stats.images_processed == 1
        assert stats.videos_processed == 1
    def test_add_failed_file(self):
        """Test de l'ajout de fichiers en √©chec."""
        stats = ProcessingStats()
        test_path = Path("test_file.jpg")
        stats.add_failed_file(test_path, "parse_error", "JSON invalide")
        assert stats.total_failed == 1
        assert len(stats.failed_files) == 1
        assert stats.failed_files[0] == "test_file.jpg: JSON invalide"
        assert stats.errors_by_type["parse_error"] == 1
        # Test comptage des erreurs par type
        stats.add_failed_file(test_path, "parse_error", "Autre erreur JSON")
        assert stats.errors_by_type["parse_error"] == 2
    def test_add_skipped_file(self):
        """Test de l'ajout de fichiers ignor√©s."""
        stats = ProcessingStats()
        test_path = Path("test_file.jpg")
        stats.add_skipped_file(test_path, "Fichier d√©j√† trait√©")
        assert stats.total_skipped == 1
        assert len(stats.skipped_files) == 1
        assert stats.skipped_files[0] == "test_file.jpg: Fichier d√©j√† trait√©"
    def test_add_fixed_extension(self):
        """Test de l'ajout de corrections d'extension."""
        stats = ProcessingStats()
        stats.add_fixed_extension("image.png", "image.jpg")
        assert stats.files_fixed_extension == 1
        assert len(stats.fixed_extensions) == 1
        assert stats.fixed_extensions[0] == "image.png ‚Üí image.jpg"
    def test_success_rate(self):
        """Test du calcul du taux de r√©ussite."""
        stats = ProcessingStats()
        # Aucun fichier
        assert stats.success_rate == 0.0
        # Quelques fichiers
        stats.total_sidecars_found = 10
        stats.total_processed = 8
        assert stats.success_rate == 80.0
        # Tous r√©ussis
        stats.total_processed = 10
        assert stats.success_rate == 100.0
    def test_timing(self):
        """Test du syst√®me de timing."""
        stats = ProcessingStats()
        assert stats.duration is None
        stats.start_processing()
        assert stats.start_time is not None
        assert stats.duration is None
        stats.end_processing()
        assert stats.end_time is not None
        assert stats.duration is not None
        assert stats.duration >= 0
class TestFindAlbumsForDirectory:
    """Tests pour la fonction find_albums_for_directory am√©lior√©e."""
    def test_empty_directory(self):
        """Test avec un r√©pertoire vide."""
        with tempfile.TemporaryDirectory() as temp_dir:
            result = find_albums_for_directory(Path(temp_dir))
            assert result == []
    def test_case_insensitive_metadata_files(self):
        """Test de la gestion insensible √† la casse."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Cr√©er des fichiers avec diff√©rentes casses
            (temp_path / "METADATA.JSON").write_text('{"title": "Album1"}', encoding='utf-8')
            (temp_path / "m√©tadonn√©es.json").write_text('{"title": "Album2"}', encoding='utf-8')
            (temp_path / "Album_Metadata.JSON").write_text('{"title": "Album3"}', encoding='utf-8')
            result = find_albums_for_directory(temp_path)
            # Doit trouver tous les albums
            assert len(result) == 3
            assert "Album1" in result
            assert "Album2" in result
            assert "Album3" in result
    def test_numbered_variations_case_insensitive(self):
        """Test des variations num√©rot√©es insensibles √† la casse."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Cr√©er des fichiers avec variations num√©rot√©es et casses diff√©rentes
            (temp_path / "M√©tadonn√©es(1).JSON").write_text('{"title": "Album1"}', encoding='utf-8')
            (temp_path / "M√âTADONN√âES(2).json").write_text('{"title": "Album2"}', encoding='utf-8')
            result = find_albums_for_directory(temp_path)
            assert len(result) == 2
            assert "Album1" in result
            assert "Album2" in result
    def test_max_depth_limit(self):
        """Test de la limite de profondeur."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Cr√©er une hi√©rarchie simple : temp_dir/parent/current
            parent_dir = temp_path / "parent"
            parent_dir.mkdir()
            current_dir = parent_dir / "current"
            current_dir.mkdir()
            # Ajouter un album au niveau parent
            (parent_dir / "metadata.json").write_text('{"title": "ParentAlbum"}', encoding='utf-8')
            # Ajouter un album au niveau racine (temp_dir)
            (temp_path / "metadata.json").write_text('{"title": "RootAlbum"}', encoding='utf-8')
            # Test avec max_depth=2 (permet de v√©rifier current_dir et parent_dir)
            result = find_albums_for_directory(current_dir, max_depth=2)
            assert "ParentAlbum" in result
            assert "RootAlbum" not in result  # temp_dir est √† depth=2, donc exclu
            # Test avec max_depth=3 (permet de v√©rifier current_dir, parent_dir et temp_dir)
            result_full = find_albums_for_directory(current_dir, max_depth=3)
            assert "ParentAlbum" in result_full
            assert "RootAlbum" in result_full
            # Test avec max_depth=1 (ne v√©rifie que current_dir)
            result_limited = find_albums_for_directory(current_dir, max_depth=1)
            assert len(result_limited) == 0  # pas d'album dans current_dir
    def test_takeout_marker_detection(self):
        """Test de la d√©tection des r√©pertoires marqueurs."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Cr√©er une hi√©rarchie avec marqueur
            root_dir = temp_path / "root"
            root_dir.mkdir()
            takeout_dir = root_dir / "mon-takeout" 
            takeout_dir.mkdir()
            photos_dir = takeout_dir / "photos"
            photos_dir.mkdir()
            # Ajouter un album au niveau root (plus haut que le marqueur)
            (root_dir / "metadata.json").write_text('{"title": "RootAlbum"}', encoding='utf-8')
            result = find_albums_for_directory(photos_dir)
            # Doit s'arr√™ter au marqueur et ne pas remonter jusqu'au root
            assert "RootAlbum" not in result
    def test_order_preservation(self):
        """Test de la pr√©servation de l'ordre de priorit√©."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Cr√©er une hi√©rarchie avec albums √† diff√©rents niveaux
            level1 = temp_path / "level1"
            level1.mkdir()
            # Album au niveau courant (priorit√© haute)
            (temp_path / "metadata.json").write_text('{"title": "CurrentLevel"}', encoding='utf-8')
            # Album au niveau parent (priorit√© basse)
            (level1 / "metadata.json").write_text('{"title": "ParentLevel"}', encoding='utf-8')
            result = find_albums_for_directory(temp_path)
            # L'album du niveau courant doit √™tre en premier
            assert result[0] == "CurrentLevel"
    def test_error_handling(self):
        """Test de la gestion d'erreurs."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Cr√©er un fichier JSON invalide
            (temp_path / "metadata.json").write_text('{"invalid": json}', encoding='utf-8')
            # Cr√©er un fichier JSON valide
            (temp_path / "album_metadata.json").write_text('{"title": "ValidAlbum"}', encoding='utf-8')
            # La fonction doit continuer malgr√© l'erreur
            result = find_albums_for_directory(temp_path)
            # Doit trouver l'album valide
            assert "ValidAlbum" in result
    @patch('google_takeout_metadata.sidecar.logger')
    def test_debug_logging(self, mock_logger):
        """Test des logs debug."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Cr√©er une hi√©rarchie avec marqueur et album au-dessus
            root_dir = temp_path / "root"
            root_dir.mkdir()
            takeout_dir = root_dir / "Google Photos"  # Un marqueur s√ªr
            takeout_dir.mkdir()
            photos_dir = takeout_dir / "photos"
            photos_dir.mkdir()
            # Ajouter un album au niveau root pour forcer la remont√©e
            (root_dir / "metadata.json").write_text('{"title": "RootAlbum"}', encoding='utf-8')
            find_albums_for_directory(photos_dir)
            # V√©rifier que le debug a √©t√© appel√© (pour n'importe quel message debug)
            assert mock_logger.debug.called, "Aucun appel au logger.debug d√©tect√©"
            # V√©rifier les appels debug pour trouver celui du marqueur
            debug_calls = [str(call) for call in mock_logger.debug.call_args_list]
            # Chercher un message contenant "marqueur"
            marker_calls = [call for call in debug_calls if "marqueur" in call.lower()]
            assert len(marker_calls) > 0, f"Pas de log marqueur trouv√© dans: {debug_calls}"
            calls = [call for call in mock_logger.debug.call_args_list 
                    if "r√©pertoire marqueur" in str(call)]
            assert len(calls) > 0
@pytest.mark.integration
def test_batch_sidecar_cleanup_with_condition_failure(tmp_path: Path) -> None:
    """Tester que les sidecars sont supprim√©s m√™me quand 'files failed condition' survient en mode batch."""
    # Cr√©er une image de test
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='blue')
    img.save(media_path)
    # Ajouter des m√©tadonn√©es existantes (description EXIF)
    try:
        subprocess.run([
            "exiftool", "-overwrite_original",
            "-EXIF:ImageDescription=Existing description",
            str(media_path)
        ], capture_output=True, text=True, check=True, timeout=30)
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping integration test")
    # Cr√©er le sidecar JSON avec une description diff√©rente (qui causera "files failed condition")
    sidecar_data = {
        "title": "test.jpg",
        "description": "New description that should not overwrite existing"
    }
    json_path = tmp_path / "test.jpg.supplemental-metadata.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # V√©rifier que le sidecar existe avant traitement
    assert json_path.exists()
    # Cr√©er le lot avec clean_sidecars=True
    batch = [(media_path, json_path, [])]  # Liste vide pour les args car on teste juste le nettoyage
    # Traiter le lot avec nettoyage activ√©
    # Ceci devrait d√©clencher "files failed condition" car la description existe d√©j√†
    # ET devrait quand m√™me supprimer le sidecar
    result = process_batch(batch, clean_sidecars=True)
    # V√©rifier que le traitement a r√©ussi (malgr√© "files failed condition")
    assert result > 0
    # V√©rifier que le sidecar a √©t√© supprim√© (le bug corrig√©)
    assert not json_path.exists(), "Le sidecar aurait d√ª √™tre supprim√© apr√®s traitement r√©ussi avec 'files failed condition'"
def test_batch_cleanup_logic_unit() -> None:
    """Test unitaire pour v√©rifier la logique de nettoyage en cas de 'files failed condition'."""
    # Ce test v√©rifie que notre modification de code est coh√©rente
    # Il ne teste pas exiftool mais la logique interne
    from google_takeout_metadata.processor_batch import process_batch
    import tempfile
    from pathlib import Path
    import json
    from unittest.mock import patch
    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_path = Path(tmp_dir)
        # Cr√©er des fichiers factices
        media_path = tmp_path / "test.jpg"
        media_path.write_text("fake image content")
        json_path = tmp_path / "test.jpg.supplemental-metadata.json"
        sidecar_data = {"title": "test.jpg", "description": "Test description"}
        json_path.write_text(json.dumps(sidecar_data))
        batch = [(media_path, json_path, ["-description=test"])]
        # Mock subprocess.run pour simuler "files failed condition"
        mock_error = subprocess.CalledProcessError(2, "exiftool")
        mock_error.stderr = "2 files failed condition"
        mock_error.stdout = ""
        with patch('google_takeout_metadata.processor_batch.subprocess.run', side_effect=mock_error):
            # V√©rifier que le fichier existe avant
            assert json_path.exists()
            # Appeler process_batch avec clean_sidecars=True
            result = process_batch(batch, clean_sidecars=True)
            # V√©rifier le succ√®s et la suppression
            assert result == 1, "Le batch devrait √™tre consid√©r√© comme r√©ussi"
            assert not json_path.exists(), "Le sidecar aurait d√ª √™tre supprim√© m√™me avec 'files failed condition'"
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
````

## File: tests/test_integration.py
````python
"""Tests d'int√©gration qui ex√©cutent r√©ellement exiftool et v√©rifient que les m√©tadonn√©es sont √©crites correctement."""
from pathlib import Path
import json
import subprocess
import pytest
import shutil
from PIL import Image
from google_takeout_metadata.processor import process_sidecar_file
from google_takeout_metadata.exif_writer import write_metadata
from google_takeout_metadata.sidecar import SidecarData
def _get_test_assets_dir() -> Path:
    """Retourne le chemin vers le dossier des assets de test."""
    return Path(__file__).parent.parent / "test_assets"
def _copy_test_asset(asset_name: str, dest_path: Path) -> None:
    """Copie un asset de test vers le chemin de destination."""
    assets_dir = _get_test_assets_dir()
    asset_path = assets_dir / asset_name
    if not asset_path.exists():
        pytest.skip(f"Asset de test {asset_name} introuvable dans {assets_dir}")
    shutil.copy2(asset_path, dest_path)
def _run_exiftool_read(media_path: Path) -> dict:
    """Ex√©cuter exiftool pour lire les m√©tadonn√©es depuis un fichier image."""
    cmd = [
        "exiftool", 
        "-json",
        "-charset", "filename=UTF8",
        "-charset", "iptc=UTF8", 
        "-charset", "exif=UTF8",
        "-charset", "XMP=UTF8",
        str(media_path)
    ]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        data = json.loads(result.stdout)
        return data[0] if data else {}
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping integration tests")
    except subprocess.CalledProcessError as e:
        pytest.fail(f"exiftool failed: {e.stderr}")
@pytest.mark.integration
def test_write_and_read_description(tmp_path: Path) -> None:
    """Tester que la description est √©crite et peut √™tre relue."""
    # Utiliser un asset de test propre
    media_path = tmp_path / "test.jpg"
    _copy_test_asset("test_clean.jpg", media_path)
    # Cr√©er le JSON sidecar
    sidecar_data = {
        "title": "test.jpg",
        "description": "Test photo with √± and √©mojis üéâ"
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les m√©tadonn√©es
    metadata = _run_exiftool_read(media_path)
    # V√©rifier que la description a √©t√© √©crite
    assert metadata.get("Description") == "Test photo with √± and √©mojis üéâ"
    assert metadata.get("ImageDescription") == "Test photo with √± and √©mojis üéâ"
@pytest.mark.integration
def test_write_and_read_people(tmp_path: Path) -> None:
    """Tester que les noms de personnes sont √©crits et peuvent √™tre relus."""
    # Utiliser un asset de test propre
    media_path = tmp_path / "test.jpg"
    _copy_test_asset("test_clean.jpg", media_path)
    # Cr√©er le JSON sidecar avec des personnes
    sidecar_data = {
        "title": "test.jpg",
        "people": [
            {"name": "Alice Dupont"},
            {"name": "Bob Martin"}
        ]
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les m√©tadonn√©es
    metadata = _run_exiftool_read(media_path)
    # V√©rifier que les personnes ont √©t√© √©crites
    keywords = metadata.get("Keywords", [])
    if isinstance(keywords, str):
        keywords = [keywords]
    assert "Alice Dupont" in keywords
    assert "Bob Martin" in keywords
@pytest.mark.integration 
def test_write_and_read_gps(tmp_path: Path) -> None:
    """Tester que les coordonn√©es GPS sont √©crites et peuvent √™tre relues."""
    # Utiliser un asset de test propre
    media_path = tmp_path / "test.jpg"
    _copy_test_asset("test_clean.jpg", media_path)
    # Cr√©er le JSON sidecar avec des donn√©es GPS
    sidecar_data = {
        "title": "test.jpg",
        "geoData": {
            "latitude": 48.8566,
            "longitude": 2.3522,
            "altitude": 35.0
        }
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les m√©tadonn√©es
    metadata = _run_exiftool_read(media_path)
    # V√©rifier que les donn√©es GPS ont √©t√© √©crites
    # exiftool retourne les coordonn√©es GPS dans un format lisible, donc on doit v√©rifier diff√©remment
    gps_lat = metadata.get("GPSLatitude")
    gps_lon = metadata.get("GPSLongitude")
    # V√©rifier que les champs GPS existent et contiennent les valeurs de degr√©s attendues
    assert gps_lat is not None, "GPSLatitude devrait √™tre d√©finie"
    assert gps_lon is not None, "GPSLongitude devrait √™tre d√©finie"
    assert "48 deg" in str(gps_lat), f"Expected 48 degrees in latitude, got: {gps_lat}"
    assert "2 deg" in str(gps_lon), f"Expected 2 degrees in longitude, got: {gps_lon}"
    # Les r√©f√©rences GPS peuvent √™tre "N"/"North" et "E"/"East" selon la version d'exiftool
    lat_ref = metadata.get("GPSLatitudeRef")
    lon_ref = metadata.get("GPSLongitudeRef")
    assert lat_ref in ["N", "North"], f"Expected N or North for latitude ref, got: {lat_ref}"
    assert lon_ref in ["E", "East"], f"Expected E or East for longitude ref, got: {lon_ref}"
@pytest.mark.integration
def test_write_and_read_favorite(tmp_path: Path) -> None:
    """Tester que le statut favori est √©crit comme notation."""
    # Cr√©er une image de test simple
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='yellow')
    img.save(media_path)
    # Cr√©er le fichier JSON annexe avec favori
    sidecar_data = {
        "title": "test.jpg",
        "favorited": True
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les m√©tadonn√©es
    metadata = _run_exiftool_read(media_path)
    # V√©rifier que la notation a √©t√© √©crite
    assert int(metadata.get("Rating", 0)) == 5
@pytest.mark.integration
def test_append_only_mode(tmp_path: Path) -> None:
    """Tester que le mode append-only n'√©crase pas la description existante."""
    # Utiliser un asset de test avec m√©tadonn√©es existantes
    media_path = tmp_path / "test.jpg"
    _copy_test_asset("test_with_metadata.jpg", media_path)
    # Cr√©er le fichier JSON annexe avec une description diff√©rente
    sidecar_data = {
        "title": "test.jpg", 
        "description": "New description from sidecar"
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar en mode append-only
    process_sidecar_file(json_path, append_only=True)
    # Relire les m√©tadonn√©es
    metadata = _run_exiftool_read(media_path)
    # En mode append-only, la description originale devrait √™tre pr√©serv√©e
    # Note: exiftool's -= operator doesn't overwrite if field exists
    assert metadata.get("ImageDescription") == "Existing description"
@pytest.mark.integration
def test_datetime_formats(tmp_path: Path) -> None:
    """Tester que la date-heure est √©crite dans le bon format."""
    # Cr√©er une image de test simple
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='orange')
    img.save(media_path)
    # Cr√©er le fichier JSON annexe avec horodatage
    sidecar_data = {
        "title": "test.jpg",
        "photoTakenTime": {"timestamp": "1736719606"}  # Horodatage Unix
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les m√©tadonn√©es
    metadata = _run_exiftool_read(media_path)
    # V√©rifier le format de la date-heure (devrait √™tre YYYY:MM:DD HH:MM:SS)
    date_original = metadata.get("DateTimeOriginal")
    assert date_original is not None
    assert ":" in date_original
    # Devrait correspondre au format EXIF datetime
    import re
    assert re.match(r'\d{4}:\d{2}:\d{2} \d{2}:\d{2}:\d{2}', date_original)
@pytest.mark.integration
def test_write_and_read_albums(tmp_path: Path) -> None:
    """Tester que les albums sont √©crits et peuvent √™tre relus."""
    # Cr√©er une image de test simple
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='cyan')
    img.save(media_path)
    # Cr√©er le fichier metadata.json d'album
    album_data = {"title": "Vacances √ât√© 2024"}
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    # Cr√©er le fichier JSON annexe
    sidecar_data = {
        "title": "test.jpg",
        "description": "Photo de vacances"
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les m√©tadonn√©es
    metadata = _run_exiftool_read(media_path)
    # V√©rifier que l'album a √©t√© √©crit comme mot-cl√©
    keywords = metadata.get("Keywords", [])
    if isinstance(keywords, str):
        keywords = [keywords]
    assert "Album: Vacances √ât√© 2024" in keywords
    # V√©rifier aussi le champ Subject
    subjects = metadata.get("Subject", [])
    if isinstance(subjects, str):
        subjects = [subjects]
    assert "Album: Vacances √ât√© 2024" in subjects
@pytest.mark.integration  
def test_albums_and_people_combined(tmp_path: Path) -> None:
    """Tester que les albums et les personnes peuvent coexister dans les mots-cl√©s."""
    # Cr√©er une image de test simple
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='magenta')
    img.save(media_path)
    # Cr√©er le fichier metadata.json d'album
    album_data = {"title": "Album Famille"}
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    # Cr√©er le fichier JSON annexe avec des personnes
    sidecar_data = {
        "title": "test.jpg",
        "people": [{"name": "Alice"}, {"name": "Bob"}]
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les m√©tadonn√©es
    metadata = _run_exiftool_read(media_path)
    # V√©rifier que les mots-cl√©s contiennent √† la fois les personnes et l'album
    keywords = metadata.get("Keywords", [])
    if isinstance(keywords, str):
        keywords = [keywords]
    # V√©rifier que nous avons √† la fois des personnes et un album
    assert "Alice" in keywords
    assert "Bob" in keywords
    assert "Album: Album Famille" in keywords
@pytest.mark.integration
def test_default_safe_behavior(tmp_path: Path) -> None:
    """Tester que le comportement par d√©faut est s√ªr (append-only) et pr√©serve les m√©tadonn√©es existantes."""
    # Cr√©er une simple image de test
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='red')
    img.save(media_path)
    # Tout d'abord, ajouter manuellement des m√©tadonn√©es en utilisant le mode √©crasement
    first_meta = SidecarData(
        filename="test.jpg",
        description="Original description",
        people=["Original Person"],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=["Original Album"]
    )
    # √âcrire les m√©tadonn√©es initiales avec le mode √©crasement
    write_metadata(media_path, first_meta, append_only=False)
    # V√©rifier que les m√©tadonn√©es initiales ont √©t√© √©crites
    initial_metadata = _run_exiftool_read(media_path)
    assert initial_metadata.get("ImageDescription") == "Original description"
    initial_keywords = initial_metadata.get("Keywords", [])
    if isinstance(initial_keywords, str):
        initial_keywords = [initial_keywords]
    assert "Original Person" in initial_keywords
    assert "Album: Original Album" in initial_keywords
    # Cr√©er le fichier JSON annexe avec une nouvelle description et une nouvelle personne
    sidecar_data = {
        "title": "test.jpg",
        "description": "New description", 
        "people": [{"name": "New Person"}]
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar en mode par d√©faut (append-only)
    process_sidecar_file(json_path)
    # Relire les m√©tadonn√©es
    final_metadata = _run_exiftool_read(media_path)
    # En mode append-only, la description d'origine doit √™tre pr√©serv√©e
    # car nous utilisons -if "not $TAG" qui n'√©crit que si le tag n'existe pas
    assert final_metadata.get("ImageDescription") == "Original description"
    # Les mots-cl√©s devraient toujours contenir les donn√©es d'origine, et les nouvelles personnes devraient √™tre AJOUT√âES (pas remplac√©es)
    # car nous utilisons = qui accumule pour les balises de type liste
    final_keywords = final_metadata.get("Keywords", [])
    if isinstance(final_keywords, str):
        final_keywords = [final_keywords]
    assert "Original Person" in final_keywords
    assert "Album: Original Album" in final_keywords
    # La nouvelle personne devrait √©galement √™tre pr√©sente
    assert "New Person" in final_keywords
@pytest.mark.integration  
def test_explicit_overwrite_behavior(tmp_path: Path) -> None:
    """Tester que le mode √©crasement explicite remplace les m√©tadonn√©es existantes."""
    # Cr√©er une simple image de test
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='blue') 
    img.save(media_path)
    # Tout d'abord, ajouter des m√©tadonn√©es initiales en utilisant le mode √©crasement
    first_meta = SidecarData(
        filename="test.jpg",
        description="Original description",
        people=["Original Person"],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=[]
    )
    write_metadata(media_path, first_meta, append_only=False)
    # V√©rifier que les m√©tadonn√©es initiales ont √©t√© √©crites
    sidecar_data = {
        "title": "test.jpg",
        "description": "New description",
        "people": [{"name": "New Person"}]
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar en mode √©crasement explicite
    process_sidecar_file(json_path, append_only=False)
    # Relire les m√©tadonn√©es
    final_metadata = _run_exiftool_read(media_path)
    # En mode √©crasement, la nouvelle description doit remplacer l'ancienne
    # Note: Nous utilisons l'op√©rateur = donc les personnes sont ajout√©es et accumulent
    final_keywords = final_metadata.get("Keywords", [])
    if isinstance(final_keywords, str):
        final_keywords = [final_keywords]
    # Les deux personnes, originale et nouvelle, devraient √™tre pr√©sentes (car = accumule pour les listes)
    assert "Original Person" in final_keywords
    assert "New Person" in final_keywords
@pytest.mark.integration
def test_append_only_vs_overwrite_video_equivalence(tmp_path: Path) -> None:
    """Tester que le mode append-only produit des r√©sultats similaires au mode √©crasement pour les vid√©os quand aucune m√©tadonn√©e n'existe."""
    # Copier les fichiers vid√©o de test (vierges)
    video_path_append = tmp_path / "test_append.mp4"
    video_path_overwrite = tmp_path / "test_overwrite.mp4"
    _copy_test_asset("test_video_clean.mp4", video_path_append)
    _copy_test_asset("test_video_clean.mp4", video_path_overwrite)
    # Cr√©er les m√©tadonn√©es √† √©crire
    meta = SidecarData(
        filename="test.mp4",
        description="Test video description",
        people=["Video Person"],
        taken_at=1736719606,
        created_at=None,
        latitude=48.8566,
        longitude=2.3522,
        altitude=35.0,
        favorite=True,
        albums=["Test Album"]
    )
    # √âcrire avec le mode append-only
    write_metadata(video_path_append, meta, append_only=True)
    # √âcrire avec le mode √©crasement
    write_metadata(video_path_overwrite, meta, append_only=False)
    # Relire les m√©tadonn√©es des deux fichiers
    metadata_append = _run_exiftool_read(video_path_append)
    metadata_overwrite = _run_exiftool_read(video_path_overwrite)
    # Comparer les champs cl√©s
    # En mode append-only, les nouvelles m√©tadonn√©es peuvent ne pas √™tre √©crites si des tags similaires existent
    # En mode overwrite, les m√©tadonn√©es sont toujours √©crites
    # Le test v√©rifie que les nouvelles m√©tadonn√©es importantes sont pr√©sentes
    # Les mots-cl√©s devraient contenir la personne et l'album dans les deux modes
    keywords_append = metadata_append.get("Keywords", [])
    keywords_overwrite = metadata_overwrite.get("Keywords", [])
    if isinstance(keywords_append, str):
        keywords_append = [keywords_append]
    if isinstance(keywords_overwrite, str):
        keywords_overwrite = [keywords_overwrite]
    # V√©rifier que les nouvelles m√©tadonn√©es importantes sont pr√©sentes
    # En mode overwrite, les nouveaux mots-cl√©s doivent √™tre pr√©sents
    # Pour les vid√©os MP4, les mots-cl√©s sont stock√©s dans Subject, pas Keywords
    subjects_append = metadata_append.get("Subject", [])
    subjects_overwrite = metadata_overwrite.get("Subject", [])
    if isinstance(subjects_append, str):
        subjects_append = [subjects_append]
    if isinstance(subjects_overwrite, str):
        subjects_overwrite = [subjects_overwrite]
    # Combiner Keywords et Subject pour une v√©rification compl√®te
    all_keywords_append = keywords_append + subjects_append
    all_keywords_overwrite = keywords_overwrite + subjects_overwrite
    assert "Video Person" in all_keywords_overwrite
    assert "Album: Test Album" in all_keywords_overwrite
    # En mode append-only, les mots-cl√©s sont ajout√©s m√™me si d'autres existent
    assert "Video Person" in all_keywords_append
    assert "Album: Test Album" in all_keywords_append
@pytest.mark.integration
def test_batch_vs_normal_mode_equivalence(tmp_path: Path) -> None:
    """Tester que le mode batch produit les m√™mes r√©sultats que le mode normal."""
    # Importer la fonction de traitement par lot
    from google_takeout_metadata.processor_batch import process_directory_batch
    from google_takeout_metadata.processor import process_directory
    # Cr√©er des donn√©es de test
    test_files = [
        ("photo1.jpg", "First test photo", "Alice"),
        ("photo2.jpg", "Second test photo", "Bob"),
        ("photo3.jpg", "Third test photo", "Charlie")
    ]
    # Cr√©er deux structures de r√©pertoires identiques
    normal_dir = tmp_path / "normal_mode"
    batch_dir = tmp_path / "batch_mode"
    normal_dir.mkdir()
    batch_dir.mkdir()
    for filename, description, person in test_files:
        # Cr√©er les deux fichiers dans les deux r√©pertoires
        for test_dir in [normal_dir, batch_dir]:
            # Cr√©er l'image
            media_path = test_dir / filename
            img = Image.new('RGB', (100, 100), color='blue')
            img.save(media_path)
            # Cr√©er le sidecar
            sidecar_data = {
                "title": filename,
                "description": description,
                "people": [{"name": person}]
            }
            json_path = test_dir / f"{filename}.json"
            json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    try:
        # Proceder avec le mode normal
        process_directory(normal_dir, use_localtime=False, append_only=True, clean_sidecars=False)
        # Traiter avec le mode par lot
        process_directory_batch(batch_dir, use_localtime=False, append_only=True, clean_sidecars=False)
        # Comparer les m√©tadonn√©es des fichiers dans les deux r√©pertoires
        for filename, expected_description, expected_person in test_files:
            normal_metadata = _run_exiftool_read(normal_dir / filename)
            batch_metadata = _run_exiftool_read(batch_dir / filename)
            # V√©rifier que les descriptions correspondent
            assert normal_metadata.get("ImageDescription") == batch_metadata.get("ImageDescription")
            assert normal_metadata.get("ImageDescription") == expected_description
            # V√©rifier que les personnes correspondent
            normal_people = normal_metadata.get("PersonInImage", [])
            batch_people = batch_metadata.get("PersonInImage", [])
            if isinstance(normal_people, str):
                normal_people = [normal_people]
            if isinstance(batch_people, str):
                batch_people = [batch_people]
            assert set(normal_people) == set(batch_people)
            assert expected_person in normal_people
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping batch vs normal mode test")
@pytest.mark.integration
def test_batch_mode_performance_benefit(tmp_path: Path) -> None:
    """Tester que le mode batch peut g√©rer de nombreux fichiers (test de performance)."""
    from google_takeout_metadata.processor_batch import process_directory_batch
    import time
    # Cr√©er de nombreux fichiers de test
    num_files = 20  # R√©duit pour CI, mais d√©montre toujours la capacit√© par lot
    for i in range(num_files):
        filename = f"perf_test_{i:03d}.jpg"
        # Cr√©er l'image
        media_path = tmp_path / filename
        img = Image.new('RGB', (50, 50), color='red')
        img.save(media_path)
        # Cr√©er le sidecar
        sidecar_data = {
            "title": filename,
            "description": f"Performance test image {i}"
        }
        json_path = tmp_path / f"{filename}.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    try:
        # Mesurer le temps de traitement par lot
        start_time = time.time()
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
        end_time = time.time()
        batch_time = end_time - start_time
        # V√©rifier que tous les fichiers ont √©t√© trait√©s
        for i in range(num_files):
            filename = f"perf_test_{i:03d}.jpg"
            media_path = tmp_path / filename
            metadata = _run_exiftool_read(media_path)
            expected_description = f"Performance test image {i}"
            assert metadata.get("ImageDescription") == expected_description
        # Imprimer le temps pris pour le traitement par lot
        print(f"Batch mode processed {num_files} files in {batch_time:.2f} seconds")
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping batch performance test")
@pytest.mark.integration  
def test_batch_mode_with_mixed_file_types(tmp_path: Path) -> None:
    """Tester le mode batch avec diff√©rents types de fichiers et m√©tadonn√©es complexes."""
    from google_takeout_metadata.processor_batch import process_directory_batch
    # Cr√©er des fichiers de test avec diff√©rents types et m√©tadonn√©es
    test_files = [
        ("mixed1.jpg", "JPEG test"),
        ("mixed2.png", "PNG test")  # PNG if supported by PIL
    ]
    for filename, description in test_files:
        # Cr√©er l'image
        media_path = tmp_path / filename
        if filename.endswith('.jpg'):
            img = Image.new('RGB', (100, 100), color='green')
            img.save(media_path, format='JPEG')
        elif filename.endswith('.png'):
            img = Image.new('RGBA', (100, 100), color=(0, 255, 0, 128))
            img.save(media_path, format='PNG')
        # Cr√©er le sidecar complexe
        sidecar_data = {
            "title": filename,
            "description": description,
            "people": [{"name": "Mixed Test Person"}],
            "favorited": True,
            "geoData": {
                "latitude": 45.5017,
                "longitude": -73.5673,
                "altitude": 20.0
            }
        }
        json_path = tmp_path / f"{filename}.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    try:
        # Traiter avec le mode par lot
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
        # V√©rifier que tous les fichiers ont √©t√© trait√©s
        for filename, expected_description in test_files:
            media_path = tmp_path / filename
            metadata = _run_exiftool_read(media_path)
            # V√©rifier la description
            assert metadata.get("ImageDescription") == expected_description
            # V√©rifier les personnes
            people = metadata.get("PersonInImage", [])
            if isinstance(people, str):
                people = [people]
            assert "Mixed Test Person" in people
            # V√©rifier la note (favori)
            rating = metadata.get("Rating")
            assert rating == 5 or rating == "5"
            # V√©rifier les donn√©es GPS (peut ne pas fonctionner pour tous les types de fichiers)
            gps_lat = metadata.get("GPSLatitude")
            if gps_lat is not None:
                # Si GPSLatitude est pr√©sent, v√©rifier qu'il est correct
                assert "45 deg" in str(gps_lat)
                assert gps_lat is not None
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping mixed file types batch test")
````

## File: tests/test_processor_batch.py
````python
"""Tests pour la fonctionnalit√© de traitement par lots."""
import json
import subprocess
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from PIL import Image
from google_takeout_metadata.processor_batch import process_batch, process_directory_batch
from google_takeout_metadata.sidecar import SidecarData
def test_process_batch_empty_batch():
    """Tester que process_batch retourne 0 pour un lot vide."""
    result = process_batch([], clean_sidecars=False)
    assert result == 0
@patch('google_takeout_metadata.processor_batch.subprocess.run')
def test_process_batch_success(mock_subprocess_run, tmp_path):
    """Tester le traitement par lots r√©ussi."""
    # Configuration
    mock_subprocess_run.return_value = Mock(returncode=0, stdout="    1 image files updated")
    media_path = tmp_path / "test.jpg"
    json_path = tmp_path / "test.jpg.json"
    args = ["-EXIF:ImageDescription=Test Description"]
    batch = [(media_path, json_path, args)]
    # Ex√©cution
    result = process_batch(batch, clean_sidecars=False)
    # V√©rification
    assert result == 1
    mock_subprocess_run.assert_called_once()
    # V√©rifier que la commande a √©t√© construite correctement
    call_args = mock_subprocess_run.call_args
    cmd = call_args[0][0]
    assert cmd[0] == "exiftool"
    assert "-overwrite_original" in cmd
    assert "-charset" in cmd
    assert "-@" in cmd
@patch('google_takeout_metadata.processor_batch.subprocess.run')
def test_process_batch_with_argfile_content(mock_subprocess_run, tmp_path):
    """V√©rifier que le fichier d'arguments est cr√©√© avec le contenu correct."""
    # Setup
    mock_subprocess_run.return_value = Mock(returncode=0, stdout="    2 image files updated")
    media_path1 = tmp_path / "test1.jpg"
    media_path2 = tmp_path / "test2.jpg"
    json_path1 = tmp_path / "test1.jpg.json"
    json_path2 = tmp_path / "test2.jpg.json"
    args1 = ["-EXIF:ImageDescription=Description 1", "-XMP:Rating=5"]
    args2 = ["-EXIF:ImageDescription=Description 2"]
    batch = [
        (media_path1, json_path1, args1),
        (media_path2, json_path2, args2)
    ]
    # Execute
    result = process_batch(batch, clean_sidecars=False)
    # Assert
    assert result == 2
    mock_subprocess_run.assert_called_once()
    # V√©rifier que le chemin du fichier d'arguments a √©t√© pass√© au sous-processus
    call_args = mock_subprocess_run.call_args
    cmd = call_args[0][0]
    assert "-@" in cmd
    # Le chemin du fichier d'arguments devrait √™tre l'argument juste apr√®s "-@"
    argfile_index = cmd.index("-@")
    assert argfile_index + 1 < len(cmd)  # S'assurer qu'il y a un argument apr√®s "-@"
@patch('google_takeout_metadata.processor_batch.subprocess.run')
def test_process_batch_clean_sidecars(mock_subprocess_run, tmp_path):
    """V√©rifier que les fichiers de sidecar sont nettoy√©s lorsqu'on le demande."""
    # Setup
    mock_subprocess_run.return_value = Mock(returncode=0, stdout="    1 image files updated")
    media_path = tmp_path / "test.jpg"
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text('{"title": "test.jpg"}')
    args = ["-EXIF:ImageDescription=Test Description"]
    batch = [(media_path, json_path, args)]
    # Execute
    result = process_batch(batch, clean_sidecars=True)
    # Assert
    assert result == 1
    assert not json_path.exists()
@patch('google_takeout_metadata.processor_batch.subprocess.run')
def test_process_batch_exiftool_not_found(mock_subprocess_run):
    """V√©rifier la gestion d'erreurs lorsque exiftool n'est pas trouv√©."""
    # Setup
    mock_subprocess_run.side_effect = FileNotFoundError("exiftool introuvable")
    media_path = Path("test.jpg")
    json_path = Path("test.jpg.json")
    args = ["-EXIF:ImageDescription=Test Description"]
    batch = [(media_path, json_path, args)]
    # Execute & Assert
    with pytest.raises(RuntimeError, match="exiftool introuvable"):
        process_batch(batch, clean_sidecars=False)
@patch('google_takeout_metadata.processor_batch.subprocess.run')
def test_process_batch_exiftool_error(mock_subprocess_run, caplog):
    """V√©rifier la gestion d'erreurs lorsque exiftool retourne une erreur."""
    # Setup
    mock_subprocess_run.side_effect = subprocess.CalledProcessError(
        1, ["exiftool"], stderr="Some error"
    )
    media_path = Path("test.jpg")
    json_path = Path("test.jpg.json")
    args = ["-EXIF:ImageDescription=Test Description"]
    batch = [(media_path, json_path, args)]
    # Execute
    result = process_batch(batch, clean_sidecars=False)
    # Assert
    assert result == 0
    assert "√âchec du traitement par lot" in caplog.text
def test_process_directory_batch_no_sidecars(tmp_path, caplog):
    """V√©rifier le traitement par lot lorsque aucun fichier de sidecar n'est trouv√©."""
    # Execute
    process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
    # Assert
    assert "Aucun fichier de m√©tadonn√©es (.json) trouv√©" in caplog.text
@pytest.mark.integration
def test_process_directory_batch_single_file(tmp_path):
    """V√©rifier le traitement par lot d'un seul fichier."""
    try:
        # Cr√©er une image de test
        media_path = tmp_path / "test.jpg"
        img = Image.new('RGB', (100, 100), color='blue')
        img.save(media_path)
        # Cr√©er le fichier JSON annexe
        sidecar_data = {
            "title": "test.jpg",
            "description": "Batch test description",
            "people": [{"name": "Batch Test Person"}]
        }
        json_path = tmp_path / "test.jpg.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Traiter en mode batch
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
        # V√©rifier que les m√©tadonn√©es ont √©t√© √©crites en les relisant
        cmd = [
            "exiftool",
            "-j",
            "-EXIF:ImageDescription",
            "-XMP-iptcExt:PersonInImage",
            str(media_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        metadata = json.loads(result.stdout)[0]
        assert metadata.get("ImageDescription") == "Batch test description"
        people = metadata.get("PersonInImage", [])
        if isinstance(people, str):
            people = [people]
        assert "Batch Test Person" in people
    except FileNotFoundError:
        pytest.skip("Exiftool non trouv√© - ignore les tests d'int√©gration")
@pytest.mark.integration  
def test_process_directory_batch_multiple_files(tmp_path):
    """V√©rifier le traitement par lot de plusieurs fichiers."""
    try:
        # Cr√©er plusieurs images de test avec leurs fichiers annexes
        files_data = [
            ("test1.jpg", "First batch test", "Person One"),
            ("test2.jpg", "Second batch test", "Person Two"),
            ("test3.jpg", "Third batch test", "Person Three")
        ]
        for filename, description, person in files_data:
            # Cr√©er l'image
            media_path = tmp_path / filename
            img = Image.new('RGB', (100, 100), color='red')
            img.save(media_path)
            # Cr√©er le fichier annexe
            sidecar_data = {
                "title": filename,
                "description": description,
                "people": [{"name": person}]
            }
            json_path = tmp_path / f"{filename}.json"
            json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Traiter en mode batch
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
        # V√©rifier que tous les fichiers ont √©t√© trait√©s correctement
        for filename, expected_description, expected_person in files_data:
            media_path = tmp_path / filename
            cmd = [
                "exiftool",
                "-j",
                "-EXIF:ImageDescription",
                "-XMP-iptcExt:PersonInImage",
                str(media_path)
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
            metadata = json.loads(result.stdout)[0]
            assert metadata.get("ImageDescription") == expected_description
            people = metadata.get("PersonInImage", [])
            if isinstance(people, str):
                people = [people]
            assert expected_person in people
    except FileNotFoundError:
        pytest.skip("Exiftool non trouv√© - ignore les tests d'int√©gration")
@pytest.mark.integration
def test_process_directory_batch_with_albums(tmp_path):
    """V√©rifier le traitement par lot avec des m√©tadonn√©es d'album."""
    try:
        # Cr√©er la structure de r√©pertoires
        album_dir = tmp_path / "Album Test"
        album_dir.mkdir()
        # Cr√©er les m√©tadonn√©es d'album
        album_metadata = {
            "title": "Test Album",
            "description": "Album for batch testing"
        }
        metadata_path = album_dir / "metadata.json"
        metadata_path.write_text(json.dumps(album_metadata), encoding="utf-8")
        # Cr√©er l'image de test dans le r√©pertoire d'album
        media_path = album_dir / "album_photo.jpg"
        img = Image.new('RGB', (100, 100), color='green')
        img.save(media_path)
        # Cr√©er le fichier annexe
        sidecar_data = {
            "title": "album_photo.jpg",
            "description": "Photo in album batch test"
        }
        json_path = album_dir / "album_photo.jpg.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Traiter en mode batch
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
        # V√©rifier que l'album a √©t√© ajout√© aux mots-cl√©s
        cmd = [
            "exiftool",
            "-j",
            "-IPTC:Keywords",
            "-XMP-dc:Subject",
            str(media_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        metadata = json.loads(result.stdout)[0]
        keywords = metadata.get("Keywords", [])
        if isinstance(keywords, str):
            keywords = [keywords]
        assert "Album: Test Album" in keywords
    except FileNotFoundError:
        pytest.skip("Exiftool non trouv√© - ignore les tests d'int√©gration")
@pytest.mark.integration
def test_process_directory_batch_clean_sidecars(tmp_path):
    """Test d'int√©gration pour le traitement par lot avec nettoyage des sidecars."""
    try:
        # Cr√©er une image de test
        media_path = tmp_path / "cleanup_test.jpg"
        img = Image.new('RGB', (100, 100), color='yellow')
        img.save(media_path)
        # Cr√©er le fichier JSON annexe
        sidecar_data = {
            "title": "cleanup_test.jpg",
            "description": "Test cleanup functionality"
        }
        json_path = tmp_path / "cleanup_test.jpg.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # V√©rifier que le fichier annexe existe avant le traitement
        assert json_path.exists()
        # Traiter avec le nettoyage activ√©
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=True)
        # V√©rifier que le fichier annexe a √©t√© nettoy√©
        assert not json_path.exists()
        # V√©rifier que les m√©tadonn√©es ont quand m√™me √©t√© √©crites
        cmd = [
            "exiftool",
            "-j",
            "-EXIF:ImageDescription",
            str(media_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        metadata = json.loads(result.stdout)[0]
        assert metadata.get("ImageDescription") == "Test cleanup functionality"
    except FileNotFoundError:
        pytest.skip("Exiftool non trouv√© - ignore les tests d'int√©gration")
@patch('google_takeout_metadata.processor_batch.parse_sidecar')
def test_process_directory_batch_invalid_sidecar(mock_parse_sidecar, tmp_path, caplog):
    """Tester le traitement par lot avec un fichier sidecar invalide."""
    # Configuration
    mock_parse_sidecar.side_effect = ValueError("Invalid JSON")
    # Cr√©er des fichiers factices
    media_path = tmp_path / "invalid.jpg"
    media_path.write_text("dummy")
    json_path = tmp_path / "invalid.jpg.json"
    json_path.write_text("invalid json")
    # Ex√©cuter
    process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
    # V√©rifier
    assert "√âchec de la pr√©paration de" in caplog.text
@patch('google_takeout_metadata.processor_batch.build_exiftool_args')
def test_process_directory_batch_no_args_generated(mock_build_args, tmp_path):
    """Tester le traitement par lot quand aucun argument exiftool n'est g√©n√©r√©."""
    # Configuration - build_exiftool_args retourne une liste vide
    mock_build_args.return_value = []
    # Cr√©er l'image de test
    media_path = tmp_path / "no_args.jpg"
    img = Image.new('RGB', (100, 100), color='white')
    img.save(media_path)
    # Cr√©er le fichier JSON annexe
    sidecar_data = {"title": "no_args.jpg"}
    json_path = tmp_path / "no_args.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Ex√©cuter (ne devrait pas planter m√™me sans arguments)
    process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
    # Aucune assertion sp√©cifique n√©cessaire - juste s'assurer que √ßa ne plante pas
def test_process_directory_batch_missing_media_file(tmp_path, caplog):
    """Tester le traitement par lot quand le fichier m√©dia est manquant."""
    # Cr√©er un fichier annexe sans fichier m√©dia correspondant
    sidecar_data = {
        "title": "missing.jpg",
        "description": "Media file does not exist"
    }
    json_path = tmp_path / "missing.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Ex√©cuter
    process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
    # V√©rifier
    assert "Fichier image introuvable" in caplog.text
@patch('google_takeout_metadata.processor_batch.fix_file_extension_mismatch')
@patch('google_takeout_metadata.processor_batch.parse_sidecar')
def test_process_directory_batch_file_extension_fix(mock_parse_sidecar, mock_fix_extension, tmp_path):
    """Tester que la correction de l'extension de fichier est g√©r√©e dans le traitement par lot."""
    # Configuration
    media_path = tmp_path / "test.jpg"
    json_path = tmp_path / "test.jpg.json"
    fixed_media_path = tmp_path / "test.jpeg"
    fixed_json_path = tmp_path / "test.jpeg.json"
    # Cr√©er les fichiers
    img = Image.new('RGB', (100, 100), color='black')
    img.save(media_path)
    json_path.write_text('{"title": "test.jpg"}')
    # Simuler la correction d'extension pour retourner des chemins diff√©rents
    mock_fix_extension.return_value = (fixed_media_path, fixed_json_path)
    # Simuler parse_sidecar pour retourner des donn√©es diff√©rentes pour chaque appel
    mock_parse_sidecar.side_effect = [
        SidecarData(filename="test.jpg", description="Original", people=[], taken_at=None, created_at=None, 
                   latitude=None, longitude=None, altitude=None, favorite=False, albums=[]),
        SidecarData(filename="test.jpeg", description="Fixed", people=[], taken_at=None, created_at=None,
                   latitude=None, longitude=None, altitude=None, favorite=False, albums=[])
    ]
    # Ex√©cuter
    process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
    # V√©rifier que fix_file_extension_mismatch a √©t√© appel√©
    mock_fix_extension.assert_called()
    # V√©rifier que parse_sidecar a √©t√© appel√© deux fois (une fois pour l'original, une fois pour le corrig√©)
    assert mock_parse_sidecar.call_count == 2
````

## File: tests/test_processor.py
````python
from pathlib import Path
import json
import unittest.mock
import os
from google_takeout_metadata.processor import (
    process_directory, 
    _is_sidecar_file, 
    fix_file_extension_mismatch
)
def test_ignore_non_sidecar(tmp_path: Path) -> None:
    (tmp_path / "data.json").write_text("{}", encoding="utf-8")
    process_directory(tmp_path)
def test_is_sidecar_file_standard_pattern() -> None:
    """Test standard pattern: photo.jpg.json"""
    assert _is_sidecar_file(Path("photo.jpg.json"))
    assert _is_sidecar_file(Path("video.mp4.json"))
    assert _is_sidecar_file(Path("image.PNG.JSON"))  # insensible √† la casse
def test_is_sidecar_file_supplemental_metadata_pattern() -> None:
    """V√©rifier le format Google Takeout: photo.jpg.supplemental-metadata.json"""
    assert _is_sidecar_file(Path("IMG_001.jpg.supplemental-metadata.json"))
    assert _is_sidecar_file(Path("video.mp4.supplemental-metadata.json"))
    assert _is_sidecar_file(Path("image.PNG.SUPPLEMENTAL-METADATA.JSON"))  # insensible √† la casse
    assert _is_sidecar_file(Path("photo.heic.supplemental-metadata.json"))
def test_is_sidecar_file_older_pattern() -> None:
    """V√©rifier l'ancien format: photo.json"""
    assert _is_sidecar_file(Path("IMG_1234.jpg.json"))  # devrait fonctionner avec la nouvelle logique
    # Note: photo.json sans extension dans le nom ne serait pas d√©tect√©
    # car c'est ambigu, mais c'est acceptable puisque parse_sidecar() valide
def test_is_sidecar_file_negative() -> None:
    """v√©rifier les fichiers qui ne devraient pas √™tre d√©tect√©s comme sidecars"""
    assert not _is_sidecar_file(Path("data.json"))  # pas d'extension d'image
    assert not _is_sidecar_file(Path("photo.txt"))  # pas un json
    assert not _is_sidecar_file(Path("photo.jpg"))  # pas un json
    assert not _is_sidecar_file(Path("metadata.json"))  # album metadata, pas un sidecar
    assert not _is_sidecar_file(Path("m√©tadonn√©es.json"))  # album metadata, pas un sidecar
def test_fix_file_extension_mismatch_rollback_on_failure(tmp_path: Path) -> None:
    """V√©rifier que fix_file_extension_mismatch annule correctement le renommage de l'image en cas d'√©chec"""
    # Cr√©er un faux fichier JPEG avec une mauvaise extension
    media_path = tmp_path / "photo.png"
    media_path.write_bytes(b'\xff\xd8\xff\xe0')  # JPEG magic bytes
    # Cr√©er le fichier JSON correspondant
    json_path = tmp_path / "photo.png.supplemental-metadata.json"
    json_data = {"title": "photo.png"}
    json_path.write_text(json.dumps(json_data), encoding='utf-8')
    # Simuler un √©chec de unlink pour le fichier JSON (fichier en lecture seule)
    original_unlink = Path.unlink
    def mock_unlink(self):
        if self.name.endswith('.supplemental-metadata.json') and 'photo.png' in str(self):
            raise OSError("Permission denied")
        return original_unlink(self)
    with unittest.mock.patch.object(Path, 'unlink', mock_unlink):
        result_image, result_json = fix_file_extension_mismatch(media_path, json_path)
        # Devrait retourner les chemins d'origine car le rollback a r√©ussi
        assert result_image == media_path
        assert result_json == json_path
        assert media_path.exists()  # L'image originale devrait exister √† nouveau
        assert not (tmp_path / "photo.jpg").exists()  # L'image renomm√©e ne devrait pas exister
        assert not (tmp_path / "photo.jpg.supplemental-metadata.json").exists()  # Pas de JSON orphelin attendu
def test_fix_file_extension_mismatch_failed_rollback(tmp_path: Path) -> None:
    """Tester fix_file_extension_mismatch lorsque l'op√©ration et le rollback √©chouent tous les deux"""
    # Cr√©er un faux fichier JPEG avec une mauvaise extension
    media_path = tmp_path / "photo.png"
    media_path.write_bytes(b'\xff\xd8\xff\xe0')  # JPEG magic bytes
    # Cr√©er le fichier JSON correspondant
    json_path = tmp_path / "photo.png.supplemental-metadata.json"
    json_data = {"title": "photo.png"}
    json_path.write_text(json.dumps(json_data), encoding='utf-8')
    # Simuler un √©chec √† la fois pour le renommage de l'image et pour le rollback du JSON
    original_unlink = Path.unlink
    def mock_unlink(self):
        if self.name.endswith('.supplemental-metadata.json'):
            raise OSError("Permission denied")
        return original_unlink(self)
    def mock_rename(self, target):
        # Simuler un √©chec lors du rollback du renommage
        if str(target).endswith('.png') and str(self).endswith('.jpg'):
            raise OSError("Rollback failed")
        # Sinon, faire le renommage r√©el
        os.rename(str(self), str(target))
    with unittest.mock.patch.object(Path, 'unlink', mock_unlink), \
         unittest.mock.patch.object(Path, 'rename', mock_rename):
        result_image, result_json = fix_file_extension_mismatch(media_path, json_path)
        # Devrait retourner le nouveau chemin de l'image mais l'ancien chemin du JSON en raison du rollback √©chou√©
        assert result_image == tmp_path / "photo.jpg"
        assert result_json == json_path  # Chemin JSON d'origine
        assert (tmp_path / "photo.jpg").exists()  # La nouvelle image devrait exister
        assert not media_path.exists()  # L'image originale ne devrait pas exister
````

## File: tests/test_sidecar.py
````python
from pathlib import Path
import json
import pytest
from google_takeout_metadata.sidecar import parse_sidecar
def test_parse_sidecar(tmp_path: Path) -> None:
    sample = {
        "title": "1729436788572.jpg",
        "description": "Magicien en or",
        "creationTime": {"timestamp": "1736719606"},
        "photoTakenTime": {"timestamp": "1736719606"},
        "geoData": {"latitude": 0.0, "longitude": 0.0, "altitude": 0.0},
        "people": [{"name": "anthony vincent"}],
    }
    json_path = tmp_path / "1729436788572.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.filename == "1729436788572.jpg"
    assert meta.description == "Magicien en or"
    assert meta.people == ["anthony vincent"]
    assert meta.taken_at == 1736719606
    assert meta.created_at == 1736719606
def test_title_mismatch(tmp_path: Path) -> None:
    data = {"title": "other.jpg"}
    json_path = tmp_path / "sample.jpg.json"
    json_path.write_text(json.dumps(data), encoding="utf-8")
    with pytest.raises(ValueError):
        parse_sidecar(json_path)
def test_parse_sidecar_supplemental_metadata_format(tmp_path: Path) -> None:
    """Tester l'analyse du nouveau format Google Takeout : IMG_001.jpg.supplemental-metadata.json"""
    sample = {
        "title": "IMG_001.jpg",
        "description": "Test photo with new format",
        "creationTime": {"timestamp": "1736719606"},
        "photoTakenTime": {"timestamp": "1736719606"},
        "people": [{"name": "test user"}],
    }
    json_path = tmp_path / "IMG_001.jpg.supplemental-metadata.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.filename == "IMG_001.jpg"
    assert meta.description == "Test photo with new format"
    assert meta.people == ["test user"]
    assert meta.taken_at == 1736719606
    assert meta.created_at == 1736719606
def test_title_mismatch_supplemental_metadata(tmp_path: Path) -> None:
    """Tester la validation du titre avec le format supplemental-metadata."""
    data = {"title": "wrong_name.jpg"}
    json_path = tmp_path / "IMG_001.jpg.supplemental-metadata.json"
    json_path.write_text(json.dumps(data), encoding="utf-8")
    with pytest.raises(ValueError, match="Le titre du sidecar.*ne correspond pas au nom de fichier attendu"):
        parse_sidecar(json_path)
def test_invalid_json(tmp_path: Path) -> None:
    json_path = tmp_path / "bad.jpg.json"
    json_path.write_text("not json", encoding="utf-8")
    with pytest.raises(ValueError):
        parse_sidecar(json_path)
def test_zero_coordinates(tmp_path: Path) -> None:
    sample = {
        "title": "a.jpg",
        "geoData": {"latitude": 0.0, "longitude": 0.0, "altitude": 10.0, "latitudeSpan": 1, "longitudeSpan": 1},
    }
    json_path = tmp_path / "a.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    # Les coordonn√©es 0/0 doivent √™tre filtr√©es car peu fiables
    assert meta.latitude is None
    assert meta.longitude is None
    assert meta.altitude is None
def test_people_deduplication(tmp_path: Path) -> None:
    """Tester que les noms de personnes sont d√©dupliqu√©s et nettoy√©s."""
    sample = {
        "title": "a.jpg",
        "people": [
            {"name": "alice"},
            {"name": " alice "},  # avec espaces
            {"name": "alice"},   # doublon
            {"name": "bob"},
            {"name": "  "},      # vide apr√®s nettoyage
            {"name": "charlie"},
            {"name": " bob "},   # autre doublon avec espaces
        ]
    }
    json_path = tmp_path / "a.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    # Devrait avoir d√©dupliqu√© et nettoy√© : ["alice", "bob", "charlie"]
    assert meta.people == ["alice", "bob", "charlie"]
def test_parse_favorite_true(tmp_path: Path) -> None:
    """Tester l'analyse d'une photo favorite avec le format Google Takeout r√©el."""
    sample = {
        "title": "favorite.jpg",
        "favorited": True
    }
    json_path = tmp_path / "favorite.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.favorite is True
def test_parse_favorite_false(tmp_path: Path) -> None:
    """Tester l'analyse d'une photo non favorite avec le format Google Takeout r√©el."""
    sample = {
        "title": "not_favorite.jpg",
        "favorited": False
    }
    json_path = tmp_path / "not_favorite.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.favorite is False
def test_parse_no_favorite_field(tmp_path: Path) -> None:
    """Tester l'analyse d'une photo sans champ favori."""
    sample = {
        "title": "no_fav.jpg",
        "description": "Test photo"
    }
    json_path = tmp_path / "no_fav.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.favorite is False
def test_parse_zero_geo_coordinates(tmp_path: Path) -> None:
    """Tester que les coordonn√©es 0/0 sont filtr√©es car peu fiables."""
    sample = {
        "title": "geo_zero.jpg",
        "geoData": {"latitude": 0.0, "longitude": 0.0, "altitude": 100.0}
    }
    json_path = tmp_path / "geo_zero.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    # Les coordonn√©es 0/0 doivent √™tre filtr√©es
    assert meta.latitude is None
    assert meta.longitude is None
    assert meta.altitude is None
def test_parse_valid_geo_coordinates(tmp_path: Path) -> None:
    """Tester que les coordonn√©es valides sont pr√©serv√©es."""
    sample = {
        "title": "geo_valid.jpg",
        "geoData": {"latitude": 48.8566, "longitude": 2.3522, "altitude": 35.0}
    }
    json_path = tmp_path / "geo_valid.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.latitude == 48.8566
    assert meta.longitude == 2.3522
    assert meta.altitude == 35.0
def test_parse_people_nested_format(tmp_path: Path) -> None:
    """Tester l'analyse des personnes au format imbriqu√© : [{"person": {"name": "X"}}]."""
    sample = {
        "title": "nested_people.jpg",
        "people": [
            {"person": {"name": "alice"}},
            {"person": {"name": "bob"}},
            {"name": "charlie"}  # format mixte
        ]
    }
    json_path = tmp_path / "nested_people.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.people == ["alice", "bob", "charlie"]
def test_parse_missing_timestamps(tmp_path: Path) -> None:
    """Tester l'analyse quand les horodatages sont manquants."""
    sample = {
        "title": "no_dates.jpg",
        "description": "Photo without dates"
    }
    json_path = tmp_path / "no_dates.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.taken_at is None
    assert meta.created_at is None
def test_parse_album_metadata(tmp_path: Path) -> None:
    """Tester l'analyse des m√©tadonn√©es d'album depuis les fichiers metadata.json."""
    from google_takeout_metadata.sidecar import parse_album_metadata
    # Tester les m√©tadonn√©es d'album de base
    album_data = {
        "title": "Vacances 2024",
        "description": "Photos des vacances d'√©t√©"
    }
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    albums = parse_album_metadata(metadata_path)
    assert albums == ["Vacances 2024"]
def test_parse_album_metadata_multiple_albums(tmp_path: Path) -> None:
    """Tester l'analyse de plusieurs r√©f√©rences d'albums."""
    from google_takeout_metadata.sidecar import parse_album_metadata
    album_data = {
        "title": "Album Principal",
        "albums": [
            {"title": "Sous-album 1"},
            {"title": "Sous-album 2"},
            "Album Simple"
        ]
    }
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    albums = parse_album_metadata(metadata_path)
    assert set(albums) == {"Album Principal", "Album Simple", "Sous-album 1", "Sous-album 2"}
def test_find_albums_for_directory(tmp_path: Path) -> None:
    """Tester la recherche d'albums pour un r√©pertoire."""
    from google_takeout_metadata.sidecar import find_albums_for_directory
    # Cr√©er les m√©tadonn√©es d'album
    album_data = {"title": "Mon Album"}
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    albums = find_albums_for_directory(tmp_path)
    assert albums == ["Mon Album"]
def test_find_albums_for_directory_no_metadata(tmp_path: Path) -> None:
    """Tester la recherche d'albums quand aucune m√©tadonn√©e n'existe."""
    from google_takeout_metadata.sidecar import find_albums_for_directory
    albums = find_albums_for_directory(tmp_path)
    assert albums == []
def test_find_albums_french_metadata_format(tmp_path: Path) -> None:
    """Tester la recherche d'albums avec le format de fichier de m√©tadonn√©es fran√ßais."""
    from google_takeout_metadata.sidecar import find_albums_for_directory
    # Cr√©er les m√©tadonn√©es d'album fran√ßais
    album_data = {"title": "Mon Album Fran√ßais"}
    metadata_path = tmp_path / "m√©tadonn√©es.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    albums = find_albums_for_directory(tmp_path)
    assert albums == ["Mon Album Fran√ßais"]
def test_find_albums_french_numbered_metadata(tmp_path: Path) -> None:
    """Tester la recherche d'albums avec des fichiers de m√©tadonn√©es fran√ßais num√©rot√©s."""
    from google_takeout_metadata.sidecar import find_albums_for_directory
    # Cr√©er plusieurs fichiers de m√©tadonn√©es fran√ßais
    album_data1 = {"title": "Album 1"}
    metadata_path1 = tmp_path / "m√©tadonn√©es.json"
    metadata_path1.write_text(json.dumps(album_data1), encoding="utf-8")
    album_data2 = {"title": "Album 2"}
    metadata_path2 = tmp_path / "m√©tadonn√©es(1).json"
    metadata_path2.write_text(json.dumps(album_data2), encoding="utf-8")
    album_data3 = {"title": "Album 3"}
    metadata_path3 = tmp_path / "m√©tadonn√©es(2).json"
    metadata_path3.write_text(json.dumps(album_data3), encoding="utf-8")
    albums = find_albums_for_directory(tmp_path)
    assert set(albums) == {"Album 1", "Album 2", "Album 3"}
def test_find_albums_mixed_formats(tmp_path: Path) -> None:
    """Tester la recherche d'albums avec des fichiers de m√©tadonn√©es mixtes anglais et fran√ßais."""
    from google_takeout_metadata.sidecar import find_albums_for_directory
    # Cr√©er les m√©tadonn√©es anglais
    album_data_en = {"title": "English Album"}
    metadata_path_en = tmp_path / "metadata.json"
    metadata_path_en.write_text(json.dumps(album_data_en), encoding="utf-8")
    # Cr√©er les m√©tadonn√©es fran√ßais
    album_data_fr = {"title": "Album Fran√ßais"}
    metadata_path_fr = tmp_path / "m√©tadonn√©es.json"
    metadata_path_fr.write_text(json.dumps(album_data_fr), encoding="utf-8")
    albums = find_albums_for_directory(tmp_path)
    assert set(albums) == {"Album Fran√ßais", "English Album"}
def test_sidecar_with_albums_from_directory(tmp_path: Path) -> None:
    """Tester que les albums sont ajout√©s depuis les m√©tadonn√©es de r√©pertoire lors du traitement des sidecars."""
    from google_takeout_metadata.processor import process_sidecar_file
    from google_takeout_metadata.sidecar import parse_sidecar
    # Cr√©er les m√©tadonn√©es d'album
    album_data = {"title": "Album Test"}
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    # Cr√©er un fichier image factice
    media_path = tmp_path / "test.jpg"
    with open(media_path, 'wb') as f:
        f.write(b'\xFF\xD8\xFF\xE0')  # En-t√™te JPEG minimal
    # Cr√©er le sidecar
    sidecar_data = {
        "title": "test.jpg",
        "description": "Test photo"
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Analyser le sidecar - les albums devraient √™tre vides initialement
    meta = parse_sidecar(json_path)
    assert meta.albums == []
    # Note: Nous ne pouvons pas tester process_sidecar_file sans exiftool
    # mais nous pouvons tester la logique de recherche d'albums s√©par√©ment
````
