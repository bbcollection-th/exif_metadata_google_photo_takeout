# Directory Structure
```
pyproject.toml
pytest.ini
README.md
requirements.txt
src/google_takeout_metadata/__init__.py
src/google_takeout_metadata/__main__.py
src/google_takeout_metadata/cli.py
src/google_takeout_metadata/exif_writer.py
src/google_takeout_metadata/processor_batch.py
src/google_takeout_metadata/processor.py
src/google_takeout_metadata/sidecar.py
src/google_takeout_metadata/statistics.py
test_assets/README.md
tests/test_hybrid_approach.py
tests/test_cli.py
tests/test_end_to_end.py
tests/test_exif_writer.py
tests/test_hybrid_approach.py
tests/test_improvements.py
tests/test_integration.py
tests/test_processor_batch.py
tests/test_processor.py
tests/test_sidecar.py
```

# Files

## File: pyproject.toml
````toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "google_takeout_metadata"
version = "0.1.0"
description = "Merge Google Takeout metadata into images"
readme = "README.md"
license = {file = "LICENSE"}
authors = [
    {name = "Anthony", email = "anthony@example.com"}
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: End Users/Desktop",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
]
dependencies = []

[project.optional-dependencies]
test = ["pytest", "pillow"]

[project.scripts]
google-takeout-metadata = "google_takeout_metadata.cli:main"

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
testpaths = ["tests"]
markers = [
    "integration: marks tests as integration tests (requiring exiftool)",
]
````

## File: pytest.ini
````
[pytest]
addopts = -ra
pythonpath = src
markers =
    integration: marks tests as integration tests requiring exiftool
````

## File: README.md
````markdown
# exif_metadata_google_photo_takeout

Ce projet permet d'incorporer les métadonnées des fichiers JSON produits par Google Takeout dans les photos correspondantes.

## Fonctionnalités

✅ **Métadonnées supportées:**
- Descriptions/légendes
- Personnes identifiées (avec déduplication automatique)
- Dates de prise de vue et de création
- Coordonnées GPS (filtrage automatique des coordonnées 0/0 peu fiables)
- Favoris (mappés sur le tag Favorite booléen)
- **Albums** (détectés depuis les fichiers metadata.json de dossier et ajoutés comme mots-clés "Album: <nom>")

✅ **Mode de fonctionnement sûr par défaut:**
- **Append-only par défaut**: Les métadonnées existantes sont préservées
- Les descriptions ne sont écrites que si elles n'existent pas déjà
- Les personnes et albums sont ajoutés aux listes existantes sans suppression
- Utiliser `--overwrite` pour forcer l'écrasement des métadonnées existantes

✅ **Options avancées:**
- `--localtime`: Conversion des timestamps en heure locale au lieu d'UTC
- `--overwrite`: Force l'écrasement des métadonnées existantes (mode destructif)
- `--batch`: Mode batch pour traitement optimisé de gros volumes de fichiers

✅ **Qualité:**
- Tests unitaires complets
- Tests d'intégration E2E avec exiftool
- Support des formats photo et vidéo
- **Arguments sécurisés** : Protection contre l'injection shell avec noms contenant des espaces
- **Opérateur `+=` optimisé** : Utilise l'opérateur exiftool `+=` pour accumulation sûre des tags de type liste

## Installation

Prérequis: `exiftool` doit être installé et accessible dans le PATH.

```bash
pip install -e .
```

## Utilisation

### Utilisation basique (mode sûr par défaut)
```bash
# Mode append-only par défaut - préserve les métadonnées existantes
google-takeout-metadata /chemin/vers/le/dossier
```

### Avec options
```bash
# Utiliser l'heure locale pour les timestamps
google-takeout-metadata --localtime /chemin/vers/le/dossier

# Mode destructif: écraser les métadonnées existantes (à utiliser avec précaution)
google-takeout-metadata --overwrite /chemin/vers/le/dossier

# Nettoyer les fichiers sidecars après traitement
google-takeout-metadata --clean-sidecars /chemin/vers/le/dossier

# Combiner les options (mode sûr avec heure locale)
google-takeout-metadata --localtime /chemin/vers/le/dossier
```

### Mode batch (optimisé pour gros volumes)
```bash
# Mode batch: traitement optimisé pour de nombreux fichiers
google-takeout-metadata --batch /chemin/vers/le/dossier

# Mode batch avec autres options
google-takeout-metadata --batch --localtime /chemin/vers/le/dossier
google-takeout-metadata --batch --overwrite /chemin/vers/le/dossier

# Exemple concret avec toutes les options (pointer vers le dossier Takeout)
google-takeout-metadata --batch --localtime --clean-sidecars "C:\Users\anthony\Downloads\google photos\Takeout"
```

**Si la commande `google-takeout-metadata` n'est pas trouvée:**
```bash
# Option 1: Utiliser le module Python directement (attention aux underscores)
python -m google_takeout_metadata --batch --localtime --clean-sidecars "/chemin/vers/dossier"

# Option 2: Utiliser l'environnement virtuel complet avec le module
C:/Users/anthony/Documents/PROJETS/exif_metadata_google_photo_takeout/.venv/Scripts/python.exe -m google_takeout_metadata --batch --localtime --clean-sidecars "C:\Users\anthony\Downloads\google photos\Takeout"

# Option 3: Utiliser l'exécutable directement depuis l'environnement virtuel
C:/Users/anthony/Documents/PROJETS/exif_metadata_google_photo_takeout/.venv/Scripts/google-takeout-metadata.exe --batch --localtime --clean-sidecars "C:\Users\anthony\Downloads\google photos\Takeout"

# Option 4: Activer l'environnement virtuel d'abord
.venv/Scripts/activate  # Sur Windows
google-takeout-metadata --batch --localtime --clean-sidecars "/chemin/vers/dossier"
```

**Avantages du mode batch:**
- **Performance améliorée** : Traitement par lots avec exiftool pour réduire le nombre d'appels système
- **Idéal pour gros volumes** : Optimisé pour traiter des milliers de fichiers
- **Moins de fragmentation** : Réduit la charge système en groupant les opérations
- **Même sécurité** : Conserve le comportement append-only par défaut

**Quand utiliser le mode batch:**
- Traitement de bibliothèques photo importantes (>100 fichiers)
- Archives Google Takeout volumineuses
- Situations où la performance est critique

**Note de performance:**
Le mode batch réduit significativement le temps de traitement en groupant les appels à exiftool. 
Pour 1000 fichiers, le gain peut être de 50-80% selon la configuration système.

Le programme parcourt récursivement le dossier, cherche les fichiers `*.json` et écrit les informations pertinentes dans les fichiers image correspondants à l'aide d'`exiftool`.

## Comportement par défaut (Sécurisé)

**Le mode append-only est désormais activé par défaut** pour éviter la perte accidentelle de métadonnées:

### ✅ Métadonnées préservées:
- **Descriptions existantes** ne sont jamais écrasées
- **Dates existantes** ne sont jamais modifiées
- **Coordonnées GPS existantes** ne sont jamais remplacées
- **Ratings existants** ne sont jamais changés

### ✅ Métadonnées ajoutées:
- **Personnes** sont ajoutées aux listes existantes (pas de suppression)
- **Albums** sont ajoutés aux mots-clés existants (pas de suppression)

### ⚠️ Mode destructif:
Utilisez `--overwrite` seulement si vous voulez explicitement écraser les métadonnées existantes.

## Détails techniques

### Opérateur exiftool `+=` pour les listes
Notre implémentation utilise l'opérateur `+=` d'exiftool pour une gestion sûre des tags de type liste :

```bash
# ✅ Correct : L'opérateur += ajoute ET crée le tag si nécessaire
exiftool "-XMP-iptcExt:PersonInImage+=John Doe" photo.jpg

# ❌ Incorrect : L'opérateur += seul ne crée pas un tag inexistant
# (ancien comportement qui échouait)
```

**Avantages de notre approche :**
- **Création automatique** : `+=` crée le tag s'il n'existe pas
- **Accumulation sûre** : Ajoute aux listes existantes sans duplication
- **Sécurité** : Arguments séparés préviennent l'injection shell avec espaces
- **Mode overwrite** : Vide explicitement puis reremplit avec `+=`

### Format Google Takeout supporté
```json
{
  "title": "IMG_20240716_200232.jpg",
  "description": "Description de la photo",
  "photoTakenTime": {"timestamp": "1721152952"},
  "creationTime": {"timestamp": "1721152952"},
  "geoData": {
    "latitude": 48.8566,
    "longitude": 2.3522,
    "altitude": 35.0
  },
  "people": [
    {"name": "John Doe"},
    {"name": "Jane Smith"}
  ],
  "favorited": true,
  "archived": false
}
```

## Tests

```bash
# Tests unitaires
pytest tests/ -m "not integration"

# Tests complets (nécessite exiftool)
pytest tests/

# Tests d'intégration uniquement
pytest tests/ -m "integration"
```

Les tests comprennent:
- **Tests unitaires**: Parsing des sidecars, génération des arguments exiftool
- **Tests d'intégration**: Écriture et relecture effective des métadonnées avec exiftool
- **Tests du mode batch**: Vérification des performances et de la compatibilité du traitement par lots
- **Tests CLI**: Validation de l'interface en ligne de commande et de toutes les options
````

## File: requirements.txt
````
pillow
pytest
````

## File: src/google_takeout_metadata/__init__.py
````python
"""Utilitaires pour fusionner les métadonnées annexes Google Takeout dans des fichiers image."""
__all__ = [
    "sidecar",
    "exif_writer",
    "processor",
]
````

## File: src/google_takeout_metadata/__main__.py
````python
"""Point d'entrée principal du paquet google_takeout_metadata."""
from google_takeout_metadata.cli import main
if __name__ == "__main__":
    main()
````

## File: src/google_takeout_metadata/cli.py
````python
"""Interface en ligne de commande."""
from __future__ import annotations
import argparse
import logging
import shutil
import sys
from pathlib import Path
from .processor import process_directory
from .processor_batch import process_directory_batch
from .statistics import ProcessingStats
import google_takeout_metadata.statistics as stats_module
def main(argv: list[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description="Fusionner les métadonnées Google Takeout dans les images")
    parser.add_argument("path", type=Path, help="Répertoire à analyser récursivement")
    parser.add_argument(
        "--localtime", action="store_true",
        help="Convertir les horodatages en heure locale au lieu de l'UTC (par défaut : UTC)"
    )
    parser.add_argument(
        "--overwrite", action="store_true",
        help="Autoriser l'écrasement des champs de métadonnées existants (par défaut, les métadonnées existantes sont préservées)"
    )
    parser.add_argument(
        "--append-only", action="store_true",
        help=argparse.SUPPRESS  # Cache l'option dépréciée de l'aide
    )
    parser.add_argument(
        "--clean-sidecars", action="store_true",
        help="Supprimer les fichiers JSON annexes après un transfert de métadonnées réussi"
    )
    parser.add_argument(
        "-v", "--verbose", action="store_true",
        help="Activer les logs détaillés (niveau DEBUG)"
    )
    parser.add_argument(
        "--batch", action="store_true",
        help="Traiter les fichiers par lots"
    )
    args = parser.parse_args(argv)
    # Configuration du logging avec le niveau approprié
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level, 
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    # Gestion de la rétrocompatibilité et validation des options
    if args.append_only and args.overwrite:
        logging.error("Impossible d'utiliser simultanément --append-only (obsolète) et --overwrite")
        sys.exit(1)
    if args.append_only:
        logging.warning("--append-only est obsolète et correspond désormais au comportement par défaut. Utilisez --overwrite pour autoriser l'écrasement des métadonnées existantes.")
    if not args.path.is_dir():
        logging.error("Le chemin indiqué n'est pas un répertoire : %s", args.path)
        sys.exit(1)
    # Réinitialiser les statistiques pour cette exécution (nouvelle instance)
    stats_module.stats = ProcessingStats()
    # Le mode par défaut est maintenant append_only=True (sécurité par défaut)
    # L'option --overwrite permet d'écraser les métadonnées existantes
    append_only = not args.overwrite
    # Vérifier que exiftool est disponible uniquement si on va traiter
    if shutil.which("exiftool") is None:
        logging.error("exiftool introuvable. Veuillez l'installer pour utiliser ce script.")
        sys.exit(1)
    if args.batch:
        process_directory_batch(args.path, use_localtime=args.localtime, append_only=append_only, clean_sidecars=args.clean_sidecars)
    else:
        process_directory(args.path, use_localtime=args.localtime, append_only=append_only, clean_sidecars=args.clean_sidecars)
if __name__ == "__main__":  # pragma: no cover - CLI entry
    main()
````

## File: src/google_takeout_metadata/exif_writer.py
````python
# Fichier : src/google_takeout_metadata/exif_writer.py
import subprocess
import logging
from datetime import datetime, timezone
from pathlib import Path
from .sidecar import SidecarData
logger = logging.getLogger(__name__)
VIDEO_EXTS = {".mp4", ".mov", ".m4v", ".3gp"}
def _is_video_file(path: Path) -> bool:
    return path.suffix.lower() in VIDEO_EXTS
def _fmt_dt(ts: int | None, use_localtime: bool) -> str | None:
    if ts is None:
        return None
    dt = datetime.fromtimestamp(ts) if use_localtime else datetime.fromtimestamp(ts, tz=timezone.utc)
    return dt.strftime("%Y:%m:%d %H:%M:%S")
def _build_keywords(meta: SidecarData) -> list[str]:
    """Centralise la logique de création des mots-clés à partir des personnes et albums."""
    return (meta.people or []) + [f"Album: {a}" for a in (meta.albums or [])]
def _sanitize_description(desc: str) -> str:
    """Centralise le nettoyage des descriptions pour ExifTool."""
    return desc.replace("\r", " ").replace("\n", " ").strip()
def write_metadata(media_path: Path, meta: SidecarData, use_localtime: bool = False, append_only: bool = True) -> None:
    """Écrit les métadonnées sur un média en utilisant ExifTool."""
    if append_only:
        # Mode append-only : utiliser build_exiftool_args qui gère déjà tout avec -wm cg
        args = build_exiftool_args(meta, media_path, use_localtime, append_only=True)
        if args:
            _run_exiftool_command(media_path, args, _append_only=True)
    else:
        # Mode écrasement : utiliser build_exiftool_args directement
        all_args = build_exiftool_args(meta, media_path, use_localtime, append_only=False)
        # Exécuter en mode écrasement
        if all_args:
            _run_exiftool_command(media_path, all_args, _append_only=False)
def build_exiftool_args(meta: SidecarData, media_path: Path = None, use_localtime: bool = False, append_only: bool = True) -> list[str]:
    """Construit les arguments exiftool pour traiter un fichier média avec les métadonnées fournies.
    Args:
        meta: Métadonnées à écrire
        media_path: Chemin du fichier média (optionnel, pour la détection vidéo)
        use_localtime: Utiliser l'heure locale au lieu d'UTC
        append_only: Mode append-only (-wm cg) ou mode écrasement
    Returns:
        Liste des arguments exiftool
    """
    args = []
    if append_only:
        # Mode append-only : inclusion de tous les tags avec -wm cg pour compatibilité batch
        # Note: Cela peut créer des doublons dans les tags de liste
        if media_path and _is_video_file(media_path):
            args.extend(["-api", "QuickTimeUTC=1"])
        args.extend(["-wm", "cg"])
        # Description
        if meta.description:
            safe_desc = _sanitize_description(meta.description)
            args.extend([f"-EXIF:ImageDescription={safe_desc}", f"-XMP-dc:Description={safe_desc}", f"-IPTC:Caption-Abstract={safe_desc}"])
            if media_path and _is_video_file(media_path):
                args.append(f"-Keys:Description={safe_desc}")
        # Tags de liste avec += (peut créer des doublons en mode batch)
        if meta.people:
            for person in meta.people:
                args.append(f"-XMP-iptcExt:PersonInImage+={person}")
        # Mots-clés (personnes + albums)
        all_keywords = _build_keywords(meta)
        if all_keywords:
            for keyword in all_keywords:
                args.append(f"-XMP-dc:Subject+={keyword}")
                args.append(f"-IPTC:Keywords+={keyword}")
        # Dates
        if (s := _fmt_dt(meta.taken_at, use_localtime)):
            args.append(f"-DateTimeOriginal={s}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:CreateDate={s}")
        base_ts = meta.created_at or meta.taken_at
        if (s := _fmt_dt(base_ts, use_localtime)):
            args.append(f"-CreateDate={s}")
            args.append(f"-ModifyDate={s}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:ModifyDate={s}")
        # GPS
        if meta.latitude is not None and meta.longitude is not None:
            lat = str(abs(meta.latitude))
            lon = str(abs(meta.longitude))
            lat_ref = "N" if meta.latitude >= 0 else "S"
            lon_ref = "E" if meta.longitude >= 0 else "W"
            gps_coords = f"{meta.latitude},{meta.longitude}"
            args.append(f"-GPSLatitude={lat}")
            args.append(f"-GPSLatitudeRef={lat_ref}")
            args.append(f"-GPSLongitude={lon}")
            args.append(f"-GPSLongitudeRef={lon_ref}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:GPSCoordinates={gps_coords}")
                args.append(f"-Keys:Location={gps_coords}")
            if meta.altitude is not None:
                alt = str(abs(meta.altitude))
                alt_ref = "1" if meta.altitude < 0 else "0"
                args.append(f"-GPSAltitude={alt}")
                args.append(f"-GPSAltitudeRef={alt_ref}")
        # Rating
        if meta.favorite:
            args.append("-XMP:Rating=5")
    else:
        # Mode écrasement : logique complète
        if media_path and _is_video_file(media_path):
            args.extend(["-api", "QuickTimeUTC=1"])
        # Description
        if meta.description:
            safe_desc = _sanitize_description(meta.description)
            args.extend([f"-EXIF:ImageDescription={safe_desc}", f"-XMP-dc:Description={safe_desc}", f"-IPTC:Caption-Abstract={safe_desc}"])
            if media_path and _is_video_file(media_path):
                args.append(f"-Keys:Description={safe_desc}")
        # Vider d'abord les listes puis les remplir
        args.extend(["-XMP-iptcExt:PersonInImage=", "-XMP-dc:Subject=", "-IPTC:Keywords="])
        # Ajouter les personnes
        if meta.people:
            for person in meta.people:
                args.append(f"-XMP-iptcExt:PersonInImage+={person}")
        # Ajouter mots-clés (personnes + albums)
        all_keywords = _build_keywords(meta)
        if all_keywords:
            for keyword in all_keywords:
                args.append(f"-XMP-dc:Subject+={keyword}")
                args.append(f"-IPTC:Keywords+={keyword}")
        # Rating
        if meta.favorite:
            args.append("-XMP:Rating=5")
        # Dates
        if (s := _fmt_dt(meta.taken_at, use_localtime)):
            args.append(f"-DateTimeOriginal={s}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:CreateDate={s}")
        base_ts = meta.created_at or meta.taken_at
        if (s := _fmt_dt(base_ts, use_localtime)):
            args.append(f"-CreateDate={s}")
            args.append(f"-ModifyDate={s}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:ModifyDate={s}")
        # GPS
        if meta.latitude is not None and meta.longitude is not None:
            lat = str(abs(meta.latitude))
            lon = str(abs(meta.longitude))
            lat_ref = "N" if meta.latitude >= 0 else "S"
            lon_ref = "E" if meta.longitude >= 0 else "W"
            gps_coords = f"{meta.latitude},{meta.longitude}"
            args.append(f"-GPSLatitude={lat}")
            args.append(f"-GPSLatitudeRef={lat_ref}")
            args.append(f"-GPSLongitude={lon}")
            args.append(f"-GPSLongitudeRef={lon_ref}")
            if media_path and _is_video_file(media_path):
                args.append(f"-QuickTime:GPSCoordinates={gps_coords}")
                args.append(f"-Keys:Location={gps_coords}")
            if meta.altitude is not None:
                alt = str(abs(meta.altitude))
                alt_ref = "1" if meta.altitude < 0 else "0"
                args.append(f"-GPSAltitude={alt}")
                args.append(f"-GPSAltitudeRef={alt_ref}")
    return args
def _run_exiftool_command(media_path: Path, args: list[str], _append_only: bool) -> None:
    """Exécute une commande exiftool avec les arguments fournis."""
    if not args:
        return
    cmd = [
        "exiftool",
        "-overwrite_original",
        "-charset", "filename=UTF8",
        "-charset", "iptc=UTF8",
        "-charset", "exif=UTF8",
    ]
    # Ajouter les arguments métadonnées
    cmd.extend(args)
    # Ajouter le fichier à traiter
    cmd.append(str(media_path))
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=60, encoding='utf-8')
        logger.debug("Exiftool output for %s: %s", media_path.name, result.stdout.strip())
    except FileNotFoundError as exc:
        raise RuntimeError("exiftool introuvable") from exc
    except subprocess.CalledProcessError as exc:
        if _append_only and exc.returncode == 2:
            logger.info(f"Aucune métadonnée manquante à écrire pour {media_path.name} (comportement normal en mode append-only).")
            return
        error_msg = f"exiftool a échoué pour {media_path.name} (code {exc.returncode}): {exc.stderr.strip() or exc.stdout.strip()}"
        raise RuntimeError(error_msg) from exc
````

## File: src/google_takeout_metadata/processor_batch.py
````python
import logging
import subprocess
import tempfile
from pathlib import Path
from typing import List, Tuple
from datetime import datetime
from .exif_writer import build_exiftool_args
from .sidecar import find_albums_for_directory, parse_sidecar
from .processor import IMAGE_EXTS, fix_file_extension_mismatch, _is_sidecar_file 
from . import statistics
logger = logging.getLogger(__name__)
def process_batch(batch: List[Tuple[Path, Path, List[str]]], clean_sidecars: bool) -> int:
    """Traiter un lot de fichiers avec exiftool via un fichier d'arguments."""
    if not batch:
        return 0
    argfile_path = None
    try:
        with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8', suffix=".txt") as argfile:
            argfile_path = argfile.name
        with open(argfile_path, 'w', encoding='utf-8') as argfile:
            for media_path, _, args in batch:
                for arg in args:
                    argfile.write(f"{arg}\n")
                argfile.write(f"{media_path}\n")
                argfile.write("-execute\n")
        logger.info(f"📦 Traitement d'un lot de {len(batch)} fichier(s)...")
        cmd = [
            "exiftool",
            "-overwrite_original",
            "-charset", "filename=UTF8",
            "-charset", "iptc=UTF8",
            "-charset", "exif=UTF8",
            "-@", argfile_path,
        ]
        timeout_seconds = 60 + (len(batch) * 5)
        result = subprocess.run(
            cmd, capture_output=True, text=True, check=True, timeout=timeout_seconds, encoding='utf-8'
        )
        # Analyser la sortie pour compter les fichiers traités
        processed_count = 0
        if result.stdout and result.stdout.strip():
            stdout_lines = result.stdout.strip().split('\n')
            for line in stdout_lines:
                if 'image files updated' in line.lower() or 'files updated' in line.lower():
                    # Extraire le nombre de fichiers mis à jour
                    try:
                        numbers = [int(word) for word in line.split() if word.isdigit()]
                        if numbers:
                            processed_count = numbers[0]
                    except (ValueError, IndexError):
                        pass
                    logger.info(f"✅ {line.strip()}")
        # Si on n'a pas pu extraire le nombre, utiliser la taille du lot
        if processed_count == 0:
            processed_count = len(batch)
            logger.info(f"✅ Lot de {len(batch)} fichier(s) traité avec succès")
        # Mettre à jour les statistiques pour chaque fichier du lot
        for media_path, _, _ in batch:
            is_image = media_path.suffix.lower() in IMAGE_EXTS
            statistics.stats.add_processed_file(media_path, is_image)
        if clean_sidecars:
            cleaned_count = 0
            for _, json_path, _ in batch:
                try:
                    json_path.unlink()
                    cleaned_count += 1
                except OSError as e:
                    logger.warning(f"Échec de la suppression du fichier de métadonnées {json_path.name}: {e}")
            statistics.stats.sidecars_cleaned += cleaned_count
        return len(batch)
    except FileNotFoundError as exc:
        raise RuntimeError("exiftool introuvable") from exc
    except subprocess.CalledProcessError as exc:
        stderr_msg = exc.stderr or ""
        stdout_msg = exc.stdout or ""
        # Analyser le type d'erreur pour donner un message plus clair
        if "files failed condition" in stderr_msg or "files failed condition" in stdout_msg:
            logger.info(f"ℹ️ Lot traité avec conditions non remplies (normal en mode append-only). "
                       f"Certaines métadonnées existaient déjà pour {len(batch)} fichier(s).")
            # En mode append-only, considérer ceci comme un succès partiel
            for media_path, _, _ in batch:
                is_image = media_path.suffix.lower() in IMAGE_EXTS
                statistics.stats.add_processed_file(media_path, is_image)
            # Nettoyer les sidecars si demandé (comme dans le cas de succès normal)
            if clean_sidecars:
                cleaned_count = 0
                for _, json_path, _ in batch:
                    try:
                        json_path.unlink()
                        cleaned_count += 1
                    except OSError as e:
                        logger.warning(f"Échec de la suppression du fichier de métadonnées {json_path.name}: {e}")
                statistics.stats.sidecars_cleaned += cleaned_count
            return len(batch)
        elif "doesn't exist or isn't writable" in stderr_msg:
            logger.warning(f"⚠️ Certains champs de métadonnées non supportés par les fichiers du lot. "
                          f"Normal pour vidéos ou certains formats. Détails: {stderr_msg.strip()}")
            # Considérer comme un succès partiel
            for media_path, _, _ in batch:
                is_image = media_path.suffix.lower() in IMAGE_EXTS  
                statistics.stats.add_processed_file(media_path, is_image)
            # Nettoyer les sidecars si demandé (comme dans le cas de succès normal)
            if clean_sidecars:
                cleaned_count = 0
                for _, json_path, _ in batch:
                    try:
                        json_path.unlink()
                        cleaned_count += 1
                    except OSError as e:
                        logger.warning(f"Échec de la suppression du fichier de métadonnées {json_path.name}: {e}")
                statistics.stats.sidecars_cleaned += cleaned_count
            return len(batch)
        elif "character(s) could not be encoded" in stderr_msg:
            error_type = "encoding_error"
            error_msg = "Problème d'encodage de caractères (émojis, accents)"
            logger.warning(f"⚠️ {error_msg}. Détails: {stderr_msg.strip()}")
        else:
            error_type = "exiftool_error"
            error_msg = f"Erreur exiftool (code {exc.returncode}): {stderr_msg.strip() or 'Erreur inconnue'}"
            logger.exception(f"❌ Échec du traitement par lot de {len(batch)} fichier(s). {error_msg}")
        # Marquer tous les fichiers du lot comme échoués
        for media_path, _, _ in batch:
            statistics.stats.add_failed_file(media_path, error_type, error_msg)
        return 0
    finally:
        if argfile_path and Path(argfile_path).exists():
            Path(argfile_path).unlink()
def process_directory_batch(root: Path, use_localtime: bool = False, append_only: bool = True, clean_sidecars: bool = False) -> None:
    """Traiter récursivement tous les fichiers sidecar sous ``root`` par lots."""
    batch: List[Tuple[Path, Path, List[str]]] = []
    BATCH_SIZE = 100
    # Initialiser les statistiques
    statistics.stats.start_processing()
    sidecar_files = [path for path in root.rglob("*.json") if _is_sidecar_file(path)]
    statistics.stats.total_sidecars_found = len(sidecar_files)
    if statistics.stats.total_sidecars_found == 0:
        logger.warning("Aucun fichier de métadonnées (.json) trouvé dans %s", root)
        statistics.stats.end_processing()
        return
    logger.info("🔍 Traitement par lots de %d fichier(s) de métadonnées dans %s", statistics.stats.total_sidecars_found, root)
    for json_path in sidecar_files:
        try:
            meta = parse_sidecar(json_path)
            directory_albums = find_albums_for_directory(json_path.parent)
            meta.albums.extend(directory_albums)
            media_path = json_path.with_name(meta.filename)
            if not media_path.exists():
                error_msg = f"Fichier image introuvable : {meta.filename}"
                statistics.stats.add_failed_file(json_path, "file_not_found", error_msg)
                logger.warning(f"❌ {error_msg}")
                continue
            fixed_media_path, fixed_json_path = fix_file_extension_mismatch(media_path, json_path)
            if fixed_json_path != json_path:
                meta = parse_sidecar(fixed_json_path)
                meta.albums.extend(find_albums_for_directory(fixed_json_path.parent))
            args = build_exiftool_args(
                meta, media_path=fixed_media_path, use_localtime=use_localtime, append_only=append_only
            )
            if args:
                batch.append((fixed_media_path, fixed_json_path, args))
            else:
                # Aucun tag à écrire pour ce sidecar
                statistics.stats.total_skipped += 1
                statistics.stats.skipped_files.append(json_path.name)
            if len(batch) >= BATCH_SIZE:
                process_batch(batch, clean_sidecars)
                batch = []
        except (ValueError, RuntimeError) as exc:
            error_msg = f"Erreur de préparation : {exc}"
            statistics.stats.add_failed_file(json_path, "preparation_error", error_msg)
            logger.warning("❌ Échec de la préparation de %s : %s", json_path.name, exc)
    if batch:
        process_batch(batch, clean_sidecars)
    statistics.stats.end_processing()
    # Affichage du résumé
    statistics.stats.print_console_summary()
    # Créer un dossier logs s'il n'existe pas
    logs_dir = root / "logs"
    logs_dir.mkdir(exist_ok=True)
    # Sauvegarde du rapport détaillé avec un nom incluant la date
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = logs_dir / f"traitement_log_{timestamp}.json"
    statistics.stats.save_detailed_report(log_file)
````

## File: src/google_takeout_metadata/processor.py
````python
"""Traitement de haut niveau des répertoires contenant des métadonnées Google Takeout."""
from __future__ import annotations
from pathlib import Path
import logging
import json
import subprocess
from datetime import datetime
from .sidecar import parse_sidecar, find_albums_for_directory
from .exif_writer import write_metadata
from . import statistics
logger = logging.getLogger(__name__)
# Séparer les extensions images et vidéos pour une meilleure cohérence
IMAGE_EXTS = {".jpg", ".jpeg", ".png", ".gif", ".webp", ".heic", ".heif", ".avif"}
VIDEO_EXTS = {".mp4", ".mov", ".m4v", ".3gp"}
ALL_MEDIA_EXTS = IMAGE_EXTS | VIDEO_EXTS
def detect_file_type(file_path: Path) -> str | None:
    """Détecter le type réel du fichier via la commande ``file`` ou les octets magiques.
    Retourne:
        L'extension correcte (avec point) ou ``None`` si la détection échoue
    """
    try:
        # Essayer d'abord la commande ``file`` (disponible sur la plupart des systèmes)
        result = subprocess.run(
            ["file", str(file_path)], 
            capture_output=True, 
            text=True, 
            timeout=10
        )
        if result.returncode == 0:
            output = result.stdout.lower()
            if "jpeg" in output or "jfif" in output:
                return ".jpg"
            elif "png" in output:
                return ".png"
            elif "gif" in output:
                return ".gif"
            elif "webp" in output:
                return ".webp"
            elif "heic" in output:
                return ".heic"
            elif "heif" in output:
                return ".heif"
            elif "mp4" in output:
                return ".mp4"
            elif "quicktime" in output or "mov" in output:
                return ".mov"
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
        pass
    # Repli : lecture des octets magiques
    try:
        with open(file_path, "rb") as f:
            header = f.read(16)
            if header.startswith(b'\xff\xd8\xff'):
                return ".jpg"
            elif header.startswith(b'\x89PNG\r\n\x1a\n'):
                return ".png"
            elif header.startswith(b'GIF8'):
                return ".gif"
            elif header.startswith(b'RIFF') and b'WEBP' in header:
                return ".webp"
            elif header[4:8] == b'ftyp':
                if b'heic' in header[:16] or b'mif1' in header[:16]:
                    return ".heic"
                elif b'mp4' in header[:16] or b'isom' in header[:16]:
                    return ".mp4"
    except (OSError, IOError):
        pass
    return None
def fix_file_extension_mismatch(media_path: Path, json_path: Path) -> tuple[Path, Path]:
    """Corriger une incohérence d'extension en renommant les fichiers et en mettant à jour le JSON.
    Args:
        media_path: Chemin du fichier image/vidéo
        json_path: Chemin du fichier JSON associé (sidecar)
    Retourne:
        Un tuple ``(new_media_path, new_json_path)``
    """
    # Détecter le type réel du fichier
    actual_ext = detect_file_type(media_path)
    if not actual_ext or actual_ext == media_path.suffix.lower():
        # Aucune incohérence détectée ou la détection a échoué
        return media_path, json_path
    # Créer de nouveaux chemins avec la bonne extension
    new_media_path = media_path.with_suffix(actual_ext)
    new_json_path = json_path.with_name(new_media_path.name + ".supplemental-metadata.json")
    logger.info("🔧 Extension incorrecte détectée pour %s (devrait être %s). Correction automatique...", 
                media_path.name, actual_ext)
    image_renamed = False
    try:
        # Renommer le fichier image
        media_path.rename(new_media_path)
        image_renamed = True
        logger.info("✅ Fichier renommé : %s → %s", media_path.name, new_media_path.name)
        # Mettre à jour le contenu JSON et renommer le fichier JSON
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        # Mettre à jour le champ title
        json_data['title'] = new_media_path.name
        # Écrire le JSON mis à jour au nouvel emplacement
        with open(new_json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        # Supprimer l'ancien fichier JSON
        json_path.unlink()
        logger.info("✅ Métadonnées mises à jour : %s → %s", json_path.name, new_json_path.name)
        # Enregistrer la correction dans les statistiques
        statistics.stats.add_fixed_extension(media_path.name, new_media_path.name)
        return new_media_path, new_json_path
    except (OSError, IOError, json.JSONDecodeError) as exc:
        logger.warning("❌ Échec de la correction d'extension pour %s : %s. "
                       "Le fichier sera traité avec son extension actuelle.", media_path.name, exc)
        # Si l'image a été renommée mais que des étapes ultérieures échouent, tenter un rollback
        if image_renamed:
            try:
                # Supprimer tout nouveau JSON éventuellement créé
                if new_json_path.exists():
                    new_json_path.unlink()
                    logger.info("🔄 Fichier JSON partiellement créé supprimé : %s", new_json_path.name)
                # Renommer l'image avec son nom d'origine
                new_media_path.rename(media_path)
                logger.info("🔄 Annulation du renommage : %s → %s", new_media_path.name, media_path.name)
                return media_path, json_path
            except (OSError, IOError) as rollback_exc:
                logger.exception("❌ Échec de l'annulation du renommage %s → %s : %s. "
                           "ATTENTION : État incohérent - fichier image renommé mais JSON non mis à jour.", 
                           new_media_path.name, media_path.name, rollback_exc)
                # Retourner les nouveaux chemins afin de refléter l'état courant
                return new_media_path, json_path
        return media_path, json_path
def _is_sidecar_file(path: Path) -> bool:
    """Vérifier si un fichier peut être un JSON annexe (Google Photos).
    Cette fonction est permissive car ``parse_sidecar()`` fait une
    validation stricte en comparant le champ ``title`` avec le nom attendu.
    Formats supportés :
    - Nouveau format : photo.jpg.supplemental-metadata.json
    - Ancien format : photo.jpg.json
    """
    if not path.suffix.lower() == ".json":
        return False
    suffixes = [s.lower() for s in path.suffixes]
    # Nouveau format Google Takeout : photo.jpg.supplemental-metadata.json
    if len(suffixes) >= 3 and suffixes[-2] == ".supplemental-metadata" and suffixes[-3] in ALL_MEDIA_EXTS:
        return True
    # Format hérité : photo.jpg.json
    if len(suffixes) >= 2 and suffixes[-2] in ALL_MEDIA_EXTS:
        return True
    # Format plus ancien : photo.json (moins spécifique mais validé ultérieurement)
    # Ne considérer ceci que si le nom de base sans .json pourrait être une image
    stem_parts = path.stem.split('.')
    if len(stem_parts) >= 2:
        potential_ext = '.' + stem_parts[-1].lower()
        if potential_ext in ALL_MEDIA_EXTS:
            return True
    return False
def process_sidecar_file(json_path: Path, use_localtime: bool = False, append_only: bool = True, clean_sidecars: bool = False) -> None:
    """Traiter un fichier annexe ``.json``.
    Args:
        json_path: Chemin du fichier JSON annexe
        use_localtime: Convertir les dates en heure locale au lieu d'UTC
        append_only: Ajouter uniquement les champs manquants
        clean_sidecars: Supprimer le JSON après un traitement réussi
    """
    try:
        meta = parse_sidecar(json_path)
    except ValueError as exc:
        statistics.stats.add_failed_file(json_path, "parse_error", f"Erreur de lecture JSON : {exc}")
        raise
    # Trouver les albums du répertoire
    directory_albums = find_albums_for_directory(json_path.parent)
    meta.albums.extend(directory_albums)
    media_path = json_path.with_name(meta.filename)
    if not media_path.exists():
        error_msg = f"Fichier image introuvable : {meta.filename}"
        statistics.stats.add_failed_file(json_path, "file_not_found", error_msg)
        raise FileNotFoundError(error_msg)
    # Détecter le type de fichier (image ou vidéo)
    is_image = media_path.suffix.lower() in IMAGE_EXTS
    # Tenter d'écrire les métadonnées dans l'image
    try:
        write_metadata(media_path, meta, use_localtime=use_localtime, append_only=append_only)
        current_json_path = json_path
        # Enregistrer le succès
        statistics.stats.add_processed_file(media_path, is_image)
    except RuntimeError as exc:
        # Vérifier s'il s'agit d'une erreur d'incohérence d'extension
        error_msg = str(exc).lower()
        if ("not a valid png" in error_msg and "looks more like a jpeg" in error_msg) or \
           ("not a valid jpeg" in error_msg and "looks more like a png" in error_msg) or \
           ("charset option" in error_msg):
            logger.info("🔍 Extension possiblement incorrecte pour %s. Tentative de correction...", media_path.name)
            # Tenter de corriger l'incohérence d'extension
            fixed_media_path, fixed_json_path = fix_file_extension_mismatch(media_path, json_path)
            if fixed_media_path != media_path or fixed_json_path != json_path:
                # Les fichiers ont été renommés (au moins partiellement), re-analyser le JSON et réessayer
                # Gérer le cas où l'image a été renommée mais pas le JSON (échec de rollback partiel)
                is_image_after_fix = fixed_media_path.suffix.lower() in IMAGE_EXTS
                actual_json_path = fixed_json_path if fixed_json_path.exists() else json_path
                meta = parse_sidecar(actual_json_path)
                directory_albums = find_albums_for_directory(actual_json_path.parent)
                meta.albums.extend(directory_albums)
                write_metadata(fixed_media_path, meta, use_localtime=use_localtime, append_only=append_only)
                current_json_path = actual_json_path
                # Enregistrer le succès après correction
                statistics.stats.add_processed_file(fixed_media_path, is_image_after_fix)
                logger.info("✅ Traitement réussi de %s après correction d'extension", fixed_media_path.name)
            else:
                # Échec de la correction d'extension, relancer l'erreur originale
                statistics.stats.add_failed_file(media_path, "extension_mismatch", str(exc))
                raise
        else:
            # Ce n'est pas une erreur d'incohérence d'extension, relancer
            statistics.stats.add_failed_file(media_path, "metadata_write_error", str(exc))
            raise
    # Nettoyer le sidecar si demandé et si l'écriture a réussi
    if clean_sidecars:
        try:
            current_json_path.unlink()
            statistics.stats.sidecars_cleaned += 1
            logger.info("🗑️ Fichier de métadonnées supprimé : %s", current_json_path.name)
        except OSError as exc:
            logger.warning("Échec de la suppression du fichier de métadonnées %s : %s", current_json_path, exc)
def process_directory(root: Path, use_localtime: bool = False, append_only: bool = True, clean_sidecars: bool = False) -> None:
    """Traiter récursivement tous les fichiers annexes sous ``root``.
    Args:
        root: Répertoire racine à parcourir récursivement
        use_localtime: Convertir les dates en heure locale au lieu d'UTC
        append_only: Ajouter uniquement les champs manquants
        clean_sidecars: Supprimer les JSON après un traitement réussi
    """
    # Initialiser les statistiques
    statistics.stats.start_processing()
    sidecar_files = [path for path in root.rglob("*.json") if _is_sidecar_file(path)]
    statistics.stats.total_sidecars_found = len(sidecar_files)
    if statistics.stats.total_sidecars_found == 0:
        logger.warning("Aucun fichier de métadonnées (.json) trouvé dans %s", root)
        statistics.stats.end_processing()
        return
    logger.info("🔍 Traitement de %d fichier(s) de métadonnées dans %s", statistics.stats.total_sidecars_found, root)
    for json_file in sidecar_files:
        try:
            process_sidecar_file(json_file, use_localtime=use_localtime, append_only=append_only, clean_sidecars=clean_sidecars)
        except (FileNotFoundError, ValueError, RuntimeError) as exc:
            logger.warning("❌ Échec du traitement de %s : %s", json_file.name, exc)
            # Les statistiques sont déjà mises à jour dans process_sidecar_file
    statistics.stats.end_processing()
    # Affichage du résumé
    statistics.stats.print_console_summary()
    # Créer un dossier logs s'il n'existe pas
    logs_dir = root / "logs"
    logs_dir.mkdir(exist_ok=True)
    # Sauvegarde du rapport détaillé avec un nom incluant la date
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = logs_dir / f"traitement_log_{timestamp}.json"
    statistics.stats.save_detailed_report(log_file)
````

## File: src/google_takeout_metadata/sidecar.py
````python
from __future__ import annotations
from dataclasses import dataclass, field
from pathlib import Path
import json
import logging
from typing import List, Optional
"""Analyse des fichiers annexes JSON de Google Takeout."""
logger = logging.getLogger(__name__)
@dataclass
class SidecarData:
    """Métadonnées sélectionnées extraites d'un JSON annexe Google Photos."""
    filename: str
    description: Optional[str]
    people: List[str]
    taken_at: Optional[int]
    created_at: Optional[int]
    latitude: Optional[float]
    longitude: Optional[float]
    altitude: Optional[float]
    favorite: bool = False
    lat_span: Optional[float] = None
    lon_span: Optional[float] = None
    albums: List[str] = field(default_factory=list)
    archived: bool = False
def parse_sidecar(path: Path) -> SidecarData:
    """Analyser ``path`` et retourner :class:`SidecarData`.
    La fonction vérifie que le champ ``title`` intégré correspond au nom de fichier
    du sidecar pour éviter d'appliquer des métadonnées au mauvais média.
    Formats supportés :
    - Nouveau format : photo.jpg.supplemental-metadata.json -> title attendu "photo.jpg"
    - Ancien format : photo.jpg.json -> title attendu "photo.jpg"
    """
    try:
        with path.open("r", encoding="utf-8") as fh:
            data = json.load(fh)
    except FileNotFoundError as exc:  # pragma: no cover - simple wrapper
        raise FileNotFoundError(f"Sidecar introuvable : {path}") from exc
    except json.JSONDecodeError as exc:
        raise ValueError(f"JSON invalide dans {path}") from exc
    title = data.get("title")
    if not title:
        raise ValueError(f"Champ 'title' manquant dans {path}")
    # Extraire le nom de fichier attendu depuis le chemin du sidecar
    # Pour le nouveau format : IMG_001.jpg.supplemental-metadata.json -> titre attendu : IMG_001.jpg
    # Pour le format hérité : IMG_001.jpg.json -> titre attendu : IMG_001.jpg
    if path.name.lower().endswith(".supplemental-metadata.json"):
        expected_title = path.name[:-len(".supplemental-metadata.json")]
    elif path.name.lower().endswith(".supplemental-metadat.json"):
        expected_title = path.name[:-len(".supplemental-metadat.json")]
    elif path.name.lower().endswith(".supplemental-me.json"):
        expected_title = path.name[:-len(".supplemental-me.json")]
    elif path.name.lower().endswith(".supplemental-meta.json"):
        expected_title = path.name[:-len(".supplemental-meta.json")]
    elif path.name.lower().endswith(".json"):
        expected_title = path.stem
    else:
        expected_title = path.stem
    if expected_title != title:
        raise ValueError(
            f"Le titre du sidecar {title!r} ne correspond pas au nom de fichier attendu {expected_title!r} provenant de {path.name!r}"
        )
    description = data.get("description")
    # Extraire les noms de personnes, supprimer les espaces et dédupliquer
    # people est [{ "name": "X" }]
    raw_people = data.get("people", []) or []
    people = []
    for p in raw_people:
        if isinstance(p, dict):
            if isinstance(p.get("name"), str):
                people.append(p["name"].strip())
    # déduplication
    people = sorted(set(filter(None, people)))
    def get_ts(key: str) -> Optional[int]:
        ts = data.get(key, {}).get("timestamp")
        if ts is None:
            return None
        try:
            return int(ts)
        except (TypeError, ValueError):
            return None
    taken_at = get_ts("photoTakenTime")
    created_at = get_ts("creationTime")
    # Extraire les données géographiques - préférer geoData, repli sur geoDataExif
    geo = data.get("geoData", {})
    if not geo or not geo.get("latitude"):
        geo = data.get("geoDataExif", {})
    latitude = geo.get("latitude")
    longitude = geo.get("longitude")
    altitude = geo.get("altitude")
    lat_span = geo.get("latitudeSpan")
    lon_span = geo.get("longitudeSpan")
    # Nettoyer les coordonnées seulement si les DEUX sont à 0/None
    # Conserver les vraies coordonnées 0.0 car elles peuvent être valides (équateur/méridien de Greenwich)
    # Google met parfois 0/0 quand pas de géo fiable → on nettoie uniquement dans ce cas
    if ((latitude in (0, 0.0, None)) and (longitude in (0, 0.0, None))) or \
       (latitude is None or longitude is None):
        latitude = longitude = altitude = None
    # Extraire le statut favori - format booléen Google Takeout
    # Note : "favorited": true si favori, champ absent si pas favori (pas false)
    favorite = bool(data.get("favorited", False))
    # Extraire le statut archivé
    archived = bool(data.get("archived", False))
    return SidecarData(
        filename=title,
        description=description,
        people=people,
        taken_at=taken_at,
        created_at=created_at,
        latitude=latitude,
        longitude=longitude,
        altitude=altitude,
        favorite=favorite,
        lat_span=lat_span,
        lon_span=lon_span,
        archived=archived,
    )
def parse_album_metadata(path: Path) -> List[str]:
    """Analyser un fichier metadata.json d'album et retourner la liste des noms d'albums.
    Les fichiers metadata.json d'albums (Google Takeout) contiennent généralement :
    {
        "title": "Nom de l'album",
        "description": "...",
        ...
    }
    Retourne une liste des noms d'albums trouvés dans le fichier.
    """
    try:
        with path.open("r", encoding="utf-8") as fh:
            data = json.load(fh)
    except (FileNotFoundError, json.JSONDecodeError):
        return []
    albums = []
    # Nom d'album principal depuis le champ title
    title = data.get("title")
    if title and isinstance(title, str):
        albums.append(title.strip())
    # Certains fichiers metadata.json peuvent avoir plusieurs références d'albums
    # Vérifier s'il y a des références d'albums dans d'autres champs
    album_refs = data.get("albums", [])
    if isinstance(album_refs, list):
        for album_ref in album_refs:
            if isinstance(album_ref, dict) and "title" in album_ref:
                album_name = album_ref["title"]
                if isinstance(album_name, str):
                    albums.append(album_name.strip())
            elif isinstance(album_ref, str):
                albums.append(album_ref.strip())
    # Supprimer les doublons et les chaînes vides
    albums = sorted(set(filter(None, albums)))
    return albums
def find_albums_for_directory(directory: Path, max_depth: int = 5) -> List[str]:
    """Trouver tous les noms d'albums applicables aux photos du répertoire donné.
    Recherche des fichiers metadata.json dans le répertoire et ses parents
    pour collecter les informations d'album.
    Args:
        directory: Répertoire de départ pour la recherche
        max_depth: Nombre maximum de niveaux parents à vérifier (défaut: 5)
    Prend en charge plusieurs motifs de fichiers metadata :
    - metadata.json (anglais)
    - métadonnées.json (français)  
    - métadonnées(1).json, métadonnées(2).json, etc. (français avec doublons)
    - album_metadata.json, folder_metadata.json (hérités)
    """
    albums = []
    metadata_patterns = [
        "metadata.json",
        "métadonnées.json", 
        "album_metadata.json", 
        "folder_metadata.json"
    ]
    # Rechercher dans le répertoire courant et ses parents avec limite de profondeur
    current_dir = directory
    depth = 0
    # Motifs de répertoires marqueurs (insensibles à la casse)
    takeout_markers = ["google photos", "takeout", "google takeout"]
    while current_dir != current_dir.parent and depth < max_depth:
        # Vérifier les motifs standards (insensible à la casse)
        for pattern in metadata_patterns:
            # Rechercher le fichier avec la casse exacte d'abord
            metadata_file = current_dir / pattern
            if metadata_file.exists():
                try:
                    albums.extend(parse_album_metadata(metadata_file))
                except (OSError, PermissionError) as e:
                    # Ignorer les erreurs de parsing et continuer
                    logger.debug(f"Erreur lors du parsing de {metadata_file}: {e}")
            else:
                # Rechercher de manière insensible à la casse si pas trouvé
                try:
                    for existing_file in current_dir.iterdir():
                        if existing_file.is_file() and existing_file.name.lower() == pattern.lower():
                            try:
                                albums.extend(parse_album_metadata(existing_file))
                            except (OSError, PermissionError) as e:
                                logger.debug(f"Erreur lors du parsing de {existing_file}: {e}")
                            break  # Un seul fichier correspondant par motif
                except (OSError, PermissionError):
                    # Ignorer les erreurs d'accès au répertoire
                    logger.debug(f"Impossible d'accéder au répertoire {current_dir}")
        # Vérifier les variations numérotées comme métadonnées(1).json, métadonnées(2).json, etc.
        # (recherche insensible à la casse)
        try:
            for metadata_file in current_dir.iterdir():
                if (metadata_file.is_file() and 
                    metadata_file.name.lower().startswith("métadonnées") and 
                    metadata_file.name.lower().endswith(".json") and
                    metadata_file.name.lower() not in ["métadonnées.json"]):  # déjà vérifié ci-dessus
                    try:
                        albums.extend(parse_album_metadata(metadata_file))
                    except (OSError, PermissionError) as e:
                        # Ignorer les erreurs de parsing et continuer
                        logger.debug(f"Erreur lors du parsing de {metadata_file}: {e}")
        except (OSError, PermissionError):
            # Ignorer les erreurs d'accès au répertoire et continuer
            logger.debug(f"Impossible d'accéder au répertoire {current_dir}")
        # Arrêter si on atteint un répertoire "marqueur" de Google Takeout
        # pour éviter de remonter trop haut dans l'arborescence
        if any(marker in current_dir.name.lower() for marker in takeout_markers):
            logger.debug(f"Arrêt de la recherche d'albums au répertoire marqueur: {current_dir}")
            break
        # Remonter au répertoire parent
        current_dir = current_dir.parent
        depth += 1
    # Déduplication et tri tout en préservant l'ordre de priorité
    # (répertoires plus proches en premier)
    unique_albums = []
    seen = set()
    for album in albums:
        if album not in seen:
            unique_albums.append(album)
            seen.add(album)
    return unique_albums
````

## File: src/google_takeout_metadata/statistics.py
````python
"""Module de gestion des statistiques et rapport de synthèse."""
from __future__ import annotations
import logging
from datetime import datetime
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Dict, Optional
import json
logger = logging.getLogger(__name__)
@dataclass
class ProcessingStats:
    """Statistiques de traitement des fichiers."""
    # Totaux
    total_sidecars_found: int = 0
    total_processed: int = 0
    total_failed: int = 0
    total_skipped: int = 0
    # Par type de fichier
    images_processed: int = 0
    videos_processed: int = 0
    # Détails des opérations
    files_fixed_extension: int = 0
    sidecars_cleaned: int = 0
    # Listes de détails pour le rapport détaillé
    failed_files: List[str] = field(default_factory=list)
    skipped_files: List[str] = field(default_factory=list)
    fixed_extensions: List[str] = field(default_factory=list)
    # Erreurs par catégorie
    errors_by_type: Dict[str, int] = field(default_factory=dict)
    # Timing
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    def start_processing(self) -> None:
        """Marquer le début du traitement."""
        self.start_time = datetime.now()
    def end_processing(self) -> None:
        """Marquer la fin du traitement."""
        self.end_time = datetime.now()
    @property
    def duration(self) -> Optional[float]:
        """Durée du traitement en secondes."""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None
    @property
    def success_rate(self) -> float:
        """Taux de réussite en pourcentage."""
        if self.total_sidecars_found == 0:
            return 0.0
        return (self.total_processed / self.total_sidecars_found) * 100
    def add_processed_file(self, file_path: Path, is_image: bool = True) -> None:
        """Ajouter un fichier traité avec succès."""
        self.total_processed += 1
        if is_image:
            self.images_processed += 1
        else:
            self.videos_processed += 1
    def add_failed_file(self, file_path: Path, error_type: str, error_msg: str) -> None:
        """Ajouter un fichier en échec."""
        self.total_failed += 1
        self.failed_files.append(f"{file_path.name}: {error_msg}")
        # Compter les erreurs par type
        if error_type in self.errors_by_type:
            self.errors_by_type[error_type] += 1
        else:
            self.errors_by_type[error_type] = 1
    def add_skipped_file(self, file_path: Path, reason: str) -> None:
        """Ajouter un fichier ignoré."""
        self.total_skipped += 1
        self.skipped_files.append(f"{file_path.name}: {reason}")
    def add_fixed_extension(self, old_name: str, new_name: str) -> None:
        """Ajouter une correction d'extension."""
        self.files_fixed_extension += 1
        self.fixed_extensions.append(f"{old_name} → {new_name}")
    def print_console_summary(self) -> None:
        """Afficher un résumé concis dans la console."""
        print("\n" + "="*60)
        print("📊 RÉSUMÉ DU TRAITEMENT")
        print("="*60)
        print(f"📁 Fichiers de métadonnées trouvés : {self.total_sidecars_found}")
        print(f"✅ Fichiers traités avec succès : {self.total_processed}")
        if self.images_processed > 0 or self.videos_processed > 0:
            print(f"   📸 Images : {self.images_processed}")
            print(f"   🎥 Vidéos : {self.videos_processed}")
        if self.total_failed > 0:
            print(f"❌ Fichiers en échec : {self.total_failed}")
        if self.total_skipped > 0:
            print(f"⏭️  Fichiers ignorés : {self.total_skipped}")
        if self.files_fixed_extension > 0:
            print(f"🔧 Extensions corrigées : {self.files_fixed_extension}")
        if self.sidecars_cleaned > 0:
            print(f"🗑️  Fichiers de métadonnées supprimés : {self.sidecars_cleaned}")
        # Taux de réussite
        if self.total_sidecars_found > 0:
            print(f"📈 Taux de réussite : {self.success_rate:.1f}%")
        # Durée
        if self.duration:
            print(f"⏱️  Durée : {self.duration:.1f}s")
        # Erreurs principales
        if self.errors_by_type:
            print("\n🔍 Types d'erreurs principales :")
            for error_type, count in sorted(self.errors_by_type.items(), key=lambda x: x[1], reverse=True)[:3]:
                print(f"   • {error_type}: {count} fichier(s)")
        print("="*60)
        if self.total_failed > 0 or self.total_skipped > 0:
            print("💡 Consultez le fichier de log détaillé pour plus d'informations.")
    def save_detailed_report(self, log_file: Path) -> None:
        """Sauvegarder un rapport détaillé dans un fichier spécifique à cette exécution."""
        report = {
            "execution_timestamp": datetime.now().isoformat(),
            "summary": {
                "total_sidecars_found": self.total_sidecars_found,
                "total_processed": self.total_processed,
                "total_failed": self.total_failed,
                "total_skipped": self.total_skipped,
                "images_processed": self.images_processed,
                "videos_processed": self.videos_processed,
                "files_fixed_extension": self.files_fixed_extension,
                "sidecars_cleaned": self.sidecars_cleaned,
                "success_rate": self.success_rate,
                "duration_seconds": self.duration
            },
            "details": {
                "failed_files": self.failed_files,
                "skipped_files": self.skipped_files,
                "fixed_extensions": self.fixed_extensions,
                "errors_by_type": self.errors_by_type
            }
        }
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"📄 Rapport détaillé sauvegardé : {log_file}")
        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde du rapport : {e}")
# Instance globale pour les statistiques
stats = ProcessingStats()
````

## File: test_assets/README.md
````markdown
# Assets de Test

Ce dossier contient des fichiers de référence pour les tests d'intégration.

## Fichiers disponibles

### Images
- **test_clean.jpg** : Image JPEG 100x100 sans métadonnées (nettoye avec `exiftool -all=`)
- **test_with_metadata.jpg** : Image JPEG 100x100 avec métadonnées de base
  - Description : "Existing description"
  - Rating : 3
  - Keywords : "Existing keyword"

### Vidéos
- **test_video_clean.mp4** : Vidéo MP4 sans métadonnées (nettoyée avec `exiftool -all=`)
- **test_video_with_metadata.mp4** : Vidéo MP4 avec métadonnées de base
  - Description : "Existing video description"
  - DateTimeOriginal : "2020:01:01 12:00:00"
  - Keywords : "Existing video keyword"

## Utilisation dans les tests

Les tests d'intégration utilisent la fonction `_copy_test_asset(asset_name, dest_path)` pour copier ces fichiers vers des répertoires temporaires avant chaque test. Cela garantit :

1. **Reproductibilité** : Chaque test commence avec les mêmes conditions
2. **Isolation** : Les tests ne s'interfèrent pas entre eux
3. **Contrôle** : Les métadonnées existantes sont connues et prévisibles

## Régénération des assets

Si nécessaire, les assets peuvent être régénérés avec :

```bash
# Se placer dans le dossier test_assets
cd test_assets

# Créer les images
python -c "
from PIL import Image
img = Image.new('RGB', (100, 100), color='red')
img.save('test_clean.jpg')
img2 = Image.new('RGB', (100, 100), color='blue') 
img2.save('test_with_metadata.jpg')
"

# Copier la vidéo de référence
cp "../Google Photos/essais/1686356837983.mp4" "test_video_clean.mp4"
cp "test_video_clean.mp4" "test_video_with_metadata.mp4"

# Nettoyer les fichiers clean
exiftool -overwrite_original -all= test_clean.jpg test_video_clean.mp4

# Ajouter des métadonnées aux fichiers with_metadata
exiftool -overwrite_original -EXIF:ImageDescription="Existing description" -XMP:Rating=3 -IPTC:Keywords="Existing keyword" test_with_metadata.jpg
exiftool -overwrite_original -api QuickTimeUTC=1 -XMP-dc:Description="Existing video description" -DateTimeOriginal="2020:01:01 12:00:00" -IPTC:Keywords="Existing video keyword" test_video_with_metadata.mp4
```
````

## File: tests/test_hybrid_approach.py
````python
import shutil
import tempfile
import subprocess
from pathlib import Path
from google_takeout_metadata.sidecar import SidecarData
from google_takeout_metadata.exif_writer import write_metadata
def read_exif_people(image_path: Path) -> list[str]:
    """Lit les personnes depuis un fichier image en utilisant exiftool."""
    try:
        cmd = ["exiftool", "-s", "-s", "-s", "-XMP-iptcExt:PersonInImage", str(image_path)]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30, encoding='utf-8')
        if result.returncode == 0 and result.stdout.strip():
            # ExifTool retourne les valeurs séparées par des virgules
            people = [p.strip() for p in result.stdout.strip().split(',')]
            return [p for p in people if p]  # Filtrer les valeurs vides
    except (subprocess.SubprocessError, OSError):
        pass
    return []
def test_hybrid_approach_no_duplicates():
    """Test de l'approche hybride : pas de doublons quand on ajoute des personnes existantes."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Copier l'image test dans le répertoire temporaire
        test_image_src = Path("test_assets/test_clean.jpg")
        test_image = Path(temp_dir) / "test_image.jpg"
        shutil.copy2(test_image_src, test_image)
        # Étape 1 : Ajouter les premières personnes (ancien takeout)
        meta1 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Anthony", "Bernard"],
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False,
            albums=["Vacances"]
        )
        write_metadata(test_image, meta1, append_only=True)
        # Vérifier l'état initial
        people_initial = read_exif_people(test_image)
        assert "Anthony" in people_initial
        assert "Bernard" in people_initial
        assert len([p for p in people_initial if p == "Anthony"]) == 1
        assert len([p for p in people_initial if p == "Bernard"]) == 1
        # Étape 2 : Ajouter nouveaux + existants (nouveau takeout avec tous les gens)
        meta2 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Anthony", "Bernard", "Cindy"],  # Contient TOUS les gens, pas juste les nouveaux
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False,
            albums=["Vacances", "Famille"]
        )
        write_metadata(test_image, meta2, append_only=True)
        # Vérifier le résultat final : pas de doublons malgré la redondance
        people_final = read_exif_people(test_image)
        print(f"Personnes finales: {people_final}")
        # Assertions critiques : aucun doublon
        assert "Anthony" in people_final
        assert "Bernard" in people_final 
        assert "Cindy" in people_final
        assert len([p for p in people_final if p == "Anthony"]) == 1, f"Anthony apparaît plusieurs fois: {people_final}"
        assert len([p for p in people_final if p == "Bernard"]) == 1, f"Bernard apparaît plusieurs fois: {people_final}"
        assert len([p for p in people_final if p == "Cindy"]) == 1, f"Cindy apparaît plusieurs fois: {people_final}"
        # Vérifier que toutes les personnes attendues sont présentes
        expected_people = {"Anthony", "Bernard", "Cindy"}
        actual_people = set(people_final)
        assert expected_people.issubset(actual_people), f"Personnes manquantes. Attendu: {expected_people}, Réel: {actual_people}"
def test_hybrid_approach_only_new_people():
    """Test de l'approche hybride : ajouter seulement les nouvelles personnes."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Copier l'image test
        test_image_src = Path("test_assets/test_clean.jpg")
        test_image = Path(temp_dir) / "test_image.jpg"
        shutil.copy2(test_image_src, test_image)
        # Étape 1 : Ajouter les premières personnes
        meta1 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Alice", "Bob"],
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False
        )
        write_metadata(test_image, meta1, append_only=True)
        # Étape 2 : Ajouter seulement les nouvelles personnes 
        meta2 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Charlie"],  # Seulement la nouvelle personne
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False
        )
        write_metadata(test_image, meta2, append_only=True)
        # Vérifier le résultat : toutes les personnes présentes, pas de doublons
        people_final = read_exif_people(test_image)
        expected_people = {"Alice", "Bob", "Charlie"}
        actual_people = set(people_final)
        assert expected_people == actual_people, f"Attendu: {expected_people}, Réel: {actual_people}"
        assert len(people_final) == 3, f"Doublons détectés: {people_final}"
if __name__ == "__main__":
    test_hybrid_approach_no_duplicates()
    test_hybrid_approach_only_new_people()
    print("✅ Tests de l'approche hybride : SUCCÈS")
````

## File: tests/test_cli.py
````python
"""Tests pour l'interface en ligne de commande."""
import json
import subprocess
import sys
from unittest.mock import patch
import pytest
from PIL import Image
from google_takeout_metadata.cli import main
def test_main_no_args(capsys):
    """Tester que la CLI sans arguments affiche l'aide."""
    with pytest.raises(SystemExit):
        main([])
    captured = capsys.readouterr()
    assert "usage:" in captured.err
def test_main_help(capsys):
    """Tester l'option d'aide de la CLI."""
    with pytest.raises(SystemExit):
        main(["--help"])
    captured = capsys.readouterr()
    assert "Fusionner les métadonnées Google Takeout dans les images" in captured.out
def test_main_invalid_directory(capsys, tmp_path):
    """Tester la CLI avec un répertoire inexistant."""
    non_existent = tmp_path / "does_not_exist"
    with pytest.raises(SystemExit):
        main([str(non_existent)])
    # L'erreur est enregistrée mais pas affichée sur stderr avec la configuration actuelle
    # Donc nous ne vérifions pas la sortie capturée, juste qu'elle se termine
def test_main_file_instead_of_directory(capsys, tmp_path):
    """Tester la CLI avec un chemin de fichier au lieu d'un répertoire."""
    test_file = tmp_path / "test.txt"
    test_file.write_text("test")
    with pytest.raises(SystemExit):
        main([str(test_file)])
    # L'erreur est enregistrée mais pas affichée sur stderr avec la configuration actuelle
    # Donc nous ne vérifions pas la sortie capturée, juste qu'elle se termine
@patch('google_takeout_metadata.cli.process_directory')
def test_main_normal_mode(mock_process_directory, tmp_path):
    """Tester le mode de traitement normal de la CLI."""
    main([str(tmp_path)])
    mock_process_directory.assert_called_once_with(
        tmp_path, use_localtime=False, append_only=True, clean_sidecars=False
    )
@patch('google_takeout_metadata.cli.process_directory_batch')
def test_main_batch_mode(mock_process_directory_batch, tmp_path):
    """Tester le mode de traitement par lot de la CLI."""
    main(["--batch", str(tmp_path)])
    mock_process_directory_batch.assert_called_once_with(
        tmp_path, use_localtime=False, append_only=True, clean_sidecars=False
    )
@patch('google_takeout_metadata.cli.process_directory')
def test_main_localtime_option(mock_process_directory, tmp_path):
    """Tester la CLI avec l'option localtime."""
    main(["--localtime", str(tmp_path)])
    mock_process_directory.assert_called_once_with(
        tmp_path, use_localtime=True, append_only=True, clean_sidecars=False
    )
@patch('google_takeout_metadata.cli.process_directory')
def test_main_overwrite_option(mock_process_directory, tmp_path):
    """Tester la CLI avec l'option overwrite."""
    main(["--overwrite", str(tmp_path)])
    mock_process_directory.assert_called_once_with(
        tmp_path, use_localtime=False, append_only=False, clean_sidecars=False
    )
@patch('google_takeout_metadata.cli.process_directory')
def test_main_clean_sidecars_option(mock_process_directory, tmp_path):
    """Tester la CLI avec l'option clean-sidecars."""
    main(["--clean-sidecars", str(tmp_path)])
    mock_process_directory.assert_called_once_with(
        tmp_path, use_localtime=False, append_only=True, clean_sidecars=True
    )
@patch('google_takeout_metadata.cli.process_directory_batch')
def test_main_batch_with_all_options(mock_process_directory_batch, tmp_path):
    """Tester le mode batch de la CLI avec toutes les options."""
    main(["--batch", "--localtime", "--overwrite", "--clean-sidecars", str(tmp_path)])
    mock_process_directory_batch.assert_called_once_with(
        tmp_path, use_localtime=True, append_only=False, clean_sidecars=True
    )
def test_main_conflicting_options(capsys):
    """Tester la CLI avec des options conflictuelles append-only et overwrite obsolètes."""
    with pytest.raises(SystemExit):
        main(["--append-only", "--overwrite", "/some/path"])
    # L'erreur est enregistrée mais pas affichée sur stderr avec la configuration actuelle
    # Donc nous ne vérifions pas la sortie capturée, juste qu'elle se termine
@patch('google_takeout_metadata.cli.process_directory')
def test_main_deprecated_append_only_warning(mock_process_directory, tmp_path, caplog):
    """Tester que la CLI avec l'option append-only obsolète affiche un avertissement."""
    main(["--append-only", str(tmp_path)])
    assert "--append-only est obsolète" in caplog.text
    mock_process_directory.assert_called_once_with(
        tmp_path, use_localtime=False, append_only=True, clean_sidecars=False
    )
@patch('google_takeout_metadata.cli.process_directory')
def test_main_verbose_logging(mock_process_directory, tmp_path, caplog):
    """Tester que la CLI avec l'option verbose active le logging de debug."""
    # Nous devons tester que basicConfig a été appelé avec le niveau DEBUG
    # mais le niveau du logger root pourrait ne pas changer pendant le test
    main(["--verbose", str(tmp_path)])
    # S'assurer simplement que la fonction a été appelée - le test de logging est plus complexe
    # en raison de la façon dont pytest gère le logging
    mock_process_directory.assert_called_once()
@pytest.mark.integration
def test_main_integration_normal_mode(tmp_path):
    """Test d'intégration pour le mode normal de la CLI avec des fichiers réels."""
    try:
        # Créer une image de test
        media_path = tmp_path / "cli_test.jpg"
        img = Image.new('RGB', (100, 100), color='purple')
        img.save(media_path)
        # Créer le sidecar
        sidecar_data = {
            "title": "cli_test.jpg",
            "description": "CLI integration test"
        }
        json_path = tmp_path / "cli_test.jpg.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Exécuter la CLI
        main([str(tmp_path)])
        # Vérifier que les métadonnées ont été écrites
        cmd = [
            "exiftool",
            "-j",
            "-EXIF:ImageDescription",
            str(media_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        metadata = json.loads(result.stdout)[0]
        assert metadata.get("ImageDescription") == "CLI integration test"
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping CLI integration test")
@pytest.mark.integration
def test_main_integration_batch_mode(tmp_path):
    """Test d'intégration pour le mode batch de la CLI avec des fichiers réels."""
    try:
        # Créer plusieurs images de test
        files_data = [
            ("batch1.jpg", "CLI batch test 1"),
            ("batch2.jpg", "CLI batch test 2")
        ]
        for filename, description in files_data:
            # Créer l'image
            media_path = tmp_path / filename
            img = Image.new('RGB', (100, 100), color='orange')
            img.save(media_path)
            # Créer le sidecar
            sidecar_data = {
                "title": filename,
                "description": description
            }
            json_path = tmp_path / f"{filename}.json"
            json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Exécuter la CLI en mode batch
        main(["--batch", str(tmp_path)])
        # Vérifier que tous les fichiers ont été traités
        for filename, expected_description in files_data:
            media_path = tmp_path / filename
            cmd = [
                "exiftool",
                "-j",
                "-EXIF:ImageDescription",
                str(media_path)
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
            metadata = json.loads(result.stdout)[0]
            assert metadata.get("ImageDescription") == expected_description
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping CLI batch integration test")
@pytest.mark.integration
def test_main_integration_clean_sidecars(tmp_path):
    """Test d'intégration pour la CLI avec nettoyage des sidecars."""
    try:
        # Créer une image de test
        media_path = tmp_path / "cleanup.jpg"
        img = Image.new('RGB', (100, 100), color='cyan')
        img.save(media_path)
        # Créer le sidecar
        sidecar_data = {
            "title": "cleanup.jpg",
            "description": "CLI cleanup test"
        }
        json_path = tmp_path / "cleanup.jpg.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Vérifier que le sidecar existe
        assert json_path.exists()
        # Exécuter la CLI avec nettoyage
        main(["--clean-sidecars", str(tmp_path)])
        # Vérifier que le sidecar a été supprimé
        assert not json_path.exists()
        # Vérifier que les métadonnées ont quand même été écrites
        cmd = [
            "exiftool",
            "-j",
            "-EXIF:ImageDescription",
            str(media_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        metadata = json.loads(result.stdout)[0]
        assert metadata.get("ImageDescription") == "CLI cleanup test"
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping CLI cleanup integration test")
def test_main_entry_point():
    """Tester que la fonction main peut être appelée sans arguments depuis le point d'entrée."""
    # Cela teste principalement que la signature de la fonction main est correcte pour les points d'entrée
    # Nous ne pouvons pas tester l'analyse CLI réelle sans mocker sys.argv
    with patch.object(sys, 'argv', ['google-takeout-metadata', '--help']):
        with pytest.raises(SystemExit):
            main()
````

## File: tests/test_end_to_end.py
````python
from pathlib import Path
import json
import subprocess
import shutil
import pytest
from PIL import Image
from google_takeout_metadata.processor import process_directory
@pytest.mark.skipif(shutil.which("exiftool") is None, reason="exiftool not installed")
def test_end_to_end(tmp_path: Path) -> None:
    # créer une image factice
    img_path = tmp_path / "sample.jpg"
    Image.new("RGB", (10, 10), color="red").save(img_path)
    # créer le sidecar correspondant
    data = {
        "title": "sample.jpg",
        "description": 'Magicien "en" or',
        "photoTakenTime": {"timestamp": "1736719606"},
        "people": [{"name": "anthony vincent"}],
    }
    (tmp_path / "sample.jpg.json").write_text(json.dumps(data), encoding="utf-8")
    process_directory(tmp_path)
    exe = shutil.which("exiftool") or "exiftool"
    result = subprocess.run(
        [
            exe,
            "-j",
            "-XMP-iptcExt:PersonInImage",
            "-XMP-dc:Subject",
            "-IPTC:Keywords",
            "-EXIF:ImageDescription",
            str(img_path),
        ],
        capture_output=True,
        text=True,
        check=True,
    )
    tags = json.loads(result.stdout)[0]
    # exiftool retourne les valeurs uniques en chaînes, les valeurs multiples en listes
    # Normaliser en listes pour la comparaison
    def normalize_to_list(value):
        if value is None:
            return []
        elif isinstance(value, list):
            return value
        else:
            return [value]
    assert normalize_to_list(tags.get("PersonInImage")) == ["anthony vincent"]
    assert normalize_to_list(tags.get("Subject")) == ["anthony vincent"]
    assert normalize_to_list(tags.get("Keywords")) == ["anthony vincent"]
    assert tags.get("ImageDescription") == 'Magicien "en" or'
````

## File: tests/test_exif_writer.py
````python
from google_takeout_metadata.sidecar import SidecarData
from google_takeout_metadata.exif_writer import write_metadata, build_exiftool_args
import subprocess
import pytest
from pathlib import Path
def test_write_metadata_error(tmp_path, monkeypatch):
    meta = SidecarData(
        filename="a.jpg",
        description="test",  # Add description to ensure args are generated
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
    )
    img = tmp_path / "a.jpg"
    img.write_bytes(b"data")
    def fake_run(*args, **kwargs):
        raise subprocess.CalledProcessError(1, "exiftool", stderr="bad")
    monkeypatch.setattr(subprocess, "run", fake_run)
    with pytest.raises(RuntimeError):
        write_metadata(img, meta)
def test_build_args_video():
    """Tester que les balises spécifiques aux vidéos sont ajoutées pour les fichiers MP4/MOV."""
    meta = SidecarData(
        filename="video.mp4",
        description="Video description",
        people=["alice"],
        taken_at=1736719606,
        created_at=None,
        latitude=48.8566,
        longitude=2.3522,
        altitude=None,
        favorite=False,
    )
    video_path = Path("video.mp4")
    args = build_exiftool_args(meta, media_path=video_path)
    # Vérifier les balises spécifiques aux vidéos
    assert "-Keys:Description=Video description" in args
    assert any("-QuickTime:CreateDate=" in arg for arg in args)
    assert any("-QuickTime:ModifyDate=" in arg for arg in args)
    assert "-Keys:Location=48.8566,2.3522" in args
    assert "-QuickTime:GPSCoordinates=48.8566,2.3522" in args
    assert "-api" in args
    assert "QuickTimeUTC=1" in args
def test_build_args_localtime():
    """Tester que le formatage de l'heure locale fonctionne."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=1736719606,  # 2025-01-12 22:06:46 UTC
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
    )
    # Test UTC (default)
    args_utc = build_exiftool_args(meta, media_path=Path("a.jpg"), use_localtime=False)
    # Test local time
    args_local = build_exiftool_args(meta, media_path=Path("a.jpg"), use_localtime=True)
    # Les chaînes de date-heure seront différentes (sauf si exécuté dans le fuseau horaire UTC)
    # mais les deux devraient contenir une forme de DateTimeOriginal
    assert any("-DateTimeOriginal=" in arg for arg in args_utc)
    assert any("-DateTimeOriginal=" in arg for arg in args_local)
def test_build_args_append_only() -> None:
    """Tester que le mode append-only utilise la syntaxe exiftool correcte.
    Note: En production, la logique append-only est maintenant dans write_metadata()
    avec l'approche hybride (ajout + nettoyage NoDups). Ce test vérifie seulement 
    la fonction d'adaptation pour maintenir la compatibilité.
    """
    meta = SidecarData(
        filename="a.jpg",
        description="desc",
        people=["alice", "bob"],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
    )
    # Normal mode
    args_normal = build_exiftool_args(meta, append_only=False)
    assert "-EXIF:ImageDescription=desc" in args_normal
    # En mode overwrite, on vide d'abord puis on ajoute
    assert "-XMP-iptcExt:PersonInImage=" in args_normal
    assert "-XMP-iptcExt:PersonInImage+=alice" in args_normal
    # Append-only mode (fonction d'adaptation simplifiée)
    args_append = build_exiftool_args(meta, append_only=True)
    # Notre nouvelle approche hybride utilise += puis NoDups cleanup
    # La fonction d'adaptation reflète cette logique simplifiée
    assert "-EXIF:ImageDescription=desc" in args_append
    assert "-XMP-iptcExt:PersonInImage+=alice" in args_append
    assert "-XMP-iptcExt:PersonInImage+=bob" in args_append
def test_build_args_favorite() -> None:
    """Tester que les photos favorites obtiennent rating=5."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=True,
    )
    args = build_exiftool_args(meta, append_only=False)
    assert "-XMP:Rating=5" in args
    # Tester le mode append-only (maintenant le comportement par défaut)
    args_append = build_exiftool_args(meta, append_only=True)
    # Devrait utiliser -wm cg pour l'écriture conditionnelle
    assert "-wm" in args_append
    assert "cg" in args_append
    assert "-XMP:Rating=5" in args_append
def test_build_args_no_favorite() -> None:
    """Tester que les photos non favorites n'obtiennent pas de rating."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
    )
    args = build_exiftool_args(meta)
    assert not any("Rating" in arg for arg in args)
def test_build_args_albums() -> None:
    """Tester que les albums sont écrits comme mots-clés avec le préfixe Album:."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=["Vacances 2024", "Famille"]
    )
    args = build_exiftool_args(meta, append_only=False)
    # En mode overwrite, on vide d'abord puis on ajoute
    assert "-XMP-dc:Subject=" in args
    assert "-IPTC:Keywords=" in args
    assert "-XMP-dc:Subject+=Album: Vacances 2024" in args
    assert "-IPTC:Keywords+=Album: Vacances 2024" in args
    assert "-XMP-dc:Subject+=Album: Famille" in args
    assert "-IPTC:Keywords+=Album: Famille" in args
def test_build_args_video_append_only() -> None:
    """Tester que les balises spécifiques aux vidéos sont incluses en mode append-only."""
    meta = SidecarData(
        filename="video.mp4",
        description="Video description",
        people=["alice"],
        taken_at=1736719606,
        created_at=None,
        latitude=48.8566,
        longitude=2.3522,
        altitude=35.0,
        favorite=False,
    )
    video_path = Path("video.mp4")
    args = build_exiftool_args(meta, media_path=video_path, append_only=True)
    # Vérifier que l'approche append-only utilise -wm cg
    assert "-wm" in args
    assert "cg" in args
    assert "-Keys:Description=Video description" in args
    # Vérifier que les dates QuickTime sont présentes
    assert any("QuickTime:CreateDate=" in arg for arg in args)
    assert any("QuickTime:ModifyDate=" in arg for arg in args)
    # Vérifier que les champs GPS spécifiques à la vidéo sont présents
    assert "-QuickTime:GPSCoordinates=48.8566,2.3522" in args
    assert "-Keys:Location=48.8566,2.3522" in args
    # Vérifier que l'altitude est présente
    assert "-GPSAltitude=35.0" in args
    # Vérifier la configuration vidéo
    assert "-api" in args
    assert "QuickTimeUTC=1" in args
def test_build_args_albums_append_only() -> None:
    """Tester les albums en mode append-only."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=["Test Album"]
    )
    args = build_exiftool_args(meta, append_only=True)
    # Les albums utilisent += qui ajoute et accumule pour les balises de type liste en mode append-only
    assert "-XMP-dc:Subject+=Album: Test Album" in args
    assert "-IPTC:Keywords+=Album: Test Album" in args
def test_build_args_no_albums() -> None:
    """Tester que la liste d'albums vide n'ajoute aucune balise d'album."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=[]
    )
    args = build_exiftool_args(meta)
    assert not any("Album:" in arg for arg in args)
def test_build_args_default_behavior() -> None:
    """Tester que le comportement par défaut est append-only (mode sécurisé)."""
    meta = SidecarData(
        filename="a.jpg",
        description="Safe description",
        people=["Safe Person"],
        taken_at=1736719606,
        created_at=None,
        latitude=48.8566,
        longitude=2.3522,
        altitude=None,
        favorite=True,
    )
    # Le comportement par défaut devrait être append-only (sécurisé)
    args = build_exiftool_args(meta)
    # Devrait utiliser -wm cg pour l'écriture conditionnelle
    assert "-wm" in args
    assert "cg" in args
    assert "-EXIF:ImageDescription=Safe description" in args
    # Devrait utiliser += pour les personnes (accumulation en mode append-only)
    assert "-XMP-iptcExt:PersonInImage+=Safe Person" in args
    # Le rating devrait être présent
    assert "-XMP:Rating=5" in args
    # GPS devrait être présent
    assert "-GPSLatitude=48.8566" in args
def test_build_args_overwrite_mode() -> None:
    """Mode de réécriture explicite (destructif)."""
    meta = SidecarData(
        filename="a.jpg",
        description="Overwrite description",
        people=["Overwrite Person"],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=True,
    )
    # Mode de réécriture explicite
    args = build_exiftool_args(meta, append_only=False)
    # Devrait utiliser l'assignation directe pour les descriptions et les ratings
    assert "-EXIF:ImageDescription=Overwrite description" in args
    # En mode overwrite, on vide d'abord puis on ajoute
    assert "-XMP-iptcExt:PersonInImage=" in args
    assert "-XMP-iptcExt:PersonInImage+=Overwrite Person" in args
    assert "-XMP:Rating=5" in args
    # Ne devrait PAS avoir de conditions -if
    assert "-if" not in args
    assert "not $EXIF:ImageDescription" not in args
    assert "not $XMP-iptcExt:PersonInImage" not in args
    assert "not $XMP:Rating" not in args
def test_build_args_people_default() -> None:
    """Tester que les personnes sont gérées de manière sécurisée par défaut."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=["Alice Dupont", "Bob Martin", "Charlie Bernard"],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
    )
    # Comportement par défaut (append-only)
    args = build_exiftool_args(meta)
    # Chaque personne devrait utiliser += (accumulation en mode append-only)
    for person in ["Alice Dupont", "Bob Martin", "Charlie Bernard"]:
        assert f"-XMP-iptcExt:PersonInImage+={person}" in args
        assert f"-XMP-dc:Subject+={person}" in args
        assert f"-IPTC:Keywords+={person}" in args
    # Ne devrait PAS avoir de conditions -if pour les personnes (elles sont des listes, utiliser +=)
    assert "not $XMP-iptcExt:PersonInImage" not in args
    assert "not $XMP-dc:Subject" not in args
    assert "not $IPTC:Keywords" not in args
def test_build_args_albums_default() -> None:
    """Tester que les albums sont gérés de manière sécurisée par défaut."""
    meta = SidecarData(
        filename="a.jpg",
        description=None,
        people=[],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=["Vacances Été 2024", "Photos de Famille", "Événements Spéciaux"]
    )
    # Comportement par défaut (append-only)
    args = build_exiftool_args(meta)
    # Chaque album devrait utiliser += (accumulation en mode append-only)
    for album in ["Vacances Été 2024", "Photos de Famille", "Événements Spéciaux"]:
        album_keyword = f"Album: {album}"
        assert f"-XMP-dc:Subject+={album_keyword}" in args
        assert f"-IPTC:Keywords+={album_keyword}" in args
    # Ne devrait PAS avoir de conditions -if pour les albums (ils sont des listes, utiliser +=)
    assert "not $XMP-dc:Subject" not in args
    assert "not $IPTC:Keywords" not in args
````

## File: tests/test_hybrid_approach.py
````python
import shutil
import tempfile
import subprocess
from pathlib import Path
from google_takeout_metadata.sidecar import SidecarData
from google_takeout_metadata.exif_writer import write_metadata
def read_exif_people(image_path: Path) -> list[str]:
    """Lit les personnes depuis un fichier image en utilisant exiftool."""
    try:
        cmd = ["exiftool", "-s", "-s", "-s", "-XMP-iptcExt:PersonInImage", str(image_path)]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30, encoding='utf-8')
        if result.returncode == 0 and result.stdout.strip():
            # ExifTool retourne les valeurs séparées par des virgules
            people = [p.strip() for p in result.stdout.strip().split(',')]
            return [p for p in people if p]  # Filtrer les valeurs vides
    except (subprocess.SubprocessError, OSError):
        pass
    return []
def test_hybrid_approach_no_duplicates():
    """Test de l'approche hybride : pas de doublons quand on ajoute des personnes existantes."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Copier l'image test dans le répertoire temporaire
        test_image_src = Path("test_assets/test_clean.jpg")
        test_image = Path(temp_dir) / "test_image.jpg"
        shutil.copy2(test_image_src, test_image)
        # Étape 1 : Ajouter les premières personnes (ancien takeout)
        meta1 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Anthony", "Bernard"],
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False,
            albums=["Vacances"]
        )
        write_metadata(test_image, meta1, append_only=True)
        # Vérifier l'état initial
        people_initial = read_exif_people(test_image)
        assert "Anthony" in people_initial
        assert "Bernard" in people_initial
        assert len([p for p in people_initial if p == "Anthony"]) == 1
        assert len([p for p in people_initial if p == "Bernard"]) == 1
        # Étape 2 : Ajouter nouveaux + existants (nouveau takeout avec tous les gens)
        meta2 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Anthony", "Bernard", "Cindy"],  # Contient TOUS les gens, pas juste les nouveaux
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False,
            albums=["Vacances", "Famille"]
        )
        write_metadata(test_image, meta2, append_only=True)
        # Vérifier le résultat final : pas de doublons malgré la redondance
        people_final = read_exif_people(test_image)
        print(f"Personnes finales: {people_final}")
        # Assertions critiques : aucun doublon
        assert "Anthony" in people_final
        assert "Bernard" in people_final 
        assert "Cindy" in people_final
        assert len([p for p in people_final if p == "Anthony"]) == 1, f"Anthony apparaît plusieurs fois: {people_final}"
        assert len([p for p in people_final if p == "Bernard"]) == 1, f"Bernard apparaît plusieurs fois: {people_final}"
        assert len([p for p in people_final if p == "Cindy"]) == 1, f"Cindy apparaît plusieurs fois: {people_final}"
        # Vérifier que toutes les personnes attendues sont présentes
        expected_people = {"Anthony", "Bernard", "Cindy"}
        actual_people = set(people_final)
        assert expected_people.issubset(actual_people), f"Personnes manquantes. Attendu: {expected_people}, Réel: {actual_people}"
def test_hybrid_approach_only_new_people():
    """Test de l'approche hybride : ajouter seulement les nouvelles personnes."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Copier l'image test
        test_image_src = Path("test_assets/test_clean.jpg")
        test_image = Path(temp_dir) / "test_image.jpg"
        shutil.copy2(test_image_src, test_image)
        # Étape 1 : Ajouter les premières personnes
        meta1 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Alice", "Bob"],
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False
        )
        write_metadata(test_image, meta1, append_only=True)
        # Étape 2 : Ajouter seulement les nouvelles personnes 
        meta2 = SidecarData(
            filename="test_image.jpg",
            description=None,
            people=["Charlie"],  # Seulement la nouvelle personne
            taken_at=None,
            created_at=None,
            latitude=None,
            longitude=None,
            altitude=None,
            favorite=False
        )
        write_metadata(test_image, meta2, append_only=True)
        # Vérifier le résultat : toutes les personnes présentes, pas de doublons
        people_final = read_exif_people(test_image)
        expected_people = {"Alice", "Bob", "Charlie"}
        actual_people = set(people_final)
        assert expected_people == actual_people, f"Attendu: {expected_people}, Réel: {actual_people}"
        assert len(people_final) == 3, f"Doublons détectés: {people_final}"
if __name__ == "__main__":
    test_hybrid_approach_no_duplicates()
    test_hybrid_approach_only_new_people()
    print("✅ Tests de l'approche hybride : SUCCÈS")
````

## File: tests/test_improvements.py
````python
"""Tests unitaires pour les améliorations des statistiques et de la recherche d'albums."""
import json
import pytest
import subprocess
import tempfile
from pathlib import Path
from unittest.mock import patch
from PIL import Image
from google_takeout_metadata.processor_batch import process_batch
from google_takeout_metadata.sidecar import find_albums_for_directory
from google_takeout_metadata.statistics import ProcessingStats
class TestProcessingStats:
    """Tests pour la classe ProcessingStats."""
    def test_init(self):
        """Test de l'initialisation des statistiques."""
        stats = ProcessingStats()
        assert stats.total_sidecars_found == 0
        assert stats.total_processed == 0
        assert stats.total_failed == 0
        assert stats.total_skipped == 0
        assert stats.images_processed == 0
        assert stats.videos_processed == 0
        assert stats.files_fixed_extension == 0
        assert stats.sidecars_cleaned == 0
        assert stats.failed_files == []
        assert stats.skipped_files == []
        assert stats.fixed_extensions == []
        assert stats.errors_by_type == {}
        assert stats.start_time is None
        assert stats.end_time is None
    def test_add_processed_file(self):
        """Test de l'ajout de fichiers traités."""
        stats = ProcessingStats()
        test_path = Path("test_image.jpg")
        # Test image
        stats.add_processed_file(test_path, is_image=True)
        assert stats.total_processed == 1
        assert stats.images_processed == 1
        assert stats.videos_processed == 0
        # Test vidéo
        stats.add_processed_file(Path("test_video.mp4"), is_image=False)
        assert stats.total_processed == 2
        assert stats.images_processed == 1
        assert stats.videos_processed == 1
    def test_add_failed_file(self):
        """Test de l'ajout de fichiers en échec."""
        stats = ProcessingStats()
        test_path = Path("test_file.jpg")
        stats.add_failed_file(test_path, "parse_error", "JSON invalide")
        assert stats.total_failed == 1
        assert len(stats.failed_files) == 1
        assert stats.failed_files[0] == "test_file.jpg: JSON invalide"
        assert stats.errors_by_type["parse_error"] == 1
        # Test comptage des erreurs par type
        stats.add_failed_file(test_path, "parse_error", "Autre erreur JSON")
        assert stats.errors_by_type["parse_error"] == 2
    def test_add_skipped_file(self):
        """Test de l'ajout de fichiers ignorés."""
        stats = ProcessingStats()
        test_path = Path("test_file.jpg")
        stats.add_skipped_file(test_path, "Fichier déjà traité")
        assert stats.total_skipped == 1
        assert len(stats.skipped_files) == 1
        assert stats.skipped_files[0] == "test_file.jpg: Fichier déjà traité"
    def test_add_fixed_extension(self):
        """Test de l'ajout de corrections d'extension."""
        stats = ProcessingStats()
        stats.add_fixed_extension("image.png", "image.jpg")
        assert stats.files_fixed_extension == 1
        assert len(stats.fixed_extensions) == 1
        assert stats.fixed_extensions[0] == "image.png → image.jpg"
    def test_success_rate(self):
        """Test du calcul du taux de réussite."""
        stats = ProcessingStats()
        # Aucun fichier
        assert stats.success_rate == 0.0
        # Quelques fichiers
        stats.total_sidecars_found = 10
        stats.total_processed = 8
        assert stats.success_rate == 80.0
        # Tous réussis
        stats.total_processed = 10
        assert stats.success_rate == 100.0
    def test_timing(self):
        """Test du système de timing."""
        stats = ProcessingStats()
        assert stats.duration is None
        stats.start_processing()
        assert stats.start_time is not None
        assert stats.duration is None
        stats.end_processing()
        assert stats.end_time is not None
        assert stats.duration is not None
        assert stats.duration >= 0
class TestFindAlbumsForDirectory:
    """Tests pour la fonction find_albums_for_directory améliorée."""
    def test_empty_directory(self):
        """Test avec un répertoire vide."""
        with tempfile.TemporaryDirectory() as temp_dir:
            result = find_albums_for_directory(Path(temp_dir))
            assert result == []
    def test_case_insensitive_metadata_files(self):
        """Test de la gestion insensible à la casse."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Créer des fichiers avec différentes casses
            (temp_path / "METADATA.JSON").write_text('{"title": "Album1"}', encoding='utf-8')
            (temp_path / "métadonnées.json").write_text('{"title": "Album2"}', encoding='utf-8')
            (temp_path / "Album_Metadata.JSON").write_text('{"title": "Album3"}', encoding='utf-8')
            result = find_albums_for_directory(temp_path)
            # Doit trouver tous les albums
            assert len(result) == 3
            assert "Album1" in result
            assert "Album2" in result
            assert "Album3" in result
    def test_numbered_variations_case_insensitive(self):
        """Test des variations numérotées insensibles à la casse."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Créer des fichiers avec variations numérotées et casses différentes
            (temp_path / "Métadonnées(1).JSON").write_text('{"title": "Album1"}', encoding='utf-8')
            (temp_path / "MÉTADONNÉES(2).json").write_text('{"title": "Album2"}', encoding='utf-8')
            result = find_albums_for_directory(temp_path)
            assert len(result) == 2
            assert "Album1" in result
            assert "Album2" in result
    def test_max_depth_limit(self):
        """Test de la limite de profondeur."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Créer une hiérarchie simple : temp_dir/parent/current
            parent_dir = temp_path / "parent"
            parent_dir.mkdir()
            current_dir = parent_dir / "current"
            current_dir.mkdir()
            # Ajouter un album au niveau parent
            (parent_dir / "metadata.json").write_text('{"title": "ParentAlbum"}', encoding='utf-8')
            # Ajouter un album au niveau racine (temp_dir)
            (temp_path / "metadata.json").write_text('{"title": "RootAlbum"}', encoding='utf-8')
            # Test avec max_depth=2 (permet de vérifier current_dir et parent_dir)
            result = find_albums_for_directory(current_dir, max_depth=2)
            assert "ParentAlbum" in result
            assert "RootAlbum" not in result  # temp_dir est à depth=2, donc exclu
            # Test avec max_depth=3 (permet de vérifier current_dir, parent_dir et temp_dir)
            result_full = find_albums_for_directory(current_dir, max_depth=3)
            assert "ParentAlbum" in result_full
            assert "RootAlbum" in result_full
            # Test avec max_depth=1 (ne vérifie que current_dir)
            result_limited = find_albums_for_directory(current_dir, max_depth=1)
            assert len(result_limited) == 0  # pas d'album dans current_dir
    def test_takeout_marker_detection(self):
        """Test de la détection des répertoires marqueurs."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Créer une hiérarchie avec marqueur
            root_dir = temp_path / "root"
            root_dir.mkdir()
            takeout_dir = root_dir / "mon-takeout" 
            takeout_dir.mkdir()
            photos_dir = takeout_dir / "photos"
            photos_dir.mkdir()
            # Ajouter un album au niveau root (plus haut que le marqueur)
            (root_dir / "metadata.json").write_text('{"title": "RootAlbum"}', encoding='utf-8')
            result = find_albums_for_directory(photos_dir)
            # Doit s'arrêter au marqueur et ne pas remonter jusqu'au root
            assert "RootAlbum" not in result
    def test_order_preservation(self):
        """Test de la préservation de l'ordre de priorité."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Créer une hiérarchie avec albums à différents niveaux
            level1 = temp_path / "level1"
            level1.mkdir()
            # Album au niveau courant (priorité haute)
            (temp_path / "metadata.json").write_text('{"title": "CurrentLevel"}', encoding='utf-8')
            # Album au niveau parent (priorité basse)
            (level1 / "metadata.json").write_text('{"title": "ParentLevel"}', encoding='utf-8')
            result = find_albums_for_directory(temp_path)
            # L'album du niveau courant doit être en premier
            assert result[0] == "CurrentLevel"
    def test_error_handling(self):
        """Test de la gestion d'erreurs."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Créer un fichier JSON invalide
            (temp_path / "metadata.json").write_text('{"invalid": json}', encoding='utf-8')
            # Créer un fichier JSON valide
            (temp_path / "album_metadata.json").write_text('{"title": "ValidAlbum"}', encoding='utf-8')
            # La fonction doit continuer malgré l'erreur
            result = find_albums_for_directory(temp_path)
            # Doit trouver l'album valide
            assert "ValidAlbum" in result
    @patch('google_takeout_metadata.sidecar.logger')
    def test_debug_logging(self, mock_logger):
        """Test des logs debug."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            # Créer une hiérarchie avec marqueur et album au-dessus
            root_dir = temp_path / "root"
            root_dir.mkdir()
            takeout_dir = root_dir / "Google Photos"  # Un marqueur sûr
            takeout_dir.mkdir()
            photos_dir = takeout_dir / "photos"
            photos_dir.mkdir()
            # Ajouter un album au niveau root pour forcer la remontée
            (root_dir / "metadata.json").write_text('{"title": "RootAlbum"}', encoding='utf-8')
            find_albums_for_directory(photos_dir)
            # Vérifier que le debug a été appelé (pour n'importe quel message debug)
            assert mock_logger.debug.called, "Aucun appel au logger.debug détecté"
            # Vérifier les appels debug pour trouver celui du marqueur
            debug_calls = [str(call) for call in mock_logger.debug.call_args_list]
            # Chercher un message contenant "marqueur"
            marker_calls = [call for call in debug_calls if "marqueur" in call.lower()]
            assert len(marker_calls) > 0, f"Pas de log marqueur trouvé dans: {debug_calls}"
            calls = [call for call in mock_logger.debug.call_args_list 
                    if "répertoire marqueur" in str(call)]
            assert len(calls) > 0
@pytest.mark.integration
def test_batch_sidecar_cleanup_with_condition_failure(tmp_path: Path) -> None:
    """Tester que les sidecars sont supprimés même quand 'files failed condition' survient en mode batch."""
    # Créer une image de test
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='blue')
    img.save(media_path)
    # Ajouter des métadonnées existantes (description EXIF)
    try:
        subprocess.run([
            "exiftool", "-overwrite_original",
            "-EXIF:ImageDescription=Existing description",
            str(media_path)
        ], capture_output=True, text=True, check=True, timeout=30)
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping integration test")
    # Créer le sidecar JSON avec une description différente (qui causera "files failed condition")
    sidecar_data = {
        "title": "test.jpg",
        "description": "New description that should not overwrite existing"
    }
    json_path = tmp_path / "test.jpg.supplemental-metadata.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Vérifier que le sidecar existe avant traitement
    assert json_path.exists()
    # Créer le lot avec clean_sidecars=True
    batch = [(media_path, json_path, [])]  # Liste vide pour les args car on teste juste le nettoyage
    # Traiter le lot avec nettoyage activé
    # Ceci devrait déclencher "files failed condition" car la description existe déjà
    # ET devrait quand même supprimer le sidecar
    result = process_batch(batch, clean_sidecars=True)
    # Vérifier que le traitement a réussi (malgré "files failed condition")
    assert result > 0
    # Vérifier que le sidecar a été supprimé (le bug corrigé)
    assert not json_path.exists(), "Le sidecar aurait dû être supprimé après traitement réussi avec 'files failed condition'"
def test_batch_cleanup_logic_unit() -> None:
    """Test unitaire pour vérifier la logique de nettoyage en cas de 'files failed condition'."""
    # Ce test vérifie que notre modification de code est cohérente
    # Il ne teste pas exiftool mais la logique interne
    from google_takeout_metadata.processor_batch import process_batch
    import tempfile
    from pathlib import Path
    import json
    from unittest.mock import patch
    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_path = Path(tmp_dir)
        # Créer des fichiers factices
        media_path = tmp_path / "test.jpg"
        media_path.write_text("fake image content")
        json_path = tmp_path / "test.jpg.supplemental-metadata.json"
        sidecar_data = {"title": "test.jpg", "description": "Test description"}
        json_path.write_text(json.dumps(sidecar_data))
        batch = [(media_path, json_path, ["-description=test"])]
        # Mock subprocess.run pour simuler "files failed condition"
        mock_error = subprocess.CalledProcessError(2, "exiftool")
        mock_error.stderr = "2 files failed condition"
        mock_error.stdout = ""
        with patch('google_takeout_metadata.processor_batch.subprocess.run', side_effect=mock_error):
            # Vérifier que le fichier existe avant
            assert json_path.exists()
            # Appeler process_batch avec clean_sidecars=True
            result = process_batch(batch, clean_sidecars=True)
            # Vérifier le succès et la suppression
            assert result == 1, "Le batch devrait être considéré comme réussi"
            assert not json_path.exists(), "Le sidecar aurait dû être supprimé même avec 'files failed condition'"
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
````

## File: tests/test_integration.py
````python
"""Tests d'intégration qui exécutent réellement exiftool et vérifient que les métadonnées sont écrites correctement."""
from pathlib import Path
import json
import subprocess
import pytest
import shutil
from PIL import Image
from google_takeout_metadata.processor import process_sidecar_file
from google_takeout_metadata.exif_writer import write_metadata
from google_takeout_metadata.sidecar import SidecarData
def _get_test_assets_dir() -> Path:
    """Retourne le chemin vers le dossier des assets de test."""
    return Path(__file__).parent.parent / "test_assets"
def _copy_test_asset(asset_name: str, dest_path: Path) -> None:
    """Copie un asset de test vers le chemin de destination."""
    assets_dir = _get_test_assets_dir()
    asset_path = assets_dir / asset_name
    if not asset_path.exists():
        pytest.skip(f"Asset de test {asset_name} introuvable dans {assets_dir}")
    shutil.copy2(asset_path, dest_path)
def _run_exiftool_read(media_path: Path) -> dict:
    """Exécuter exiftool pour lire les métadonnées depuis un fichier image."""
    cmd = [
        "exiftool", 
        "-json",
        "-charset", "filename=UTF8",
        "-charset", "iptc=UTF8", 
        "-charset", "exif=UTF8",
        "-charset", "XMP=UTF8",
        str(media_path)
    ]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        data = json.loads(result.stdout)
        return data[0] if data else {}
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping integration tests")
    except subprocess.CalledProcessError as e:
        pytest.fail(f"exiftool failed: {e.stderr}")
@pytest.mark.integration
def test_write_and_read_description(tmp_path: Path) -> None:
    """Tester que la description est écrite et peut être relue."""
    # Utiliser un asset de test propre
    media_path = tmp_path / "test.jpg"
    _copy_test_asset("test_clean.jpg", media_path)
    # Créer le JSON sidecar
    sidecar_data = {
        "title": "test.jpg",
        "description": "Test photo with ñ and émojis 🎉"
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les métadonnées
    metadata = _run_exiftool_read(media_path)
    # Vérifier que la description a été écrite
    assert metadata.get("Description") == "Test photo with ñ and émojis 🎉"
    assert metadata.get("ImageDescription") == "Test photo with ñ and émojis 🎉"
@pytest.mark.integration
def test_write_and_read_people(tmp_path: Path) -> None:
    """Tester que les noms de personnes sont écrits et peuvent être relus."""
    # Utiliser un asset de test propre
    media_path = tmp_path / "test.jpg"
    _copy_test_asset("test_clean.jpg", media_path)
    # Créer le JSON sidecar avec des personnes
    sidecar_data = {
        "title": "test.jpg",
        "people": [
            {"name": "Alice Dupont"},
            {"name": "Bob Martin"}
        ]
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les métadonnées
    metadata = _run_exiftool_read(media_path)
    # Vérifier que les personnes ont été écrites
    keywords = metadata.get("Keywords", [])
    if isinstance(keywords, str):
        keywords = [keywords]
    assert "Alice Dupont" in keywords
    assert "Bob Martin" in keywords
@pytest.mark.integration 
def test_write_and_read_gps(tmp_path: Path) -> None:
    """Tester que les coordonnées GPS sont écrites et peuvent être relues."""
    # Utiliser un asset de test propre
    media_path = tmp_path / "test.jpg"
    _copy_test_asset("test_clean.jpg", media_path)
    # Créer le JSON sidecar avec des données GPS
    sidecar_data = {
        "title": "test.jpg",
        "geoData": {
            "latitude": 48.8566,
            "longitude": 2.3522,
            "altitude": 35.0
        }
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les métadonnées
    metadata = _run_exiftool_read(media_path)
    # Vérifier que les données GPS ont été écrites
    # exiftool retourne les coordonnées GPS dans un format lisible, donc on doit vérifier différemment
    gps_lat = metadata.get("GPSLatitude")
    gps_lon = metadata.get("GPSLongitude")
    # Vérifier que les champs GPS existent et contiennent les valeurs de degrés attendues
    assert gps_lat is not None, "GPSLatitude devrait être définie"
    assert gps_lon is not None, "GPSLongitude devrait être définie"
    assert "48 deg" in str(gps_lat), f"Expected 48 degrees in latitude, got: {gps_lat}"
    assert "2 deg" in str(gps_lon), f"Expected 2 degrees in longitude, got: {gps_lon}"
    # Les références GPS peuvent être "N"/"North" et "E"/"East" selon la version d'exiftool
    lat_ref = metadata.get("GPSLatitudeRef")
    lon_ref = metadata.get("GPSLongitudeRef")
    assert lat_ref in ["N", "North"], f"Expected N or North for latitude ref, got: {lat_ref}"
    assert lon_ref in ["E", "East"], f"Expected E or East for longitude ref, got: {lon_ref}"
@pytest.mark.integration
def test_write_and_read_favorite(tmp_path: Path) -> None:
    """Tester que le statut favori est écrit comme notation."""
    # Créer une image de test simple
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='yellow')
    img.save(media_path)
    # Créer le fichier JSON annexe avec favori
    sidecar_data = {
        "title": "test.jpg",
        "favorited": True
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les métadonnées
    metadata = _run_exiftool_read(media_path)
    # Vérifier que la notation a été écrite
    assert int(metadata.get("Rating", 0)) == 5
@pytest.mark.integration
def test_append_only_mode(tmp_path: Path) -> None:
    """Tester que le mode append-only n'écrase pas la description existante."""
    # Utiliser un asset de test avec métadonnées existantes
    media_path = tmp_path / "test.jpg"
    _copy_test_asset("test_with_metadata.jpg", media_path)
    # Créer le fichier JSON annexe avec une description différente
    sidecar_data = {
        "title": "test.jpg", 
        "description": "New description from sidecar"
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar en mode append-only
    process_sidecar_file(json_path, append_only=True)
    # Relire les métadonnées
    metadata = _run_exiftool_read(media_path)
    # En mode append-only, la description originale devrait être préservée
    # Note: exiftool's -= operator doesn't overwrite if field exists
    assert metadata.get("ImageDescription") == "Existing description"
@pytest.mark.integration
def test_datetime_formats(tmp_path: Path) -> None:
    """Tester que la date-heure est écrite dans le bon format."""
    # Créer une image de test simple
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='orange')
    img.save(media_path)
    # Créer le fichier JSON annexe avec horodatage
    sidecar_data = {
        "title": "test.jpg",
        "photoTakenTime": {"timestamp": "1736719606"}  # Horodatage Unix
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les métadonnées
    metadata = _run_exiftool_read(media_path)
    # Vérifier le format de la date-heure (devrait être YYYY:MM:DD HH:MM:SS)
    date_original = metadata.get("DateTimeOriginal")
    assert date_original is not None
    assert ":" in date_original
    # Devrait correspondre au format EXIF datetime
    import re
    assert re.match(r'\d{4}:\d{2}:\d{2} \d{2}:\d{2}:\d{2}', date_original)
@pytest.mark.integration
def test_write_and_read_albums(tmp_path: Path) -> None:
    """Tester que les albums sont écrits et peuvent être relus."""
    # Créer une image de test simple
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='cyan')
    img.save(media_path)
    # Créer le fichier metadata.json d'album
    album_data = {"title": "Vacances Été 2024"}
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    # Créer le fichier JSON annexe
    sidecar_data = {
        "title": "test.jpg",
        "description": "Photo de vacances"
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les métadonnées
    metadata = _run_exiftool_read(media_path)
    # Vérifier que l'album a été écrit comme mot-clé
    keywords = metadata.get("Keywords", [])
    if isinstance(keywords, str):
        keywords = [keywords]
    assert "Album: Vacances Été 2024" in keywords
    # Vérifier aussi le champ Subject
    subjects = metadata.get("Subject", [])
    if isinstance(subjects, str):
        subjects = [subjects]
    assert "Album: Vacances Été 2024" in subjects
@pytest.mark.integration  
def test_albums_and_people_combined(tmp_path: Path) -> None:
    """Tester que les albums et les personnes peuvent coexister dans les mots-clés."""
    # Créer une image de test simple
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='magenta')
    img.save(media_path)
    # Créer le fichier metadata.json d'album
    album_data = {"title": "Album Famille"}
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    # Créer le fichier JSON annexe avec des personnes
    sidecar_data = {
        "title": "test.jpg",
        "people": [{"name": "Alice"}, {"name": "Bob"}]
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar
    process_sidecar_file(json_path)
    # Relire les métadonnées
    metadata = _run_exiftool_read(media_path)
    # Vérifier que les mots-clés contiennent à la fois les personnes et l'album
    keywords = metadata.get("Keywords", [])
    if isinstance(keywords, str):
        keywords = [keywords]
    # Vérifier que nous avons à la fois des personnes et un album
    assert "Alice" in keywords
    assert "Bob" in keywords
    assert "Album: Album Famille" in keywords
@pytest.mark.integration
def test_default_safe_behavior(tmp_path: Path) -> None:
    """Tester que le comportement par défaut est sûr (append-only) et préserve les métadonnées existantes."""
    # Créer une simple image de test
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='red')
    img.save(media_path)
    # Tout d'abord, ajouter manuellement des métadonnées en utilisant le mode écrasement
    first_meta = SidecarData(
        filename="test.jpg",
        description="Original description",
        people=["Original Person"],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=["Original Album"]
    )
    # Écrire les métadonnées initiales avec le mode écrasement
    write_metadata(media_path, first_meta, append_only=False)
    # Vérifier que les métadonnées initiales ont été écrites
    initial_metadata = _run_exiftool_read(media_path)
    assert initial_metadata.get("ImageDescription") == "Original description"
    initial_keywords = initial_metadata.get("Keywords", [])
    if isinstance(initial_keywords, str):
        initial_keywords = [initial_keywords]
    assert "Original Person" in initial_keywords
    assert "Album: Original Album" in initial_keywords
    # Créer le fichier JSON annexe avec une nouvelle description et une nouvelle personne
    sidecar_data = {
        "title": "test.jpg",
        "description": "New description", 
        "people": [{"name": "New Person"}]
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar en mode par défaut (append-only)
    process_sidecar_file(json_path)
    # Relire les métadonnées
    final_metadata = _run_exiftool_read(media_path)
    # En mode append-only, la description d'origine doit être préservée
    # car nous utilisons -if "not $TAG" qui n'écrit que si le tag n'existe pas
    assert final_metadata.get("ImageDescription") == "Original description"
    # Les mots-clés devraient toujours contenir les données d'origine, et les nouvelles personnes devraient être AJOUTÉES (pas remplacées)
    # car nous utilisons = qui accumule pour les balises de type liste
    final_keywords = final_metadata.get("Keywords", [])
    if isinstance(final_keywords, str):
        final_keywords = [final_keywords]
    assert "Original Person" in final_keywords
    assert "Album: Original Album" in final_keywords
    # La nouvelle personne devrait également être présente
    assert "New Person" in final_keywords
@pytest.mark.integration  
def test_explicit_overwrite_behavior(tmp_path: Path) -> None:
    """Tester que le mode écrasement explicite remplace les métadonnées existantes."""
    # Créer une simple image de test
    media_path = tmp_path / "test.jpg"
    img = Image.new('RGB', (100, 100), color='blue') 
    img.save(media_path)
    # Tout d'abord, ajouter des métadonnées initiales en utilisant le mode écrasement
    first_meta = SidecarData(
        filename="test.jpg",
        description="Original description",
        people=["Original Person"],
        taken_at=None,
        created_at=None,
        latitude=None,
        longitude=None,
        altitude=None,
        favorite=False,
        albums=[]
    )
    write_metadata(media_path, first_meta, append_only=False)
    # Vérifier que les métadonnées initiales ont été écrites
    sidecar_data = {
        "title": "test.jpg",
        "description": "New description",
        "people": [{"name": "New Person"}]
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Traiter le sidecar en mode écrasement explicite
    process_sidecar_file(json_path, append_only=False)
    # Relire les métadonnées
    final_metadata = _run_exiftool_read(media_path)
    # En mode écrasement, la nouvelle description doit remplacer l'ancienne
    # Note: Nous utilisons l'opérateur = donc les personnes sont ajoutées et accumulent
    final_keywords = final_metadata.get("Keywords", [])
    if isinstance(final_keywords, str):
        final_keywords = [final_keywords]
    # Les deux personnes, originale et nouvelle, devraient être présentes (car = accumule pour les listes)
    assert "Original Person" in final_keywords
    assert "New Person" in final_keywords
@pytest.mark.integration
def test_append_only_vs_overwrite_video_equivalence(tmp_path: Path) -> None:
    """Tester que le mode append-only produit des résultats similaires au mode écrasement pour les vidéos quand aucune métadonnée n'existe."""
    # Copier les fichiers vidéo de test (vierges)
    video_path_append = tmp_path / "test_append.mp4"
    video_path_overwrite = tmp_path / "test_overwrite.mp4"
    _copy_test_asset("test_video_clean.mp4", video_path_append)
    _copy_test_asset("test_video_clean.mp4", video_path_overwrite)
    # Créer les métadonnées à écrire
    meta = SidecarData(
        filename="test.mp4",
        description="Test video description",
        people=["Video Person"],
        taken_at=1736719606,
        created_at=None,
        latitude=48.8566,
        longitude=2.3522,
        altitude=35.0,
        favorite=True,
        albums=["Test Album"]
    )
    # Écrire avec le mode append-only
    write_metadata(video_path_append, meta, append_only=True)
    # Écrire avec le mode écrasement
    write_metadata(video_path_overwrite, meta, append_only=False)
    # Relire les métadonnées des deux fichiers
    metadata_append = _run_exiftool_read(video_path_append)
    metadata_overwrite = _run_exiftool_read(video_path_overwrite)
    # Comparer les champs clés
    # En mode append-only, les nouvelles métadonnées peuvent ne pas être écrites si des tags similaires existent
    # En mode overwrite, les métadonnées sont toujours écrites
    # Le test vérifie que les nouvelles métadonnées importantes sont présentes
    # Les mots-clés devraient contenir la personne et l'album dans les deux modes
    keywords_append = metadata_append.get("Keywords", [])
    keywords_overwrite = metadata_overwrite.get("Keywords", [])
    if isinstance(keywords_append, str):
        keywords_append = [keywords_append]
    if isinstance(keywords_overwrite, str):
        keywords_overwrite = [keywords_overwrite]
    # Vérifier que les nouvelles métadonnées importantes sont présentes
    # En mode overwrite, les nouveaux mots-clés doivent être présents
    # Pour les vidéos MP4, les mots-clés sont stockés dans Subject, pas Keywords
    subjects_append = metadata_append.get("Subject", [])
    subjects_overwrite = metadata_overwrite.get("Subject", [])
    if isinstance(subjects_append, str):
        subjects_append = [subjects_append]
    if isinstance(subjects_overwrite, str):
        subjects_overwrite = [subjects_overwrite]
    # Combiner Keywords et Subject pour une vérification complète
    all_keywords_append = keywords_append + subjects_append
    all_keywords_overwrite = keywords_overwrite + subjects_overwrite
    assert "Video Person" in all_keywords_overwrite
    assert "Album: Test Album" in all_keywords_overwrite
    # En mode append-only, les mots-clés sont ajoutés même si d'autres existent
    assert "Video Person" in all_keywords_append
    assert "Album: Test Album" in all_keywords_append
@pytest.mark.integration
def test_batch_vs_normal_mode_equivalence(tmp_path: Path) -> None:
    """Tester que le mode batch produit les mêmes résultats que le mode normal."""
    # Importer la fonction de traitement par lot
    from google_takeout_metadata.processor_batch import process_directory_batch
    from google_takeout_metadata.processor import process_directory
    # Créer des données de test
    test_files = [
        ("photo1.jpg", "First test photo", "Alice"),
        ("photo2.jpg", "Second test photo", "Bob"),
        ("photo3.jpg", "Third test photo", "Charlie")
    ]
    # Créer deux structures de répertoires identiques
    normal_dir = tmp_path / "normal_mode"
    batch_dir = tmp_path / "batch_mode"
    normal_dir.mkdir()
    batch_dir.mkdir()
    for filename, description, person in test_files:
        # Créer les deux fichiers dans les deux répertoires
        for test_dir in [normal_dir, batch_dir]:
            # Créer l'image
            media_path = test_dir / filename
            img = Image.new('RGB', (100, 100), color='blue')
            img.save(media_path)
            # Créer le sidecar
            sidecar_data = {
                "title": filename,
                "description": description,
                "people": [{"name": person}]
            }
            json_path = test_dir / f"{filename}.json"
            json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    try:
        # Proceder avec le mode normal
        process_directory(normal_dir, use_localtime=False, append_only=True, clean_sidecars=False)
        # Traiter avec le mode par lot
        process_directory_batch(batch_dir, use_localtime=False, append_only=True, clean_sidecars=False)
        # Comparer les métadonnées des fichiers dans les deux répertoires
        for filename, expected_description, expected_person in test_files:
            normal_metadata = _run_exiftool_read(normal_dir / filename)
            batch_metadata = _run_exiftool_read(batch_dir / filename)
            # Vérifier que les descriptions correspondent
            assert normal_metadata.get("ImageDescription") == batch_metadata.get("ImageDescription")
            assert normal_metadata.get("ImageDescription") == expected_description
            # Vérifier que les personnes correspondent
            normal_people = normal_metadata.get("PersonInImage", [])
            batch_people = batch_metadata.get("PersonInImage", [])
            if isinstance(normal_people, str):
                normal_people = [normal_people]
            if isinstance(batch_people, str):
                batch_people = [batch_people]
            assert set(normal_people) == set(batch_people)
            assert expected_person in normal_people
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping batch vs normal mode test")
@pytest.mark.integration
def test_batch_mode_performance_benefit(tmp_path: Path) -> None:
    """Tester que le mode batch peut gérer de nombreux fichiers (test de performance)."""
    from google_takeout_metadata.processor_batch import process_directory_batch
    import time
    # Créer de nombreux fichiers de test
    num_files = 20  # Réduit pour CI, mais démontre toujours la capacité par lot
    for i in range(num_files):
        filename = f"perf_test_{i:03d}.jpg"
        # Créer l'image
        media_path = tmp_path / filename
        img = Image.new('RGB', (50, 50), color='red')
        img.save(media_path)
        # Créer le sidecar
        sidecar_data = {
            "title": filename,
            "description": f"Performance test image {i}"
        }
        json_path = tmp_path / f"{filename}.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    try:
        # Mesurer le temps de traitement par lot
        start_time = time.time()
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
        end_time = time.time()
        batch_time = end_time - start_time
        # Vérifier que tous les fichiers ont été traités
        for i in range(num_files):
            filename = f"perf_test_{i:03d}.jpg"
            media_path = tmp_path / filename
            metadata = _run_exiftool_read(media_path)
            expected_description = f"Performance test image {i}"
            assert metadata.get("ImageDescription") == expected_description
        # Imprimer le temps pris pour le traitement par lot
        print(f"Batch mode processed {num_files} files in {batch_time:.2f} seconds")
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping batch performance test")
@pytest.mark.integration  
def test_batch_mode_with_mixed_file_types(tmp_path: Path) -> None:
    """Tester le mode batch avec différents types de fichiers et métadonnées complexes."""
    from google_takeout_metadata.processor_batch import process_directory_batch
    # Créer des fichiers de test avec différents types et métadonnées
    test_files = [
        ("mixed1.jpg", "JPEG test"),
        ("mixed2.png", "PNG test")  # PNG if supported by PIL
    ]
    for filename, description in test_files:
        # Créer l'image
        media_path = tmp_path / filename
        if filename.endswith('.jpg'):
            img = Image.new('RGB', (100, 100), color='green')
            img.save(media_path, format='JPEG')
        elif filename.endswith('.png'):
            img = Image.new('RGBA', (100, 100), color=(0, 255, 0, 128))
            img.save(media_path, format='PNG')
        # Créer le sidecar complexe
        sidecar_data = {
            "title": filename,
            "description": description,
            "people": [{"name": "Mixed Test Person"}],
            "favorited": True,
            "geoData": {
                "latitude": 45.5017,
                "longitude": -73.5673,
                "altitude": 20.0
            }
        }
        json_path = tmp_path / f"{filename}.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    try:
        # Traiter avec le mode par lot
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
        # Vérifier que tous les fichiers ont été traités
        for filename, expected_description in test_files:
            media_path = tmp_path / filename
            metadata = _run_exiftool_read(media_path)
            # Vérifier la description
            assert metadata.get("ImageDescription") == expected_description
            # Vérifier les personnes
            people = metadata.get("PersonInImage", [])
            if isinstance(people, str):
                people = [people]
            assert "Mixed Test Person" in people
            # Vérifier la note (favori)
            rating = metadata.get("Rating")
            assert rating == 5 or rating == "5"
            # Vérifier les données GPS (peut ne pas fonctionner pour tous les types de fichiers)
            gps_lat = metadata.get("GPSLatitude")
            if gps_lat is not None:
                # Si GPSLatitude est présent, vérifier qu'il est correct
                assert "45 deg" in str(gps_lat)
                assert gps_lat is not None
    except FileNotFoundError:
        pytest.skip("exiftool introuvable - skipping mixed file types batch test")
````

## File: tests/test_processor_batch.py
````python
"""Tests pour la fonctionnalité de traitement par lots."""
import json
import subprocess
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from PIL import Image
from google_takeout_metadata.processor_batch import process_batch, process_directory_batch
from google_takeout_metadata.sidecar import SidecarData
def test_process_batch_empty_batch():
    """Tester que process_batch retourne 0 pour un lot vide."""
    result = process_batch([], clean_sidecars=False)
    assert result == 0
@patch('google_takeout_metadata.processor_batch.subprocess.run')
def test_process_batch_success(mock_subprocess_run, tmp_path):
    """Tester le traitement par lots réussi."""
    # Configuration
    mock_subprocess_run.return_value = Mock(returncode=0, stdout="    1 image files updated")
    media_path = tmp_path / "test.jpg"
    json_path = tmp_path / "test.jpg.json"
    args = ["-EXIF:ImageDescription=Test Description"]
    batch = [(media_path, json_path, args)]
    # Exécution
    result = process_batch(batch, clean_sidecars=False)
    # Vérification
    assert result == 1
    mock_subprocess_run.assert_called_once()
    # Vérifier que la commande a été construite correctement
    call_args = mock_subprocess_run.call_args
    cmd = call_args[0][0]
    assert cmd[0] == "exiftool"
    assert "-overwrite_original" in cmd
    assert "-charset" in cmd
    assert "-@" in cmd
@patch('google_takeout_metadata.processor_batch.subprocess.run')
def test_process_batch_with_argfile_content(mock_subprocess_run, tmp_path):
    """Vérifier que le fichier d'arguments est créé avec le contenu correct."""
    # Setup
    mock_subprocess_run.return_value = Mock(returncode=0, stdout="    2 image files updated")
    media_path1 = tmp_path / "test1.jpg"
    media_path2 = tmp_path / "test2.jpg"
    json_path1 = tmp_path / "test1.jpg.json"
    json_path2 = tmp_path / "test2.jpg.json"
    args1 = ["-EXIF:ImageDescription=Description 1", "-XMP:Rating=5"]
    args2 = ["-EXIF:ImageDescription=Description 2"]
    batch = [
        (media_path1, json_path1, args1),
        (media_path2, json_path2, args2)
    ]
    # Execute
    result = process_batch(batch, clean_sidecars=False)
    # Assert
    assert result == 2
    mock_subprocess_run.assert_called_once()
    # Vérifier que le chemin du fichier d'arguments a été passé au sous-processus
    call_args = mock_subprocess_run.call_args
    cmd = call_args[0][0]
    assert "-@" in cmd
    # Le chemin du fichier d'arguments devrait être l'argument juste après "-@"
    argfile_index = cmd.index("-@")
    assert argfile_index + 1 < len(cmd)  # S'assurer qu'il y a un argument après "-@"
@patch('google_takeout_metadata.processor_batch.subprocess.run')
def test_process_batch_clean_sidecars(mock_subprocess_run, tmp_path):
    """Vérifier que les fichiers de sidecar sont nettoyés lorsqu'on le demande."""
    # Setup
    mock_subprocess_run.return_value = Mock(returncode=0, stdout="    1 image files updated")
    media_path = tmp_path / "test.jpg"
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text('{"title": "test.jpg"}')
    args = ["-EXIF:ImageDescription=Test Description"]
    batch = [(media_path, json_path, args)]
    # Execute
    result = process_batch(batch, clean_sidecars=True)
    # Assert
    assert result == 1
    assert not json_path.exists()
@patch('google_takeout_metadata.processor_batch.subprocess.run')
def test_process_batch_exiftool_not_found(mock_subprocess_run):
    """Vérifier la gestion d'erreurs lorsque exiftool n'est pas trouvé."""
    # Setup
    mock_subprocess_run.side_effect = FileNotFoundError("exiftool introuvable")
    media_path = Path("test.jpg")
    json_path = Path("test.jpg.json")
    args = ["-EXIF:ImageDescription=Test Description"]
    batch = [(media_path, json_path, args)]
    # Execute & Assert
    with pytest.raises(RuntimeError, match="exiftool introuvable"):
        process_batch(batch, clean_sidecars=False)
@patch('google_takeout_metadata.processor_batch.subprocess.run')
def test_process_batch_exiftool_error(mock_subprocess_run, caplog):
    """Vérifier la gestion d'erreurs lorsque exiftool retourne une erreur."""
    # Setup
    mock_subprocess_run.side_effect = subprocess.CalledProcessError(
        1, ["exiftool"], stderr="Some error"
    )
    media_path = Path("test.jpg")
    json_path = Path("test.jpg.json")
    args = ["-EXIF:ImageDescription=Test Description"]
    batch = [(media_path, json_path, args)]
    # Execute
    result = process_batch(batch, clean_sidecars=False)
    # Assert
    assert result == 0
    assert "Échec du traitement par lot" in caplog.text
def test_process_directory_batch_no_sidecars(tmp_path, caplog):
    """Vérifier le traitement par lot lorsque aucun fichier de sidecar n'est trouvé."""
    # Execute
    process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
    # Assert
    assert "Aucun fichier de métadonnées (.json) trouvé" in caplog.text
@pytest.mark.integration
def test_process_directory_batch_single_file(tmp_path):
    """Vérifier le traitement par lot d'un seul fichier."""
    try:
        # Créer une image de test
        media_path = tmp_path / "test.jpg"
        img = Image.new('RGB', (100, 100), color='blue')
        img.save(media_path)
        # Créer le fichier JSON annexe
        sidecar_data = {
            "title": "test.jpg",
            "description": "Batch test description",
            "people": [{"name": "Batch Test Person"}]
        }
        json_path = tmp_path / "test.jpg.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Traiter en mode batch
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
        # Vérifier que les métadonnées ont été écrites en les relisant
        cmd = [
            "exiftool",
            "-j",
            "-EXIF:ImageDescription",
            "-XMP-iptcExt:PersonInImage",
            str(media_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        metadata = json.loads(result.stdout)[0]
        assert metadata.get("ImageDescription") == "Batch test description"
        people = metadata.get("PersonInImage", [])
        if isinstance(people, str):
            people = [people]
        assert "Batch Test Person" in people
    except FileNotFoundError:
        pytest.skip("Exiftool non trouvé - ignore les tests d'intégration")
@pytest.mark.integration  
def test_process_directory_batch_multiple_files(tmp_path):
    """Vérifier le traitement par lot de plusieurs fichiers."""
    try:
        # Créer plusieurs images de test avec leurs fichiers annexes
        files_data = [
            ("test1.jpg", "First batch test", "Person One"),
            ("test2.jpg", "Second batch test", "Person Two"),
            ("test3.jpg", "Third batch test", "Person Three")
        ]
        for filename, description, person in files_data:
            # Créer l'image
            media_path = tmp_path / filename
            img = Image.new('RGB', (100, 100), color='red')
            img.save(media_path)
            # Créer le fichier annexe
            sidecar_data = {
                "title": filename,
                "description": description,
                "people": [{"name": person}]
            }
            json_path = tmp_path / f"{filename}.json"
            json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Traiter en mode batch
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
        # Vérifier que tous les fichiers ont été traités correctement
        for filename, expected_description, expected_person in files_data:
            media_path = tmp_path / filename
            cmd = [
                "exiftool",
                "-j",
                "-EXIF:ImageDescription",
                "-XMP-iptcExt:PersonInImage",
                str(media_path)
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
            metadata = json.loads(result.stdout)[0]
            assert metadata.get("ImageDescription") == expected_description
            people = metadata.get("PersonInImage", [])
            if isinstance(people, str):
                people = [people]
            assert expected_person in people
    except FileNotFoundError:
        pytest.skip("Exiftool non trouvé - ignore les tests d'intégration")
@pytest.mark.integration
def test_process_directory_batch_with_albums(tmp_path):
    """Vérifier le traitement par lot avec des métadonnées d'album."""
    try:
        # Créer la structure de répertoires
        album_dir = tmp_path / "Album Test"
        album_dir.mkdir()
        # Créer les métadonnées d'album
        album_metadata = {
            "title": "Test Album",
            "description": "Album for batch testing"
        }
        metadata_path = album_dir / "metadata.json"
        metadata_path.write_text(json.dumps(album_metadata), encoding="utf-8")
        # Créer l'image de test dans le répertoire d'album
        media_path = album_dir / "album_photo.jpg"
        img = Image.new('RGB', (100, 100), color='green')
        img.save(media_path)
        # Créer le fichier annexe
        sidecar_data = {
            "title": "album_photo.jpg",
            "description": "Photo in album batch test"
        }
        json_path = album_dir / "album_photo.jpg.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Traiter en mode batch
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
        # Vérifier que l'album a été ajouté aux mots-clés
        cmd = [
            "exiftool",
            "-j",
            "-IPTC:Keywords",
            "-XMP-dc:Subject",
            str(media_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        metadata = json.loads(result.stdout)[0]
        keywords = metadata.get("Keywords", [])
        if isinstance(keywords, str):
            keywords = [keywords]
        assert "Album: Test Album" in keywords
    except FileNotFoundError:
        pytest.skip("Exiftool non trouvé - ignore les tests d'intégration")
@pytest.mark.integration
def test_process_directory_batch_clean_sidecars(tmp_path):
    """Test d'intégration pour le traitement par lot avec nettoyage des sidecars."""
    try:
        # Créer une image de test
        media_path = tmp_path / "cleanup_test.jpg"
        img = Image.new('RGB', (100, 100), color='yellow')
        img.save(media_path)
        # Créer le fichier JSON annexe
        sidecar_data = {
            "title": "cleanup_test.jpg",
            "description": "Test cleanup functionality"
        }
        json_path = tmp_path / "cleanup_test.jpg.json"
        json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
        # Vérifier que le fichier annexe existe avant le traitement
        assert json_path.exists()
        # Traiter avec le nettoyage activé
        process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=True)
        # Vérifier que le fichier annexe a été nettoyé
        assert not json_path.exists()
        # Vérifier que les métadonnées ont quand même été écrites
        cmd = [
            "exiftool",
            "-j",
            "-EXIF:ImageDescription",
            str(media_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=30)
        metadata = json.loads(result.stdout)[0]
        assert metadata.get("ImageDescription") == "Test cleanup functionality"
    except FileNotFoundError:
        pytest.skip("Exiftool non trouvé - ignore les tests d'intégration")
@patch('google_takeout_metadata.processor_batch.parse_sidecar')
def test_process_directory_batch_invalid_sidecar(mock_parse_sidecar, tmp_path, caplog):
    """Tester le traitement par lot avec un fichier sidecar invalide."""
    # Configuration
    mock_parse_sidecar.side_effect = ValueError("Invalid JSON")
    # Créer des fichiers factices
    media_path = tmp_path / "invalid.jpg"
    media_path.write_text("dummy")
    json_path = tmp_path / "invalid.jpg.json"
    json_path.write_text("invalid json")
    # Exécuter
    process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
    # Vérifier
    assert "Échec de la préparation de" in caplog.text
@patch('google_takeout_metadata.processor_batch.build_exiftool_args')
def test_process_directory_batch_no_args_generated(mock_build_args, tmp_path):
    """Tester le traitement par lot quand aucun argument exiftool n'est généré."""
    # Configuration - build_exiftool_args retourne une liste vide
    mock_build_args.return_value = []
    # Créer l'image de test
    media_path = tmp_path / "no_args.jpg"
    img = Image.new('RGB', (100, 100), color='white')
    img.save(media_path)
    # Créer le fichier JSON annexe
    sidecar_data = {"title": "no_args.jpg"}
    json_path = tmp_path / "no_args.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Exécuter (ne devrait pas planter même sans arguments)
    process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
    # Aucune assertion spécifique nécessaire - juste s'assurer que ça ne plante pas
def test_process_directory_batch_missing_media_file(tmp_path, caplog):
    """Tester le traitement par lot quand le fichier média est manquant."""
    # Créer un fichier annexe sans fichier média correspondant
    sidecar_data = {
        "title": "missing.jpg",
        "description": "Media file does not exist"
    }
    json_path = tmp_path / "missing.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Exécuter
    process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
    # Vérifier
    assert "Fichier image introuvable" in caplog.text
@patch('google_takeout_metadata.processor_batch.fix_file_extension_mismatch')
@patch('google_takeout_metadata.processor_batch.parse_sidecar')
def test_process_directory_batch_file_extension_fix(mock_parse_sidecar, mock_fix_extension, tmp_path):
    """Tester que la correction de l'extension de fichier est gérée dans le traitement par lot."""
    # Configuration
    media_path = tmp_path / "test.jpg"
    json_path = tmp_path / "test.jpg.json"
    fixed_media_path = tmp_path / "test.jpeg"
    fixed_json_path = tmp_path / "test.jpeg.json"
    # Créer les fichiers
    img = Image.new('RGB', (100, 100), color='black')
    img.save(media_path)
    json_path.write_text('{"title": "test.jpg"}')
    # Simuler la correction d'extension pour retourner des chemins différents
    mock_fix_extension.return_value = (fixed_media_path, fixed_json_path)
    # Simuler parse_sidecar pour retourner des données différentes pour chaque appel
    mock_parse_sidecar.side_effect = [
        SidecarData(filename="test.jpg", description="Original", people=[], taken_at=None, created_at=None, 
                   latitude=None, longitude=None, altitude=None, favorite=False, albums=[]),
        SidecarData(filename="test.jpeg", description="Fixed", people=[], taken_at=None, created_at=None,
                   latitude=None, longitude=None, altitude=None, favorite=False, albums=[])
    ]
    # Exécuter
    process_directory_batch(tmp_path, use_localtime=False, append_only=True, clean_sidecars=False)
    # Vérifier que fix_file_extension_mismatch a été appelé
    mock_fix_extension.assert_called()
    # Vérifier que parse_sidecar a été appelé deux fois (une fois pour l'original, une fois pour le corrigé)
    assert mock_parse_sidecar.call_count == 2
````

## File: tests/test_processor.py
````python
from pathlib import Path
import json
import unittest.mock
import os
from google_takeout_metadata.processor import (
    process_directory, 
    _is_sidecar_file, 
    fix_file_extension_mismatch
)
def test_ignore_non_sidecar(tmp_path: Path) -> None:
    (tmp_path / "data.json").write_text("{}", encoding="utf-8")
    process_directory(tmp_path)
def test_is_sidecar_file_standard_pattern() -> None:
    """Test standard pattern: photo.jpg.json"""
    assert _is_sidecar_file(Path("photo.jpg.json"))
    assert _is_sidecar_file(Path("video.mp4.json"))
    assert _is_sidecar_file(Path("image.PNG.JSON"))  # insensible à la casse
def test_is_sidecar_file_supplemental_metadata_pattern() -> None:
    """Vérifier le format Google Takeout: photo.jpg.supplemental-metadata.json"""
    assert _is_sidecar_file(Path("IMG_001.jpg.supplemental-metadata.json"))
    assert _is_sidecar_file(Path("video.mp4.supplemental-metadata.json"))
    assert _is_sidecar_file(Path("image.PNG.SUPPLEMENTAL-METADATA.JSON"))  # insensible à la casse
    assert _is_sidecar_file(Path("photo.heic.supplemental-metadata.json"))
def test_is_sidecar_file_older_pattern() -> None:
    """Vérifier l'ancien format: photo.json"""
    assert _is_sidecar_file(Path("IMG_1234.jpg.json"))  # devrait fonctionner avec la nouvelle logique
    # Note: photo.json sans extension dans le nom ne serait pas détecté
    # car c'est ambigu, mais c'est acceptable puisque parse_sidecar() valide
def test_is_sidecar_file_negative() -> None:
    """vérifier les fichiers qui ne devraient pas être détectés comme sidecars"""
    assert not _is_sidecar_file(Path("data.json"))  # pas d'extension d'image
    assert not _is_sidecar_file(Path("photo.txt"))  # pas un json
    assert not _is_sidecar_file(Path("photo.jpg"))  # pas un json
    assert not _is_sidecar_file(Path("metadata.json"))  # album metadata, pas un sidecar
    assert not _is_sidecar_file(Path("métadonnées.json"))  # album metadata, pas un sidecar
def test_fix_file_extension_mismatch_rollback_on_failure(tmp_path: Path) -> None:
    """Vérifier que fix_file_extension_mismatch annule correctement le renommage de l'image en cas d'échec"""
    # Créer un faux fichier JPEG avec une mauvaise extension
    media_path = tmp_path / "photo.png"
    media_path.write_bytes(b'\xff\xd8\xff\xe0')  # JPEG magic bytes
    # Créer le fichier JSON correspondant
    json_path = tmp_path / "photo.png.supplemental-metadata.json"
    json_data = {"title": "photo.png"}
    json_path.write_text(json.dumps(json_data), encoding='utf-8')
    # Simuler un échec de unlink pour le fichier JSON (fichier en lecture seule)
    original_unlink = Path.unlink
    def mock_unlink(self):
        if self.name.endswith('.supplemental-metadata.json') and 'photo.png' in str(self):
            raise OSError("Permission denied")
        return original_unlink(self)
    with unittest.mock.patch.object(Path, 'unlink', mock_unlink):
        result_image, result_json = fix_file_extension_mismatch(media_path, json_path)
        # Devrait retourner les chemins d'origine car le rollback a réussi
        assert result_image == media_path
        assert result_json == json_path
        assert media_path.exists()  # L'image originale devrait exister à nouveau
        assert not (tmp_path / "photo.jpg").exists()  # L'image renommée ne devrait pas exister
        assert not (tmp_path / "photo.jpg.supplemental-metadata.json").exists()  # Pas de JSON orphelin attendu
def test_fix_file_extension_mismatch_failed_rollback(tmp_path: Path) -> None:
    """Tester fix_file_extension_mismatch lorsque l'opération et le rollback échouent tous les deux"""
    # Créer un faux fichier JPEG avec une mauvaise extension
    media_path = tmp_path / "photo.png"
    media_path.write_bytes(b'\xff\xd8\xff\xe0')  # JPEG magic bytes
    # Créer le fichier JSON correspondant
    json_path = tmp_path / "photo.png.supplemental-metadata.json"
    json_data = {"title": "photo.png"}
    json_path.write_text(json.dumps(json_data), encoding='utf-8')
    # Simuler un échec à la fois pour le renommage de l'image et pour le rollback du JSON
    original_unlink = Path.unlink
    def mock_unlink(self):
        if self.name.endswith('.supplemental-metadata.json'):
            raise OSError("Permission denied")
        return original_unlink(self)
    def mock_rename(self, target):
        # Simuler un échec lors du rollback du renommage
        if str(target).endswith('.png') and str(self).endswith('.jpg'):
            raise OSError("Rollback failed")
        # Sinon, faire le renommage réel
        os.rename(str(self), str(target))
    with unittest.mock.patch.object(Path, 'unlink', mock_unlink), \
         unittest.mock.patch.object(Path, 'rename', mock_rename):
        result_image, result_json = fix_file_extension_mismatch(media_path, json_path)
        # Devrait retourner le nouveau chemin de l'image mais l'ancien chemin du JSON en raison du rollback échoué
        assert result_image == tmp_path / "photo.jpg"
        assert result_json == json_path  # Chemin JSON d'origine
        assert (tmp_path / "photo.jpg").exists()  # La nouvelle image devrait exister
        assert not media_path.exists()  # L'image originale ne devrait pas exister
````

## File: tests/test_sidecar.py
````python
from pathlib import Path
import json
import pytest
from google_takeout_metadata.sidecar import parse_sidecar
def test_parse_sidecar(tmp_path: Path) -> None:
    sample = {
        "title": "1729436788572.jpg",
        "description": "Magicien en or",
        "creationTime": {"timestamp": "1736719606"},
        "photoTakenTime": {"timestamp": "1736719606"},
        "geoData": {"latitude": 0.0, "longitude": 0.0, "altitude": 0.0},
        "people": [{"name": "anthony vincent"}],
    }
    json_path = tmp_path / "1729436788572.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.filename == "1729436788572.jpg"
    assert meta.description == "Magicien en or"
    assert meta.people == ["anthony vincent"]
    assert meta.taken_at == 1736719606
    assert meta.created_at == 1736719606
def test_title_mismatch(tmp_path: Path) -> None:
    data = {"title": "other.jpg"}
    json_path = tmp_path / "sample.jpg.json"
    json_path.write_text(json.dumps(data), encoding="utf-8")
    with pytest.raises(ValueError):
        parse_sidecar(json_path)
def test_parse_sidecar_supplemental_metadata_format(tmp_path: Path) -> None:
    """Tester l'analyse du nouveau format Google Takeout : IMG_001.jpg.supplemental-metadata.json"""
    sample = {
        "title": "IMG_001.jpg",
        "description": "Test photo with new format",
        "creationTime": {"timestamp": "1736719606"},
        "photoTakenTime": {"timestamp": "1736719606"},
        "people": [{"name": "test user"}],
    }
    json_path = tmp_path / "IMG_001.jpg.supplemental-metadata.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.filename == "IMG_001.jpg"
    assert meta.description == "Test photo with new format"
    assert meta.people == ["test user"]
    assert meta.taken_at == 1736719606
    assert meta.created_at == 1736719606
def test_title_mismatch_supplemental_metadata(tmp_path: Path) -> None:
    """Tester la validation du titre avec le format supplemental-metadata."""
    data = {"title": "wrong_name.jpg"}
    json_path = tmp_path / "IMG_001.jpg.supplemental-metadata.json"
    json_path.write_text(json.dumps(data), encoding="utf-8")
    with pytest.raises(ValueError, match="Le titre du sidecar.*ne correspond pas au nom de fichier attendu"):
        parse_sidecar(json_path)
def test_invalid_json(tmp_path: Path) -> None:
    json_path = tmp_path / "bad.jpg.json"
    json_path.write_text("not json", encoding="utf-8")
    with pytest.raises(ValueError):
        parse_sidecar(json_path)
def test_zero_coordinates(tmp_path: Path) -> None:
    sample = {
        "title": "a.jpg",
        "geoData": {"latitude": 0.0, "longitude": 0.0, "altitude": 10.0, "latitudeSpan": 1, "longitudeSpan": 1},
    }
    json_path = tmp_path / "a.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    # Les coordonnées 0/0 doivent être filtrées car peu fiables
    assert meta.latitude is None
    assert meta.longitude is None
    assert meta.altitude is None
def test_people_deduplication(tmp_path: Path) -> None:
    """Tester que les noms de personnes sont dédupliqués et nettoyés."""
    sample = {
        "title": "a.jpg",
        "people": [
            {"name": "alice"},
            {"name": " alice "},  # avec espaces
            {"name": "alice"},   # doublon
            {"name": "bob"},
            {"name": "  "},      # vide après nettoyage
            {"name": "charlie"},
            {"name": " bob "},   # autre doublon avec espaces
        ]
    }
    json_path = tmp_path / "a.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    # Devrait avoir dédupliqué et nettoyé : ["alice", "bob", "charlie"]
    assert meta.people == ["alice", "bob", "charlie"]
def test_parse_favorite_true(tmp_path: Path) -> None:
    """Tester l'analyse d'une photo favorite avec le format Google Takeout réel."""
    sample = {
        "title": "favorite.jpg",
        "favorited": True
    }
    json_path = tmp_path / "favorite.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.favorite is True
def test_parse_favorite_false(tmp_path: Path) -> None:
    """Tester l'analyse d'une photo non favorite avec le format Google Takeout réel."""
    sample = {
        "title": "not_favorite.jpg",
        "favorited": False
    }
    json_path = tmp_path / "not_favorite.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.favorite is False
def test_parse_no_favorite_field(tmp_path: Path) -> None:
    """Tester l'analyse d'une photo sans champ favori."""
    sample = {
        "title": "no_fav.jpg",
        "description": "Test photo"
    }
    json_path = tmp_path / "no_fav.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.favorite is False
def test_parse_zero_geo_coordinates(tmp_path: Path) -> None:
    """Tester que les coordonnées 0/0 sont filtrées car peu fiables."""
    sample = {
        "title": "geo_zero.jpg",
        "geoData": {"latitude": 0.0, "longitude": 0.0, "altitude": 100.0}
    }
    json_path = tmp_path / "geo_zero.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    # Les coordonnées 0/0 doivent être filtrées
    assert meta.latitude is None
    assert meta.longitude is None
    assert meta.altitude is None
def test_parse_valid_geo_coordinates(tmp_path: Path) -> None:
    """Tester que les coordonnées valides sont préservées."""
    sample = {
        "title": "geo_valid.jpg",
        "geoData": {"latitude": 48.8566, "longitude": 2.3522, "altitude": 35.0}
    }
    json_path = tmp_path / "geo_valid.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.latitude == 48.8566
    assert meta.longitude == 2.3522
    assert meta.altitude == 35.0
def test_parse_people_nested_format(tmp_path: Path) -> None:
    """Tester l'analyse des personnes au format imbriqué : [{"person": {"name": "X"}}]."""
    sample = {
        "title": "nested_people.jpg",
        "people": [
            {"person": {"name": "alice"}},
            {"person": {"name": "bob"}},
            {"name": "charlie"}  # format mixte
        ]
    }
    json_path = tmp_path / "nested_people.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.people == ["alice", "bob", "charlie"]
def test_parse_missing_timestamps(tmp_path: Path) -> None:
    """Tester l'analyse quand les horodatages sont manquants."""
    sample = {
        "title": "no_dates.jpg",
        "description": "Photo without dates"
    }
    json_path = tmp_path / "no_dates.jpg.json"
    json_path.write_text(json.dumps(sample), encoding="utf-8")
    meta = parse_sidecar(json_path)
    assert meta.taken_at is None
    assert meta.created_at is None
def test_parse_album_metadata(tmp_path: Path) -> None:
    """Tester l'analyse des métadonnées d'album depuis les fichiers metadata.json."""
    from google_takeout_metadata.sidecar import parse_album_metadata
    # Tester les métadonnées d'album de base
    album_data = {
        "title": "Vacances 2024",
        "description": "Photos des vacances d'été"
    }
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    albums = parse_album_metadata(metadata_path)
    assert albums == ["Vacances 2024"]
def test_parse_album_metadata_multiple_albums(tmp_path: Path) -> None:
    """Tester l'analyse de plusieurs références d'albums."""
    from google_takeout_metadata.sidecar import parse_album_metadata
    album_data = {
        "title": "Album Principal",
        "albums": [
            {"title": "Sous-album 1"},
            {"title": "Sous-album 2"},
            "Album Simple"
        ]
    }
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    albums = parse_album_metadata(metadata_path)
    assert set(albums) == {"Album Principal", "Album Simple", "Sous-album 1", "Sous-album 2"}
def test_find_albums_for_directory(tmp_path: Path) -> None:
    """Tester la recherche d'albums pour un répertoire."""
    from google_takeout_metadata.sidecar import find_albums_for_directory
    # Créer les métadonnées d'album
    album_data = {"title": "Mon Album"}
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    albums = find_albums_for_directory(tmp_path)
    assert albums == ["Mon Album"]
def test_find_albums_for_directory_no_metadata(tmp_path: Path) -> None:
    """Tester la recherche d'albums quand aucune métadonnée n'existe."""
    from google_takeout_metadata.sidecar import find_albums_for_directory
    albums = find_albums_for_directory(tmp_path)
    assert albums == []
def test_find_albums_french_metadata_format(tmp_path: Path) -> None:
    """Tester la recherche d'albums avec le format de fichier de métadonnées français."""
    from google_takeout_metadata.sidecar import find_albums_for_directory
    # Créer les métadonnées d'album français
    album_data = {"title": "Mon Album Français"}
    metadata_path = tmp_path / "métadonnées.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    albums = find_albums_for_directory(tmp_path)
    assert albums == ["Mon Album Français"]
def test_find_albums_french_numbered_metadata(tmp_path: Path) -> None:
    """Tester la recherche d'albums avec des fichiers de métadonnées français numérotés."""
    from google_takeout_metadata.sidecar import find_albums_for_directory
    # Créer plusieurs fichiers de métadonnées français
    album_data1 = {"title": "Album 1"}
    metadata_path1 = tmp_path / "métadonnées.json"
    metadata_path1.write_text(json.dumps(album_data1), encoding="utf-8")
    album_data2 = {"title": "Album 2"}
    metadata_path2 = tmp_path / "métadonnées(1).json"
    metadata_path2.write_text(json.dumps(album_data2), encoding="utf-8")
    album_data3 = {"title": "Album 3"}
    metadata_path3 = tmp_path / "métadonnées(2).json"
    metadata_path3.write_text(json.dumps(album_data3), encoding="utf-8")
    albums = find_albums_for_directory(tmp_path)
    assert set(albums) == {"Album 1", "Album 2", "Album 3"}
def test_find_albums_mixed_formats(tmp_path: Path) -> None:
    """Tester la recherche d'albums avec des fichiers de métadonnées mixtes anglais et français."""
    from google_takeout_metadata.sidecar import find_albums_for_directory
    # Créer les métadonnées anglais
    album_data_en = {"title": "English Album"}
    metadata_path_en = tmp_path / "metadata.json"
    metadata_path_en.write_text(json.dumps(album_data_en), encoding="utf-8")
    # Créer les métadonnées français
    album_data_fr = {"title": "Album Français"}
    metadata_path_fr = tmp_path / "métadonnées.json"
    metadata_path_fr.write_text(json.dumps(album_data_fr), encoding="utf-8")
    albums = find_albums_for_directory(tmp_path)
    assert set(albums) == {"Album Français", "English Album"}
def test_sidecar_with_albums_from_directory(tmp_path: Path) -> None:
    """Tester que les albums sont ajoutés depuis les métadonnées de répertoire lors du traitement des sidecars."""
    from google_takeout_metadata.processor import process_sidecar_file
    from google_takeout_metadata.sidecar import parse_sidecar
    # Créer les métadonnées d'album
    album_data = {"title": "Album Test"}
    metadata_path = tmp_path / "metadata.json"
    metadata_path.write_text(json.dumps(album_data), encoding="utf-8")
    # Créer un fichier image factice
    media_path = tmp_path / "test.jpg"
    with open(media_path, 'wb') as f:
        f.write(b'\xFF\xD8\xFF\xE0')  # En-tête JPEG minimal
    # Créer le sidecar
    sidecar_data = {
        "title": "test.jpg",
        "description": "Test photo"
    }
    json_path = tmp_path / "test.jpg.json"
    json_path.write_text(json.dumps(sidecar_data), encoding="utf-8")
    # Analyser le sidecar - les albums devraient être vides initialement
    meta = parse_sidecar(json_path)
    assert meta.albums == []
    # Note: Nous ne pouvons pas tester process_sidecar_file sans exiftool
    # mais nous pouvons tester la logique de recherche d'albums séparément
````
